<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PowerPoint 从入门到跑路</title>
      <link href="efficiency/learning-ppt-from-scratch/"/>
      <url>efficiency/learning-ppt-from-scratch/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>自查。【秋叶 Office 三合一课程之 PowerPoint 篇】的学习笔记。</p><a id="more"></a><h2 id="PPT-课程总结"><a href="#PPT-课程总结" class="headerlink" title="PPT 课程总结"></a>PPT 课程总结</h2><h3 id="02-个人介绍"><a href="#02-个人介绍" class="headerlink" title="02 个人介绍"></a>02 个人介绍</h3><h4 id="02-1-纯文字"><a href="#02-1-纯文字" class="headerlink" title="02-1 纯文字"></a>02-1 纯文字</h4><p><strong>纯文字制作高大上的个人简介</strong></p><p><img src="https://i.loli.net/2020/08/22/8umlNPrhASW7gnX.png" alt="02-1.png" loading="lazy"></p><ol><li>单行文本框和段落文本框有差别。根据需要来选择</li><li>把PPT页面当画布，每一个 文本框都是一个单独的对象。</li><li>为大段的文字段落设置合适 的”行距”能够增加文字的”透气性”，使之更易于阅读，推荐设置1.2或1.3倍行距。</li><li>文字的”加粗”命令一般会用到标题或强调的文本上；某些字体的”加粗”效果并不明显，更换笔画更粗的字体做标题效果更好</li><li>大段文字不要使用纯黑，使用深灰色（浅黑），可以降低阅读时的压迫感，营造更好的阅读体验。</li></ol><h4 id="02-2-字体运用"><a href="#02-2-字体运用" class="headerlink" title="02-2 字体运用"></a>02-2 字体运用</h4><p><strong>运用字体打造个性化人物介绍</strong></p><p><img src="https://i.loli.net/2020/08/22/A7VBeyZlMsJYqT9.png" alt="02-2.png" loading="lazy"></p><ol><li>字体是传递PPT内在情感 的重要手段；根据PPT的内容为文字选择一款合适的字体至关重要。</li><li>根据字体的需求是确定还是模糊、是否知道字体的名字等条件的不同，搜索字体的渠道方法也各不相同。</li><li>双击打开字体文件（通常是.ttf或.otf)，点击安装即可安装字体。</li><li>除了在一开始就使用竖排文本框进行文字输入，对于已经输入完成的横排文本，可以通过“文字方向”命令调整为竖排。英文单词请注意竖排时的规范。</li></ol><h4 id="02-3-不拘一格"><a href="#02-3-不拘一格" class="headerlink" title="02-3 不拘一格"></a>02-3 不拘一格</h4><p><strong>不拘一格才能玩出创意</strong></p><p><img src="https://i.loli.net/2020/08/22/euPoGhKVUN1BXW2.png" alt="02-3_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/hMHo5GbeTk3lLy9.png" alt="02-3_2.png" loading="lazy"></p><ol><li>将PPT页面背景进行填充可以增加PPT的设计感，使之更加显眼；背景填充分为纯色、渐变、图片或纹理、图案 四种填充方式。</li><li>PPT在放映时只会显示页面范围内的元素，超出页面范围的元素不会被显示。很多时候我们会利用这一特性完成一些特定的设计。</li><li>项目符号常被用来展示一系 列具有并列关系的文本。 通过切换项目符号所使用的字体，我们可以轻松制作出许多图标类的项目符号来。</li><li>我们可以在”形状格式”菜单中设置文字的颜色、透明度、各种阴影和映像等效果。 但一定要注意的是确认自己选择了对应的”文本选项”</li></ol><h3 id="03-工作总结"><a href="#03-工作总结" class="headerlink" title="03 工作总结"></a>03 工作总结</h3><h4 id="03-1-封面页"><a href="#03-1-封面页" class="headerlink" title="03-1 封面页"></a>03-1 封面页</h4><p><strong>简单快速制作封面页</strong></p><p><img src="https://i.loli.net/2020/08/22/HNOUsePZgLF3tv7.png" alt="03-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/p7MDCqAJbOr4nYI.png" alt="03-1_2.png" loading="lazy"></p><ol><li>矩形在PPT设计中的使用频率非常高；综合调节填充色和轮廓色，可以打造出矩形色 块、无色矩形、线框矩形等多种风格。</li><li>在PPT中Ctrl+拖动对象，可以复制此对象；Shift+拖动对象，可以锁定对象移动方向； Ctrl+Shift+拖动对象，可以实现水平/竖直方向的复制。</li><li>Ctrl+鼠标滚轮，可以对编辑区实现比例放大或缩小，方便我们做整体调节或细微调整。</li><li>当需要将两行长度不等的文本做整体对齐时，先将较短一行拉长，然后使用”分散对齐”，效果往往会不错。</li></ol><h4 id="03-2-目录页"><a href="#03-2-目录页" class="headerlink" title="03-2 目录页"></a>03-2 目录页</h4><p><strong>干净清爽的目录页</strong></p><p><img src="https://i.loli.net/2020/08/22/BUFHGQ5YWCODT9c.png" alt="03-2.png" loading="lazy"></p><ol><li>除了形状以外，线条也是PPT中常见的元素；色块不但可以是矩形块，也可以是圆形等其它形状。形状与形状之间往往会搭配使用。</li><li>如果想要绘制正圆形、正方形、正三角形、正多边形等形状，在绘制的同时按住Shift键就可以了。</li><li>“F4”键在大部分时候可以实现重复上一步操作，在批量调整格式或等距复制时非常有用。</li><li>单独使用文本框输入文字， 然后再将文本框与形状叠放在一起，比直接在形状内部键入文字更加灵活，并且还不会受到形状旋转的影响。</li></ol><h4 id="03-3-内容页"><a href="#03-3-内容页" class="headerlink" title="03-3 内容页"></a>03-3 内容页</h4><p><strong>结构明晰的内容页</strong></p><p><img src="https://i.loli.net/2020/08/22/Ml8a2OmbzdBFsR3.png" alt="03-3_1.png" loading="lazy"></p><ol><li>PPT内页的设计可以从网站设计中找寻参考；导航条式左右版的内页设计有利于清晰展现整个PPT的进程，在章节不多时可以考虑使用</li><li>在“形状轮廓”下拉菜单中，除了可以设置线条的颜色，粗细,还可以将线条转变为虚线以及其他线条，营造出不一样的视觉效果。</li><li>如果某一些文字关联性很强，即便它们有不一样的格式，也可以使用同一个文本框来输入,然后分别设置格式。方便整体移动、添加动画等操作。</li><li>使用“格式刷”可以快速将外观格式在文本之间传递，快速完成多对象的格式统一。这一功能不仅局限于文本，对在形状、图片等同类对象间复制外观均有效。</li></ol><p><img src="https://i.loli.net/2020/08/22/tqY4KbQGfTPZIXa.png" alt="03-3_2.png" loading="lazy"></p><h4 id="03-4-排版窍门"><a href="#03-4-排版窍门" class="headerlink" title="03-4 排版窍门"></a>03-4 排版窍门</h4><p><strong>省时高效的排版窍门</strong></p><p><img src="https://i.loli.net/2020/08/22/9rhjHCT6sYQ8qiX.png" alt="03-4.png" loading="lazy"></p><ol><li>合理使用母版和版式，可以大幅度降低PPT设计中的重复劳动；母版与普通页面的一对多的对应关系，可以使整个PPT风格维持稳定。</li><li>在母版中的某一版式页增加元素或进行修改，普通页面中所有使用该版式的页面都会随之变化。</li><li>母版中的元素,在页面中无法被选中、修改,因此不能把需要有变化的文字(如页面小标题)直接以文本框的方式放入母版内，只能采取插入占位符的方式插入。</li><li>在套用版式时，如果要继续套用当前版式，就回车新建页面；如果要更換其它版式，就点击“版式”按钮进行选择。</li></ol><h3 id="04-多媒体相册"><a href="#04-多媒体相册" class="headerlink" title="04 多媒体相册"></a>04 多媒体相册</h3><h4 id="04-1-封面"><a href="#04-1-封面" class="headerlink" title="04-1 封面"></a>04-1 封面</h4><p><strong>巧用裁剪做封面和图文混排</strong></p><p><img src="https://i.loli.net/2020/08/22/aeCNv4LW58Pbm3Z.png" alt="04-1.png" loading="lazy"></p><ol><li>在PPT中使用图片通常会因为排版需求而修改，为了保证图片内容不变形,调整图片比例需要使用“裁剪”命令。</li><li>在图片裁剪状态下拖动裁剪框可以制定图片的裁剪方案，按住Ctrl+Shift拖动裁剪框可使对侧的裁剪框做对称调整</li><li>为了得到更精确的裁剪效果，可以使用裁剪的扩展模式：按纵横比裁剪。在此模式下选择“方形1:1即可将图片裁剪为正方形。</li><li>选中对象后Ctrl+D可以直接复制对象,等同于Ctrl+C(复制)后Ctrl+V(粘贴)。</li></ol><h4 id="04-2-单页多图"><a href="#04-2-单页多图" class="headerlink" title="04-2 单页多图"></a>04-2 单页多图</h4><p><strong>单页多图式页面制作</strong></p><p>不同排版构思，具体操作也不同：一页一图 or 一页多图？</p><p><img src="https://i.loli.net/2020/08/22/jXqSNvVCHOGoI2Z.png" alt="04-2.png" loading="lazy"></p><ol><li>合适的背景图片可以起到营造真实效果的作用选择时不能仅以“好看<br>为标准；记住推荐的高清兔费图片站点 Unsplash</li><li>想让PPT以图片为页面背景，并非是将图片放大到和页面相同大小后置于底层，而是将其“填充”为背景。比例不合适的图片可能需要先裁剪后填充</li><li>使用“图片样式”可以一键为图片赋予各种丰富的造型，但清晰度上会有一定损耗。如果只是制作成照片形式，可手动完成。但需要注意调整连接线类型为“斜角”。</li><li>综合使用“裁剪”功能和删除背景”功能，可以将图片的部分内容“抠取出来，用来打造真实的空间层次感非常有效。</li></ol><h4 id="04-3-单页全图"><a href="#04-3-单页全图" class="headerlink" title="04-3 单页全图"></a>04-3 单页全图</h4><p><strong>单页全图式页面制作</strong></p><p>不同排版构思，具体操作也不同：一页一图 or 一页多图？</p><p><img src="https://i.loli.net/2020/08/22/Qrq7CdjwD3x1BUt.png" alt="04-3_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/DshZIoKeyNCB9bV.png" alt="04-3_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/7n5HVakljSDUYrW.png" alt="04-3_3.png" loading="lazy"></p><ol><li>“单图多页”相册的制作要点是避免单调的图片陈列；综合使用形状和文字素材，搭配对图片的裁剪才能做好单图相册页面。</li><li>横向图片与页面比例相似，往往可以直接填充。但考虑到图文结合效果，通常需要<br>人为裁剪留出摆放文字的位置</li><li>通过覆盖渐变矩形，可以隐去图片的硬边缘，使得图片向背景的过渡更自然柔和。这一招常被用在纵向图片与页面的结合中</li><li>利用“艺术效果-虚化”，可以为使用方向图片自身的虚化版本作为背景图。这样得到的背景可以保证与主图片色调上的统一。</li></ol><h4 id="04-4-背景音乐"><a href="#04-4-背景音乐" class="headerlink" title="04-4 背景音乐"></a>04-4 背景音乐</h4><p><strong>添加背景音乐与播放设置</strong></p><p><img src="https://i.loli.net/2020/08/22/Pb4isHVgnAlGRax.png" alt="04-4.png" loading="lazy"></p><p>添加BGM步骤：挑选合适的音乐 &gt;&gt; 正确插入音乐 &gt;&gt; 完成播放设置</p><ol><li>在相册PPT中添加背景音乐最好使用情绪恰当的纯音乐；在”网易云音乐”上可以轻松找到他人整理后特定主题的歌单。</li><li>为了方便在旧版 Powerpoint 环境下正常播放PPT中的音乐，需要保证音乐的格式为WAV格式。</li><li>保证音乐能够正确播放还需注意目录问题。插入PPT之后的音乐文件不能更改目录，因此需要在插入前就装入文件夹，而不是制作完成才移动装入。</li><li>为了使相册PPT能够自动循环播放，我们还需要进行音频在后台播放、幻灯片自动翻页、幻灯片放映模式等设置</li></ol><h3 id="05-数据展示"><a href="#05-数据展示" class="headerlink" title="05 数据展示"></a>05 数据展示</h3><h4 id="05-1-数据展示"><a href="#05-1-数据展示" class="headerlink" title="05-1 数据展示"></a>05-1 数据展示</h4><p><strong>数据展示，如何清晰又直观</strong></p><p><img src="https://i.loli.net/2020/08/22/dpAnLXx24BVbkFI.jpg" alt="05-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/ep9z1DnNTu8bdLS.jpg" alt="05-1_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/EQvX4x3dJqDiHZF.jpg" alt="05-1_3.png" loading="lazy"></p><ol><li>对于单个或多个相互独立的数据，通过放大数字的方法进行展示即可</li><li>当我们需要展示的数据/信息比较多的时候，可以通过表格进行数据展示</li><li>数据展示最好的方式是进行可视化处理，也就是将其转化为图表</li></ol><h4 id="05-2-表格使用"><a href="#05-2-表格使用" class="headerlink" title="05-2 表格使用"></a>05-2 表格使用</h4><p><strong>使用表格展示数据</strong></p><p><img src="https://i.loli.net/2020/08/22/LT3WCtqXfwe8HSA.png" alt="05-2_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/O4aAcCb8IoJFthv.jpg" alt="05-2_2.png" loading="lazy"></p><p>表格展示步骤：插入表格 &gt;&gt; 套用央样式 &gt;&gt; 调整细节 &gt;&gt; 版式设计</p><ol><li>插入表格有两种方法拖动鼠标法和手动输入法</li><li>通过套用样式可以给表格快速进行初步的美化</li><li>对表格进行细节调整的时候,其实主要借助布局和设计两大功能</li><li>个专业的表格包括表标题、行标题、列标题、数据、脚注和资料来源6个要素</li></ol><hr><p><strong>真正的数据处理大师是Excel，把Excel中的表格搬到PPT中正确的方法是?</strong></p><ol><li>在Excel中选中表格，按Ctrl+C复制</li><li>在PPT页面按Ctrl+Alt+V调出「选择性粘贴」</li><li>选中粘贴链接 &gt;&gt; Microsoft Excel工作表对象</li><li>点击确定即可</li></ol><p>这种方法的好处是Excel中的数据变化，PPT中表格的数据会同步更新。</p><h4 id="05-3-图表说话"><a href="#05-3-图表说话" class="headerlink" title="05-3 图表说话"></a>05-3 图表说话</h4><p><strong>用图表说话更显专业</strong></p><p>各类型图表的使用示例：</p><p><img src="https://i.loli.net/2020/08/22/ZciwJpBvMDPs9kG.png" alt="05-3_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/LjIHsi5K9QyzNda.png" alt="05-3_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/H3R4oPc6wy72lCZ.png" alt="05-3_3.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/Xb4PRcHwzVESJdO.png" alt="05-3_4.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/vWqcBklQZDNRGwI.png" alt="05-3_5.png" loading="lazy"></p><ol><li>PPT中最常用的图表有柱形图、条形图、饼图、环形图、折线图、雷达图和<br>散点图</li><li>对各类型图表使用的选择（具体见图片）</li></ol><p><img src="https://i.loli.net/2020/08/22/wCSLi6q9oKY2fBX.png" alt="05-3_6.png" loading="lazy"></p><h3 id="06-图示化"><a href="#06-图示化" class="headerlink" title="06 图示化"></a>06 图示化</h3><h4 id="06-1-SmartArt"><a href="#06-1-SmartArt" class="headerlink" title="06-1 SmartArt"></a>06-1 SmartArt</h4><p><strong>搞定图示化只需一个神器</strong></p><p><img src="https://i.loli.net/2020/08/22/vZp87I4f3LJCktU.png" alt="06-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/YLVeSMCGB29FAzf.png" alt="06-1_2.png" loading="lazy"></p><ol><li>图示化展示会让PPT逻辑更加清晰、视觉效果更加直观明了。</li><li>借助PPT中的Smartart可以轻松实现图示化。在选择图示的时侯要根据內容的逻辑关系来选择。</li></ol><h4 id="06-2-四步法"><a href="#06-2-四步法" class="headerlink" title="06-2 四步法"></a>06-2 四步法</h4><p><strong>四步快速做出高大上图示化</strong></p><p><img src="https://i.loli.net/2020/08/22/LqPV96uCxeDNsvo.jpg" alt="06-2.png" loading="lazy"></p><ol><li>四步法做出图示化：梳理逻辑 &gt;&gt; 调整级别 &gt;&gt; 调整细节 &gt;&gt; 版<br>式设计</li><li>调整细节是最重要的一步，包括三个方面的调整：字体、颜色和大小</li></ol><h4 id="06-3-更多用法"><a href="#06-3-更多用法" class="headerlink" title="06-3 更多用法"></a>06-3 更多用法</h4><p><strong>SmartArt 的更多用法（封面、时间轴页、团队介绍）</strong></p><p><img src="https://i.loli.net/2020/08/22/JZwoTC7vaYKSfEi.jpg" alt="06-3_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/g8dmJeltpsbziT3.jpg" alt="06-3_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/22/NVDYpBxl6ARzWXo.jpg" alt="06-3_3.png" loading="lazy"></p><h3 id="07-年会-PPT"><a href="#07-年会-PPT" class="headerlink" title="07 年会 PPT"></a>07 年会 PPT</h3><h4 id="07-1-立体质感"><a href="#07-1-立体质感" class="headerlink" title="07-1 立体质感"></a>07-1 立体质感</h4><p><strong>立体质感文字更带感</strong></p><ol><li>在年会PPT中使用立体文字可以加强PPT的视觉冲击力，很多时候我们就直接使用PPT内置的功能就制作出来。</li><li>更改文字的填充方式、挑选预设的渐变方案、调整渐变方案中渐变光圈游标的颜色都可以使文字的渐变效果发生变化。</li><li>为文字设置三维旋转、添加透视效果可以实现基础的立体感打造。要注意必须先选择“透视”分类下的预设方案,才能激活“透视”数值框,填写具体数值。</li></ol><h4 id="07-2-动画渲染"><a href="#07-2-动画渲染" class="headerlink" title="07-2 动画渲染"></a>07-2 动画渲染</h4><p><strong>开场动画渲染震撼场面</strong></p><ol><li>一套完整的动画效果往往由数十步动画构成；想要学习PPT动画，就要培养自己拆分动画步骤的能力</li><li>强调动画中的”放大缩小“动画，采用的是百分比来设置变化后的对象大小</li><li>在“动画”选项卡右侧的“计时”功能区可以设置动画方案的多个参数。例如动画的开始条件、动画的持续时间、动画的延退时长等</li><li>如果在动画步骤中存在一个占用较长时间的动画,同时有需要其它动画与之并行的话,其它动画之间的先后顺序需要使用“动画延迟”来控制。</li></ol><h4 id="07-3-动画特效"><a href="#07-3-动画特效" class="headerlink" title="07-3 动画特效"></a>07-3 动画特效</h4><p><strong>贯穿全场的动画特效</strong></p><p><img src="https://i.loli.net/2020/08/23/2zc1iKJvWm6hd3w.png" alt="07-3.png" loading="lazy"></p><ol><li>不但形状填充可以设置渐变色，线条也可以设置为渐变线；渐变设置不但可以设置颜色的渐变，还可以综合设置透明度的变化</li><li>想要使页面过渡看起来连贯流畅，有三个要点：色调一致的肖景、引导视线的线条、“平移”切换动画</li><li>要先学会解读教范例、高手作品动画窗格中的动画方案,然后才能逐渐反过来将自己的创作意图转变为动画方案设计</li><li>如果要为多个对象设置相同的动画方案，学会使用动画刷（2013以上版本），能够节约大量时间</li></ol><h4 id="07-4-数据展示"><a href="#07-4-数据展示" class="headerlink" title="07-4 数据展示"></a>07-4 数据展示</h4><p><strong>让业绩数据更耀眼</strong></p><ol><li>滚动数字动画的核心是“平滑”切換与图片裁男只有Office 365的订阅用户才有“平滑”切换效果。</li><li>想要使数字竖向排列,只需要缩减横向文本框的宽度即可。如果直接选择“竖排文本框”输入,数字的方向会出现问题。</li><li>将文字、形状“剪切粘贴为图片”是PPT中非常常见的操作技巧，一般会和”图片裁剪”或“艺术效果“结合起来使用。</li><li>平滑切换会自动判断相邻页面中相同元素的位置、大小等属性的区别，然后做出平滑过渡。而裁剪区域就像一个窗口，让我们只能看到窗框内部的变化。</li></ol><h3 id="08-页面美化"><a href="#08-页面美化" class="headerlink" title="08 页面美化"></a>08 页面美化</h3><h4 id="08-1-四步法"><a href="#08-1-四步法" class="headerlink" title="08-1 四步法"></a>08-1 四步法</h4><p><strong>四步法，PPT秒变高大上</strong></p><p><img src="https://i.loli.net/2020/08/23/qS3h7xjIifbtZUC.png" alt="08-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/YsHXEPhZbV7ULoQ.jpg" alt="08-1_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/i1DXzUtGYk3OL4o.jpg" alt="08-1_3.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/mdAGOu3Rpj4LZwv.jpg" alt="08-1_4.png" loading="lazy"></p><p>丑逼PPT：滥用字体、层次不清、配色难看、图文不符（四步法依次改善）</p><ol><li>四步法包括：统一字体 &gt;&gt; 突出标题 &gt;&gt; 巧取颜色 &gt;&gt; 快速配图。一般情况下将字体统一为微软雅黑系列字体。</li><li>常见的突出标题的方法有放大、加粗、添加项目符号等。巧取颜色，一般从Logo、模板或图片中取色。</li><li>活用四步法,我们可以把页PPT改为很多不同的版式。</li></ol><p>Tips：</p><ol><li>替換字体除了可以框选替换，还可以通过开始栏的“替换”调出“替换字体”来实现，一般用于多页PPT批量替换</li><li>PPT中正文字体颜色的选择（通常）：背景是浅色则正文用深灰，背景是深色则正文用白色</li><li>标题正文的字号设置（通常）：标题字号为正文的1.5倍</li><li>大段文字排版中行间距设置（通常）：1.25~1.5倍最佳</li></ol><h4 id="08-2-进阶应用"><a href="#08-2-进阶应用" class="headerlink" title="08-2 进阶应用"></a>08-2 进阶应用</h4><p><strong>页面美化四步法进阶使用</strong></p><p><strong>人物介绍</strong>：</p><p><img src="https://i.loli.net/2020/08/23/vNIYnSLDKyOx5VH.jpg" alt="08-2_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/qZfteBW2gK3mVIT.jpg" alt="08-2_2.png" loading="lazy"></p><p><strong>教学课件</strong>：</p><p><img src="https://i.loli.net/2020/08/23/831BUFRwHpsXtcq.jpg" alt="08-2_3.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/a73IbywgvmUku4p.jpg" alt="08-2_4.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/LhwgOnPspNvrjby.jpg" alt="08-2_5.png" loading="lazy"></p><p><strong>封面目录</strong>：</p><p><img src="https://i.loli.net/2020/08/23/vrQTcVJeOYXMzxF.jpg" alt="08-2_6.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/ciGAemWz7swPfyr.jpg" alt="08-2_7.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/JM3B9IamVUtqehW.jpg" alt="08-2_8.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/QiH6yLmIaGOs5R2.jpg" alt="08-2_9.png" loading="lazy"></p><ol><li>具体使用四步法的时侯要灵活使用。比如没有Logo的时候，先配图再取色</li></ol><h4 id="08-3-美化表格"><a href="#08-3-美化表格" class="headerlink" title="08-3 美化表格"></a>08-3 美化表格</h4><p><strong>四步法让表格更美观</strong></p><p><img src="https://i.loli.net/2020/08/23/sP7lmxkBYfo3aby.png" alt="08-3_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/JCrq3gQhMfLAs6x.png" alt="08-3_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/BNmaRGswLSpTJ5o.png" alt="08-3_3.png" loading="lazy"></p><ol><li>美化表格的四步法包括：统一字体 &gt;&gt; 突出标题 &gt;&gt; 调整布局 &gt;&gt; 突出重点；突出标题的方法有加粗变色、底纹反衬</li><li>调整布局的时侯通常要将表格内文字居中对齐垂直居中；突出重点最常用的方法是添加半透明底纹</li></ol><h4 id="08-4-美化图表"><a href="#08-4-美化图表" class="headerlink" title="08-4 美化图表"></a>08-4 美化图表</h4><p><strong>四步法让图表更直观</strong></p><p><img src="https://i.loli.net/2020/08/23/E8tQnx3BHmcilNR.png" alt="08-4_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/kc7foFZPLgMKQnA.jpg" alt="08-4_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/23/aWGQBmuMozRX4nN.jpg" alt="08-4_3.png" loading="lazy"></p><ol><li>美化图表的四步法包括：清除样式 &gt;&gt; 清除样式 &gt;&gt; 快速统一 &gt;&gt; 突出重点 &gt;&gt; 创意呈现</li><li>快速统一包括统一字体、配色等；通过改变颜色可以突出图表的重点；创意呈现其实就是将形状图标等复制粘贴到图表上</li></ol><h3 id="09-一些技巧"><a href="#09-一些技巧" class="headerlink" title="09 一些技巧"></a>09 一些技巧</h3><h4 id="09-1-对齐"><a href="#09-1-对齐" class="headerlink" title="09-1 对齐"></a>09-1 对齐</h4><p><strong>做PPT，你对齐了吗？</strong></p><p><img src="https://i.loli.net/2020/08/24/xuH3i79kns28qKL.jpg" alt="09-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/Uf94sed2b7upKLN.jpg" alt="09-1_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/Sg1ncM3DWlOkpRV.jpg" alt="09-1_3.png" loading="lazy"></p><ol><li>PPT为我们提供了8种对齐方式,我们平时最常用的有6种；将常用的对齐工具添加到快速访问工具栏会更加方便我们的操作</li><li>借助参考线可以做好跨页版面的对齐，参考线的基础操作要熟练掌握；使用iSlide插件的智能参考线功能可以帮助我们更快速的建立参考线</li></ol><h4 id="09-2-Logo-页码"><a href="#09-2-Logo-页码" class="headerlink" title="09-2 Logo/页码"></a>09-2 Logo/页码</h4><p><strong>批量添加Logo和页码</strong></p><ol><li>使用PPT母版视图可以批量添加Logo；添加Logo要注意可能会出现Logo被图片/色块挡住的情况</li><li>通过插入幻灯片编号可以给PPT批量添加页码;通过自定义幻灯片大小中的更改幻灯片编号起始值可以设置从第二页开始添加页码</li></ol><h4 id="09-3-快捷键"><a href="#09-3-快捷键" class="headerlink" title="09-3 快捷键"></a>09-3 快捷键</h4><p><strong>PPT中必会的快捷键</strong></p><p><img src="https://i.loli.net/2020/08/24/DNOBWTa4gxvofkQ.jpg" alt="09-3.png" loading="lazy"></p><p><strong>基本功能键</strong>：</p><p>Ctrl+S：保存</p><p>Ctrl+X/C/V：剪切/复制/粘贴</p><p>Ctrl+D：复制+粘贴</p><p>Ctrl+N：新建一套演示文稿</p><p>Ctrl+M：新建一页幻灯片</p><p>Ctrl+Z：撤销操作</p><p>Ctrl+Y：重做操作</p><p>F4：重复上一次操作</p><p><strong>对象快捷键</strong>：</p><p>Ctrl+G：组合</p><p>Ctrl+左键单击：多选</p><p>Ctrl+左键拖动：复制</p><p>Shift+缩放：等比缩放</p><p>Shift+移动：锁定方向移动</p><p>Ctrl+Shift+拖动裁剪框：镜像对称的裁剪（裁剪模式下）</p><p><strong>文本快捷键</strong>：</p><p>Ctrl+B：加粗</p><p>Ctrl+[/Ctrl+]：放大/缩小字号</p><p><strong>放映快捷键</strong>：</p><p>F5：从头放映幻灯片</p><p>Shift+F5：从当前页放映幻灯片</p><h3 id="10-一些工具"><a href="#10-一些工具" class="headerlink" title="10 一些工具"></a>10 一些工具</h3><h4 id="10-1-iSlide"><a href="#10-1-iSlide" class="headerlink" title="10-1 iSlide"></a>10-1 iSlide</h4><p><strong>iSlide一站式解决PPT素材问题</strong></p><p>图片库、插图库、图标库、智能图表</p><p><img src="https://i.loli.net/2020/08/24/eigm6oCZvcPJAn4.png" alt="10-1_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/2VFYMJ5AQbhwHfR.png" alt="10-1_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/r9z3mndLbtWH8wU.png" alt="10-1_3.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/Z7GH5i9w6YmtDMA.jpg" alt="10-1_4.png" loading="lazy"></p><h4 id="10-2-其他功能"><a href="#10-2-其他功能" class="headerlink" title="10-2 其他功能"></a>10-2 其他功能</h4><p><strong>iSlide其他实用功能</strong></p><p>一键优化、设计排版、PPT导出</p><h4 id="10-3-综合应用"><a href="#10-3-综合应用" class="headerlink" title="10-3 综合应用"></a>10-3 综合应用</h4><p><strong>iSlide综合应用</strong></p><p><img src="https://i.loli.net/2020/08/24/gI78exSK52XzMm6.png" alt="10-3.png" loading="lazy"></p><p>基于iSlide的三步法快速做出一套风格统一、逻辑清晰的PPT：选择主题 &gt;&gt; 插入图示 &gt;&gt; 选择配色</p><ol><li>正式开始做PPT之前要对文字稿进行结构化梳理；选择主题库可以一键搞定封面页和结束页,主题决定了PPT的风格</li><li>插入图示的时候一定要根据内容的逻辑关系来选择；转场页和目录页需要我们手动输入</li><li>色彩库有很多分类详细的配色方案，点击应用到全部页面可以一键替换</li></ol><h4 id="10-4-美化大师"><a href="#10-4-美化大师" class="headerlink" title="10-4 美化大师"></a>10-4 美化大师</h4><p><strong>美化大师，PPT素材宝库</strong></p><p>图片库、形状库、资源广场</p><p><img src="https://i.loli.net/2020/08/24/zVnaWDFw58NfKEP.png" alt="10-4_1.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/JlI96ciREqWXGNd.png" alt="10-4_2.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/sH1N8AhkwGdCFV3.png" alt="10-4_3.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/P6zYCXk7BoZuy2t.png" alt="10-4_4.png" loading="lazy"></p><p><img src="https://i.loli.net/2020/08/24/7IF8epSWAJH3Ero.png" alt="10-4_5.png" loading="lazy"></p><h4 id="10-5-提升效率"><a href="#10-5-提升效率" class="headerlink" title="10-5 提升效率"></a>10-5 提升效率</h4><p><strong>美化大师，提升效率的利器</strong></p><p><img src="https://i.loli.net/2020/08/24/c3yCt6svQL8rpB9.png" alt="10-5.png" loading="lazy"></p><ol><li>使用美化大师的内容规划和魔法換装功能我们可以快速做出一套PPT。</li></ol><h2 id="工具-网站记录"><a href="#工具-网站记录" class="headerlink" title="工具/网站记录"></a>工具/网站记录</h2><p><a href="http://www.zhaozi.cn/" target="_blank" rel="noopener">找字网</a>：根据类型查找字体</p><p><a href="http://www.qiuziti.com/" target="_blank" rel="noopener">求字体</a>：根据截图查找字体</p><p><a href="https://unsplash.com/" target="_blank" rel="noopener">Unsplash</a>：查找图片的资源网站</p><p><a href="https://music.163.com/" target="_blank" rel="noopener">网易云音乐</a>：寻找优质背景音乐（按照情绪等关键词搜索歌单）</p><p><a href="http://www.aigei.com/" target="_blank" rel="noopener">爱给网</a>：寻找优质背景音乐</p><p><a href="https://www.alltoall.net/" target="_blank" rel="noopener">ALL TO ALL</a>：在线音频格式转换</p><p><a href="https://www.pexels.com/zh-cn/" target="_blank" rel="noopener">Pexels</a>：可商用图片素材库</p><p><a href="https://pixabay.com/" target="_blank" rel="noopener">Pixabay</a>：可商用图片素材库</p><p><a href="https://www.islide.cc/" target="_blank" rel="noopener">iSlide</a>：基于PowerPoint的插件工具</p><p><a href="http://meihua.docer.com/" target="_blank" rel="noopener">美化大师</a>：PPT素材宝库、一键美化</p><p><a href="http://www.papocket.com/" target="_blank" rel="noopener">口袋动画</a>：一款让PPT动画炫酷起来的插件</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.bilibili.com/video/BV12A411t7Q7" target="_blank" rel="noopener">秋叶学习视频__新手到高手PPT篇</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 高效 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Office </tag>
            
            <tag> PowerPoint </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【转载】GIF 图示：Excel 超强技巧</title>
      <link href="efficiency/gif-super-skills-of-excel/"/>
      <url>efficiency/gif-super-skills-of-excel/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Excel 超强技巧，看 GIF 图秒懂。Mark 一下方便以后查看。</p><a id="more"></a><h2 id="图解-Excel-技巧"><a href="#图解-Excel-技巧" class="headerlink" title="图解 Excel 技巧"></a>图解 Excel 技巧</h2><ol><li>Ctrl + C/V：复制&amp;粘贴区域</li></ol><p><img src="https://i.loli.net/2020/08/17/65aQCpsfqmowB1T.gif" alt="image-1.gif" loading="lazy"></p><ol start="2"><li>Ctrl + X/V：剪切&amp;粘贴区域</li></ol><p><img src="https://i.loli.net/2020/08/17/Un3IMq87CBvziWw.gif" alt="image-2.gif" loading="lazy"></p><ol start="3"><li>Ctrl + Z：撤销上一个操作</li></ol><p><img src="https://i.loli.net/2020/08/17/nUjuN6zpY1VZJqM.gif" alt="image-3.gif" loading="lazy"></p><ol start="4"><li>F4：重复执行上一命令或操作</li></ol><p><img src="https://i.loli.net/2020/08/17/YDV9Hpq7ofh5U36.gif" alt="image-4.gif" loading="lazy"></p><ol start="5"><li>Ctrl + F：显示查找功能菜单</li></ol><p><img src="https://i.loli.net/2020/08/17/ajiIltVwHKPNCxX.jpg" alt="image-5.jpeg" loading="lazy"></p><ol start="6"><li><p>Ctrl + S：保存工作簿</p></li><li><p>F12：工作簿另存为</p></li></ol><p><img src="https://i.loli.net/2020/08/17/OtgGmaV13z2Whwy.gif" alt="image-7.gif" loading="lazy"></p><ol start="8"><li>Ctrl + P：打印</li></ol><p><img src="https://i.loli.net/2020/08/17/PCkzat7OjRKrdw6.gif" alt="image-8.gif" loading="lazy"></p><ol start="9"><li>Ctrl + F1：显示或隐藏功能区</li></ol><p><img src="https://i.loli.net/2020/08/17/pLth4OgXRnHaWFz.gif" alt="image-9.gif" loading="lazy"></p><ol start="10"><li>Alt：功能区上显示快捷键提示</li></ol><p><img src="https://i.loli.net/2020/08/17/gePzncJImRLUw57.gif" alt="image-10.gif" loading="lazy"></p><ol start="11"><li>拖放：剪切并粘贴</li></ol><p><img src="https://i.loli.net/2020/08/17/rV2KWdOEGaSlktC.gif" alt="image-11.gif" loading="lazy"></p><ol start="12"><li>Ctrl + 拖放：复制并粘贴</li></ol><p><img src="https://i.loli.net/2020/08/17/Uu8wXdioNl17jVv.gif" alt="image-12.gif" loading="lazy"></p><ol start="13"><li>Shift + 拖放：移动并插入</li></ol><p><img src="https://i.loli.net/2020/08/17/6KgoyuZwfeBAQHI.gif" alt="image-13.gif" loading="lazy"></p><ol start="14"><li>Ctrl + A：选中当前区域或整个工作簿</li></ol><p><img src="https://i.loli.net/2020/08/17/qZNgkCXwsKBVaR2.gif" alt="image-14.gif" loading="lazy"></p><ol start="15"><li>Shift + 点击：选择相邻区域</li></ol><p><img src="https://i.loli.net/2020/08/17/2bgCjvlq4m5UIER.gif" alt="image-15.gif" loading="lazy"></p><ol start="16"><li>Ctrl + 选择：选择不相邻的区域</li></ol><p><img src="https://i.loli.net/2020/08/17/5cCZiltQxLXERyD.gif" alt="image-16.gif" loading="lazy"></p><ol start="17"><li>Shift + 方向键：选择区域扩展一个单元格</li></ol><p><img src="https://i.loli.net/2020/08/17/BJgvrTuaYn48cwt.gif" alt="image-17.gif" loading="lazy"></p><ol start="18"><li>Ctrl + Shift + 方向键：选择区域扩展到最后一个非空单元格</li></ol><p><img src="https://i.loli.net/2020/08/17/5WmHOBv7YgJoPLF.gif" alt="image-18.gif" loading="lazy"></p><ol start="19"><li>Ctrl + G：显示“定位”菜单</li></ol><p><img src="https://i.loli.net/2020/08/17/M6k5bQdrGTlCYLt.gif" alt="image-19.gif" loading="lazy"></p><ol start="20"><li>Alt + ；：选择可见单元格，使用此快捷键，选择当前选定范围内的可见单元格。被隐藏的行和列单元格不会被选中。</li></ol><p><img src="https://i.loli.net/2020/08/17/AGRNBrDtpghU8bH.gif" alt="image-20.gif" loading="lazy"></p><ol start="21"><li>F2：编辑活动单元格，使用此快捷键，进入单元格编辑状态，光标插入到单元格内容的末尾。</li></ol><p><img src="https://i.loli.net/2020/08/17/zIjuBTmhsxlkyt2.gif" alt="image-21.gif" loading="lazy"></p><ol start="22"><li>ESC：取消编辑，使用此快捷键，取消编辑，在此之前进行的编辑不会生效。</li></ol><p><img src="https://i.loli.net/2020/08/17/2gjEFZRICAxPLrn.gif" alt="image-22.gif" loading="lazy"></p><ol start="23"><li>Alt + Enter：单元格内换行。</li></ol><p><img src="https://i.loli.net/2020/08/17/E8pPFrZ61M3YINq.gif" alt="image-23.gif" loading="lazy"></p><ol start="24"><li>Ctrl + Enter：在选定的多个单元格输入相同的数据</li></ol><p><img src="https://i.loli.net/2020/08/17/OxZWJ5IagERQokf.gif" alt="image-24.gif" loading="lazy"></p><ol start="25"><li>Ctrl + D：将活动单元格上一行数据复制到活动单元格。</li></ol><p><img src="https://i.loli.net/2020/08/17/BmxpXeq5jRNFCr2.gif" alt="image-25.gif" loading="lazy"></p><ol start="26"><li>Ctrl + R：将活动单元格左侧数据复制到活动单元格。</li></ol><p><img src="https://i.loli.net/2020/08/17/CFgQWPLv4ap3tVj.gif" alt="image-26.gif" loading="lazy"></p><ol start="27"><li>Alt + ↓：显示下拉列表。</li></ol><p><img src="https://i.loli.net/2020/08/17/aQU3FTgbYIlB926.gif" alt="image-27.gif" loading="lazy"></p><ol start="28"><li>Ctrl + 1：显示设置单元格格式菜单</li></ol><p><img src="https://i.loli.net/2020/08/17/FziL5jvr3HUSN6K.gif" alt="image-28.gif" loading="lazy"></p><ol start="29"><li>Ctrl + Shift + ~：应用常规数字格式</li></ol><p><img src="https://i.loli.net/2020/08/17/U853ZJvjOwkLRu4.gif" alt="image-29.gif" loading="lazy"></p><h2 id="转载声明"><a href="#转载声明" class="headerlink" title="转载声明"></a>转载声明</h2><p>本文转自网络文章 <a href="https://baijiahao.baidu.com/s?id=1644278506153266286&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">Excel教程：Excel超强技巧，看图秒懂</a>，转载仅为个人收藏、知识分享，版权归原作者所有。如有侵权，请联系<a href="mailto:atomicoo95@gmail.com">博主</a>进行删除。</p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 高效 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Excel </tag>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数学天坑】之反向传播算法数学原理</title>
      <link href="mathematics/principle-of-back-propagation/"/>
      <url>mathematics/principle-of-back-propagation/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>神经网络的训练主要由三个部分组成：1) 网络模型；2) 损失函数；3) 参数学习算法。</p><p>我们今天的主角，<strong>反向传播算法</strong>（Back Propagation Algorithm，BP Algorithm），常与参数优化算法（譬如梯度下降法等）结合使用，属于参数学习算法的一部分（或者说是参数学习算法的好搭档，取决于你怎么理解“参数学习算法”这一概念）。该算法会对网络中所有权重计算损失函数的梯度，并反馈给参数优化算法用以更新权重。</p><p>对于反向传播算法，有两个常见的误区在这里澄清一下：</p><ul><li>反向传播算法是<strong>梯度计算方法</strong>，而不是参数学习算法</li><li>反向传播算法理论上可以用于计算<strong>任何函数</strong>的梯度，而不是仅仅用于多层神经网络</li></ul><p>本文会从“<strong>维度相容</strong>”的角度介绍快速矩阵求导和反向传播算法。</p><a id="more"></a><h2 id="抛砖引玉"><a href="#抛砖引玉" class="headerlink" title="抛砖引玉"></a>抛砖引玉</h2><p>先用一个例子来抛砖引玉。</p><p>已知：$J=(X \bold{\omega}-\bold{y})^{T} (X \bold{\omega}-\bold{y})=|X \bold{\omega}-\bold{y}|^{2}$，其中 $X \in \R^{m \times n}, \bold{\omega} \in \R^{n}, \bold{y} \in \R^{m}$。</p><p>求 $\cfrac{\partial{J}}{\partial{X}}, \cfrac{\partial{J}}{\partial{\bold{\omega}}}, \cfrac{\partial{J}}{\partial{\bold{y}}}$？</p><p>熟悉的老铁们可能已经发现了，这其实就是线性回归的矩阵表示形式。在实践时，我们通常只会对 $\bold{\omega}$ 求导来进行最小二乘估计，但在这里因为本例只是作为一个引子，所以会对 $X, \bold{\omega}, \bold{y}$ 都进行求导，并无实际意义。</p><p>反向传播算法的关键在于“<strong>链式求导法则</strong>”：若函数 $f, g$ 可导，则 $(f \circ g)^{‘}(x)=f^{‘}(g(x))g^{‘}(x)$。</p><h2 id="快速矩阵求导"><a href="#快速矩阵求导" class="headerlink" title="快速矩阵求导"></a>快速矩阵求导</h2><p>这一节将介绍如何从“维度相容”的角度进行快速矩阵求导，其核心在于<strong>维度相容原则</strong>。那么，什么是维度相容原则？</p><p><strong>维度相容原则</strong>：通过<strong>换序</strong>、<strong>转置</strong>操作使求导结果满足维度条件。</p><blockquote><p>关于矩阵求导结果需要满足的维度条件，涉及到矩阵求导布局的问题，具体可以参考我之前的博客 <a href="https://atomicoo.com/theory/matrix-vector-derivation-1/">【数学天坑】之矩阵/向量求导（一）</a>。<br>另外，维度相容实际上是多元微分中的知识，我在之前的博客中也说过，<strong>矩阵求导本质就是多元函数求导</strong>，这也就很容易理解为什么可以把维度相容“移植”过来使用了，毕竟知识都是相通的。</p></blockquote><p>从维度相容角度进行快速矩阵求导说起来就四个字“简单粗暴”，不够严谨但足够好用。套用一下“爆炸即艺术”的句式，那就是——<strong>简单即艺术</strong>！</p><p>快速矩阵求导共有两个步骤，我们用上一节的例子来进行说明。</p><p>步骤一：把所有参数看成是标量来进行求导。</p><p>显然，这一步哪怕是中学生也能轻松完成，$J=(X \bold{\omega}-\bold{y})^{2}$，由链式法则，按照标量方式求导的结果：</p><p>$$ \begin{aligned} \cfrac{\partial{J}}{\partial{X}}=2 (X \bold{\omega}-\bold{y}) \bold{\omega} \\ \cfrac{\partial{J}}{\partial{\bold{\omega}}}=2 (X \bold{\omega}-\bold{y}) X \\ \cfrac{\partial{J}}{\partial{\bold{y}}}=-2 (X \bold{\omega}-\bold{y}) \end{aligned} $$</p><p>检查求导结果的维度，除了 $\cfrac{\partial{J}}{\partial{\bold{y}}}$ 之外，$\cfrac{\partial{J}}{\partial{X}}$ 和 $\cfrac{\partial{J}}{\partial{\bold{\omega}}}$ 都是不满足维度条件的。</p><p>步骤二：利用换序、转置操作调整维度满足条件。</p><p>考虑矩阵求导维度，有：</p><p>$$ \cfrac{\partial{J}}{\partial{X}} \in \R^{m \times n}, \cfrac{\partial{J}}{\partial{\bold{\omega}}} \in \R^{n} $$</p><p>根据维度相容原则：</p><p>$$ \cfrac{\partial{J}}{\partial{X}} \in \R^{m \times n}, (X \bold{\omega}-\bold{y}) \in \R^{m}, \bold{\omega} \in \R^{n} $$</p><p>通过换序、转置操作可得：</p><p>$$ \cfrac{\partial{J}}{\partial{X}}=2 (X \bold{\omega}-\bold{y}) \bold{\omega}^{T} $$</p><p>满足维度条件。同理可得：</p><p>$$ \cfrac{\partial{J}}{\partial{\bold{\omega}}}=2 X^{T} (X \bold{\omega}-\bold{y}) $$</p><p>简单总结下维度相容快速矩阵求导：</p><ul><li>步骤一：把所有参数看成是标量来进行求导</li><li>步骤二：利用换序、转置操作调整维度满足条件</li></ul><h2 id="快速反向传播"><a href="#快速反向传播" class="headerlink" title="快速反向传播"></a>快速反向传播</h2><p>前面已经提到过，反向传播算法的关键在于“<strong>链式求导法则</strong>”，在反向时需要一层一层地往前进行链式求导。</p><p>第 $l$ 层的前向传播过程：</p><p>$$ \begin{aligned} \bold{z}^{(l+1)}&amp;=W^{(l)} \bold{a}^{(l)}+\bold{b}^{(l)} \\ \bold{a}^{(l+1)}&amp;=f(\bold{z}^{(l+1)}) \end{aligned} $$</p><p>其中，$\bold{a}^{(l)}$ 为第 $l$ 层的输入，$W^{(l)}, \bold{b}^{(l)}$ 为第 $l$ 层的参数，$\bold{z}^{(l+1)}$ 为第 $l$ 层的中间结果，$f(\cdot)$ 为激活函数，$\bold{a}^{(l+1)}$ 为第 $l$ 层的激活值（也是第 $l$ 层的输出）。</p><p>设损失函数为 $J(W, \bold{b}) \in \R$（这里不做具体定义，$J$ 可以为任意损失函数），由链式法则：</p><p>$$ \begin{aligned} \cfrac{\partial{J}}{\partial{W^{(l)}}}&amp;=\cfrac{\partial{J}}{\partial{\bold{z}^{(l+1)}}} \cfrac{\partial{\bold{z}^{(l+1)}}}{\partial{W^{(l)}}}=\bold{\delta}^{(l+1)} (\bold{a}^{(l)})^{T} \\ \cfrac{\partial{J}}{\partial{\bold{b}^{(l)}}}&amp;=\cfrac{\partial{J}}{\partial{\bold{z}^{(l+1)}}} \cfrac{\partial{\bold{z}^{(l+1)}}}{\partial{\bold{b}^{(l)}}}=\bold{\delta}^{(l+1)} \end{aligned} $$</p><p>以上，为方便书写，记 $\bold{\delta}^{(l)} \triangleq \cfrac{\partial{J}}{\partial{\bold{z}^{(l)}}}$。考虑矩阵求导维度条件：</p><p>$$\cfrac{\partial{J}}{\partial{W^{(l)}}} \in \R^{s^{(l+1)} \times s^{(l)}}, \bold{\delta}^{(l+1)}=\cfrac{\partial{J}}{\partial{\bold{z}^{(l+1)}}} \in \R^{s^{(l+1)}}$$</p><p>其中，$s^{(l+1)}$ 为第 $l$ 层中间结果与激活值的维度，即 $\bold{a}^{(l+1)}, \bold{z}^{(l+1)} \in \R^{s^{(l+1)}}$。</p><p>所以根据<strong>维度相容原则</strong>增加转置符号调整 $\bold{a}^{(l)}$ 为 $(\bold{a}^{(l)})^{T}$。</p><p>接下来就只需要搞定 $\bold{\delta}^{(l)}$ 即可。那么应该如何求解 $\bold{\delta}^{(l)}$？</p><p>因为在前向传播过程中，链式依赖关系为 $\cdots \to \bold{z}^{(l)} \to \bold{a}^{(l)} \to \bold{z}^{(l+1)} \to \bold{a}^{(l+1)} \to \cdots$，所以可以用递推的方式依次求解：</p><p>$$ \bold{\delta}^{(l)}=\cfrac{\partial{J}}{\partial{\bold{z}^{(l)}}}=\cfrac{\partial{J}}{\partial{\bold{z}^{(l+1)}}} \cfrac{\partial{\bold{z}^{(l+1)}}}{\partial{\bold{a}^{(l)}}} \cfrac{\partial{\bold{a}^{(l)}}}{\partial{\bold{z}^{(l)}}} $$</p><p>考虑矩阵求导维度：</p><p>$$ \begin{aligned} \cfrac{\partial{J}}{\partial{\bold{z}^{(l+1)}}}&amp;=\bold{\delta}^{(l+1)} \in \R^{s^{(l+1)}} \\ \cfrac{\partial{\bold{z}^{(l+1)}}}{\partial{\bold{a}^{(l)}}}&amp;=W^{(l)} \in \R^{s^{(l+1)} \times s^{(l)}} \\ \cfrac{\partial{\bold{a}^{(l)}}}{\partial{\bold{z}^{(l)}}}&amp;=f^{‘}(\bold{z}^{(l)}) \in \R^{s^{(l)}} \end{aligned}$$</p><p>根据<strong>维度相容原则</strong>，调整结果得：</p><p>$$ \bold{\delta}^{(l)}=((W^{(l)})^{T} \bold{\delta}^{(l+1)}) \odot f^{‘}(\bold{z}^{(l)}) $$</p><p>注意，$\bold{a}^{(l)}=f(\bold{z}^{(l)})$ 是逐元素运算，因此 $\cfrac{\partial{\bold{a}^{(l)}}}{\partial{\bold{z}^{(l)}}}=f^{‘}(\bold{z}^{(l)})$ 也是逐元素求导的形式，在公式中使用 <strong>Hadamard 积</strong>。</p><p>至此，反向传播算法的推导已经全部完成，整理整个反向传播过程如下：</p><ul><li>利用前向传播公式进行前向传播计算，得到各层的激活值 $\bold{a}^{(l)}$</li><li>对于输出层（第 $L$ 层），计算 $\bold{\delta}{(L)}$</li><li>对于隐藏层，利用递推公式依次计算 $\bold{\delta}{(l)}$</li><li>计算各层参数 $W^{(l)}, \bold{b}^{(l)}$ 的偏导</li></ul><blockquote><p>前文中曾经提过，反向传播算法作为梯度计算方法，需要配合参数优化算法（譬如随机梯度下降法）才能用于训练神经网络参数，本文并不会介绍关于参数优化算法的内容，但后续我会找时间专门开一篇博客来介绍相关内容。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从<strong>维度相容</strong>的角度进行快速矩阵求导遵循“<strong>简单即艺术</strong>”的信条，在数学证明上显然并不严谨，但对于机器学习相关领域的学习者来说已经足以应付大多数情况。当然，对于想要更进一步的人来说，还是需要脚踏实地地去查阅各种相关资料，将矩阵求导方面的知识完全吃透。而搞定了矩阵求导，包括<strong>反向传播算法</strong>在内的许多算法的推导也将迎刃而解。</p><blockquote><p>Ps：本文的主角以其说是反向传播算法，倒不如说是“维度相容”，在整个推导过程中“维度相容”贯通始终。</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="noopener">神经网络反向传播的数学原理</a></p><p><a href="https://zhuanlan.zhihu.com/p/25202034" target="_blank" rel="noopener">神经网络反向传播时的梯度到底怎么求？</a></p><p><a href="https://zhuanlan.zhihu.com/p/66534632" target="_blank" rel="noopener">神经网络15分钟 - 反向传播到底是怎么传播的？</a></p><p><a href="https://zhuanlan.zhihu.com/p/71892752" target="_blank" rel="noopener">前向传播算法和反向传播算法（BP算法）及其推导</a></p><p><a href="https://zhuanlan.zhihu.com/p/39195266" target="_blank" rel="noopener">反向传播算法推导-全连接神经网络</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> BP算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数学天坑】之矩阵/向量求导（四）</title>
      <link href="mathematics/matrix-vector-derivation-4/"/>
      <url>mathematics/matrix-vector-derivation-4/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>这是【数学天坑】系列之矩阵/向量求导的第四部分。</p><p>在上一篇文章的总结部分提到过一点，微分法虽然很大程度上避免了定义法的局限性，但在面对复杂链式求导的情况时仍然会很麻烦，因此还需要一种更优的方法，那就是：<strong>链式求导法</strong>。</p><p>链式求导法，对于在机器学习中可能遇到的绝大多数复杂链式求导，我们只需记忆一些常用的求导结果，然后再利用链式法则来求解即可。</p><a id="more"></a><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><h3 id="向量对向量"><a href="#向量对向量" class="headerlink" title="向量对向量"></a>向量对向量</h3><p>若有 $\bold{x} \in \R^{n}, \bold{y} \in \R^{m}, \bold{z} \in \R^{p}$，且三者间存在链式关系 $f: \bold{x} \to \bold{y} \to \bold{z}$，则有如下链式求导法则：</p><p>$$ \cfrac{\partial{\bold{z}}}{\partial{\bold{x}}}=\cfrac{\partial{\bold{z}}}{\partial{\bold{y}}} \cfrac{\partial{\bold{y}}}{\partial{\bold{x}}} \tag{1.1} $$</p><p>注意几个求导结果的维度，$\cfrac{\partial{\bold{z}}}{\partial{\bold{x}}} \in \R^{p \times n}, \cfrac{\partial{\bold{z}}}{\partial{\bold{y}}} \in \R^{p \times m}, \cfrac{\partial{\bold{y}}}{\partial{\bold{x}}} \in \R^{m \times n}$，从<strong>维度相容</strong>的角度可以快速求解。</p><p>同样的结论可以推广到更多的向量、更长的链式依赖关系 $f: \bold{x} \to \bold{y_{1}} \to \cdots \to \bold{y_{q}} \to \bold{z}$，记 $\bold{x} \triangleq \bold{y_{0}}, \bold{z} \triangleq \bold{y_{q+1}}$，则：</p><p>$$ \cfrac{\partial{\bold{z}}}{\partial{\bold{x}}}=\cfrac{\partial{\bold{y_{q+1}}}}{\partial{\bold{y_{0}}}}=\displaystyle\prod_{i=1}^{q+1}{\cfrac{\partial{\bold{y_{i}}}}{\partial{\bold{y_{i-1}}}}} \tag{1.2} $$</p><h3 id="标量对向量"><a href="#标量对向量" class="headerlink" title="标量对向量"></a>标量对向量</h3><p>若有 $\bold{x} \in \R^{n}, \bold{y} \in \R^{m}, \bold{z} \in \R$，且三者间存在链式关系 $f: \bold{x} \to \bold{y} \to z$，则此时 $\cfrac{\partial{z}}{\partial{\bold{x}}} \in \R^{n}, \cfrac{\partial{z}}{\partial{\bold{y}}} \in \R^{m}, \cfrac{\partial{\bold{y}}}{\partial{\bold{x}}} \in \R^{m \times n}$，有：</p><p>$$ \cfrac{\partial{z}}{\partial{\bold{x}}}=((\cfrac{\partial{z}}{\partial{\bold{y}}})^{T} \cfrac{\partial{\bold{y}}}{\partial{\bold{x}}})^{T}=(\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}})^{T} \cfrac{\partial{z}}{\partial{\bold{y}}} \tag{2.1} $$</p><p>同样的，该结论可以推广到更多的向量、更长的链式依赖关系 $f: \bold{x} \to \bold{y_{1}} \to \cdots \to \bold{y_{q}} \to z$，记 $\bold{x} \triangleq \bold{y_{0}}$，则：</p><p>$$ \cfrac{\partial{z}}{\partial{\bold{x}}}=\cfrac{\partial{z}}{\partial{\bold{y_{0}}}}=(\displaystyle\prod_{i=1}^{q}{\cfrac{\partial{\bold{y_{i}}}}{\partial{\bold{y_{i-1}}}}})^{T} \cfrac{\partial{z}}{\partial{\bold{y}}} \tag{2.2} $$</p><p>可以看到，在整个链式关系中，除了 $\bold{y_{q}} \to z$ 的部分，其他部分其实就跟上一种情况（向量对向量）是完全一样的。</p><h3 id="标量对矩阵"><a href="#标量对矩阵" class="headerlink" title="标量对矩阵"></a>标量对矩阵</h3><p>矩阵对矩阵求导的情况是比较复杂的，因此在前几篇博客中并没有过多地关注，在本文我也并不打算过于深入的研究，但还是可以对机器学习中常见的矩阵对矩阵求导的类型做一些分析。</p><p>先回归定义法。若有 $X \in \R^{m \times n}, Y \in \R^{p \times q}, z \in \R$，且三者间存在链式关系 $f: X \to Y \to z$，则 $z$ 对 $X$ 中任一分量求导：</p><p>$$ \cfrac{\partial{z}}{\partial{x_{ij}}}=\displaystyle\sum_{k,l}{\cfrac{\partial{z}}{\partial{y_{kl}}} \cfrac{\partial{y_{kl}}}{\partial{x_{ij}}}}=tr((\cfrac{\partial{z}}{\partial{Y}})^{T} \cfrac{\partial{Y}}{\partial{x_{ij}}}) \tag{3} $$</p><p>机器学习中一个常见的问题：$z=f(Y), Y=A X+B$，求解 $\cfrac{\partial{z}}{\partial{X}}$。</p><p>考虑 $(3)$ 式中的 $\cfrac{\partial{y_{kl}}}{\partial{x_{ij}}}$：</p><p>$$ \cfrac{\partial{y_{kl}}}{\partial{x_{ij}}}=\cfrac{\partial{\displaystyle\sum_{s}{a_{ks} x_{sl}}}}{\partial{x_{ij}}}=\cfrac{\partial{a_{ki} x_{il}}}{\partial{x_{ij}}}=a_{ki} \delta_{lj} $$</p><p>其中，$\delta_{lj}=\begin{cases} 1 &amp;\text{if } l = j \\ 0 &amp;\text{if } l \neq j \end{cases}$</p><p>进而可得：</p><p>$$ \cfrac{\partial{z}}{\partial{x_{ij}}}=\displaystyle\sum_{k,l}{\cfrac{\partial{z}}{\partial{y_{kl}}} a_{ki} \delta_{lj}}=\displaystyle\sum_{k}{a_{ki} \cfrac{\partial{z}}{\partial{y_{kj}}}}=\langle A_{,i}, \cfrac{\partial{z}}{\partial{Y_{,j}}} \rangle $$</p><p>按照定义排列成矩阵形式：</p><p>$$ \cfrac{\partial{z}}{\partial{X}}=A^{T} \cfrac{\partial{z}}{\partial{Y}} $$</p><p>因此，对于这种常见类型，可以直接记忆其结论：</p><p>$$ z=f(Y), Y=A X+B \implies \cfrac{\partial{z}}{\partial{X}}=A^{T} \cfrac{\partial{z}}{\partial{Y}} $$</p><p>此外还有几个常见类型：</p><p>$$ z=f(\bold{y}), \bold{y}=A \bold{x}+\bold{b} \implies \cfrac{\partial{z}}{\partial{\bold{x}}}=A^{T} \cfrac{\partial{z}}{\partial{\bold{y}}} $$</p><p>$$ z=f(Y), Y=X A+B \implies \cfrac{\partial{z}}{\partial{X}}=\cfrac{\partial{z}}{\partial{Y}} A^{T} $$</p><p>$$ z=f(\bold{y}), \bold{y}=X \bold{a}+\bold{b} \implies \cfrac{\partial{z}}{\partial{X}}=\cfrac{\partial{z}}{\partial{\bold{y}}} \bold{a}^{T} $$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，关于矩阵/向量求导已记录了四篇博客，搞定了矩阵/向量求导的三种方法：定义法、微分法和链式求导法。关于链式求导法，重点分析了向量对向量、标量对向量的类型，针对标量对矩阵的类型，考虑到矩阵对矩阵求导的复杂性，只分析了机器学习中常见的几种情况。在掌握这四篇博客内容的前提下，应付机器学习中多数的矩阵求导应该是没有问题的了。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/pinard/p/10825264.html" target="_blank" rel="noopener">机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则</a></p><p><a href="https://zhuanlan.zhihu.com/p/46908990" target="_blank" rel="noopener">机器学习中的矩阵求导技术</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数学天坑】之矩阵/向量求导（三）</title>
      <link href="mathematics/matrix-vector-derivation-3/"/>
      <url>mathematics/matrix-vector-derivation-3/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>这是【数学天坑】系列之矩阵/向量求导的第三部分。</p><p>在前两篇文章中，已经介绍了矩阵/向量求导的定义、求导布局以及定义法求导的内容，同时通过几个例子说明了定义法求导的局限性，因此我们需要寻找一种更优的方法来完成矩阵/向量求导的任务。这篇博客将从导数与微分的关系出发，引出矩阵求导的<strong>微分法</strong>。</p><a id="more"></a><h2 id="导数微分"><a href="#导数微分" class="headerlink" title="导数微分"></a>导数微分</h2><p>在高等数学中，我们曾经学过一元函数的导数与微分的关系，那么这样的关系是否能推广到多元函数中？而又能否利用这样的关系来进行矩阵求导？显然，两个答案都是肯定的。</p><h3 id="导数微分关系"><a href="#导数微分关系" class="headerlink" title="导数微分关系"></a>导数微分关系</h3><p>现在先来回顾下高数里的知识，在单变量微积分中，导数与微分的关系：</p><p>$$ df=f^{‘}(x)dx=\cfrac{df}{dx} dx \tag{1} $$</p><p>据此可以写出多变量时的情况（对应标量对向量求导）：</p><p>$$ df=\displaystyle\sum_{i=1}^{n}{\cfrac{\partial{f}}{\partial{x_{i}}} dx_{i}}=(\cfrac{\partial{f}}{\partial{\bold{x}}})^{T} d\bold{x} \tag{2} $$</p><p>进一步可以推广到矩阵（对应标量对矩阵求导）：</p><p>$$ df=\displaystyle\sum_{i=1}^{m}\displaystyle\sum_{j=1}^{n}{\cfrac{\partial{f}}{\partial{X_{ij}}} dX_{ij}}=tr((\cfrac{\partial{f}}{\partial{X}})^{T} dX) \tag{3} $$</p><p>这里使用了矩阵迹的一个性质：</p><p>$$ tr(A^{T} B)=\displaystyle\sum_{i,j}{a_{ij} b_{ij}} $$</p><p>由于标量可以视作一个 $1 \times 1$ 维的矩阵，其转置与迹都仍为自身，所以可以将以上三种情况统一起来：</p><p>$$ df=tr((\cfrac{\partial{f}}{\partial{X}})^{T} dX) \tag{∗} $$</p><h3 id="矩阵微分法则"><a href="#矩阵微分法则" class="headerlink" title="矩阵微分法则"></a>矩阵微分法则</h3><p>在进一步研究如何通过矩阵微分来求导之前，先花点时间眼熟下矩阵微分的几条运算法则：</p><ul><li>$d(X \pm Y)=dX \pm dY$</li><li>$d(X Y)=X dY+Y dX$</li><li>$d(X^{T})=(dX)^{T}$</li><li>$dtr(X)=tr(dX)$</li><li>$d(X \odot Y)=X \odot dY+dX \odot Y$</li><li>$d\sigma(X)=\sigma^{‘}(X) \odot dX$</li><li>$dX^{-1}=-X^{-1} dX X^{-1}$</li><li>$d|X|=|X| tr(X^{-1} dX)$</li></ul><h2 id="微分法"><a href="#微分法" class="headerlink" title="微分法"></a>微分法</h2><p>根据上一节得到的结论：$(∗)$ 式，我们完全可以利用矩阵微分来对矩阵进行求导，这就是所谓的<strong>微分法</strong>。</p><p>具体来说就是：若有标量函数 $f: \R^{m \times n} \to \R$，即标量函数 $f$ 是由加减乘逆迹行列式逐元素等矩阵运算构成，则可以利用<strong>微分运算法则</strong>先求出其微分 $df$，再利用<strong>迹技巧</strong>（trace trick）给求出的微分套上 $tr(\cdot)$，并将除 $dX$ 外的其他项交换至 $dX$ 的左侧，最终构造出 $df=tr(M^{T} dX)$ 的形式，这样就可以直接得到：$\cfrac{\partial{f}}{\partial{X}}=M$。</p><h3 id="矩阵迹技巧"><a href="#矩阵迹技巧" class="headerlink" title="矩阵迹技巧"></a>矩阵迹技巧</h3><p>微分法求导需要的预备知识包括<strong>矩阵微分运算法则</strong>和<strong>矩阵迹技巧</strong>两部分，前者在上一节已经列出，那么这里我们就把主要用到的几个迹技巧也列出来：</p><ul><li>$tr(x)=x$</li><li>$tr(A)=tr(A^{T})$</li><li>$tr(AB)=tr(BA)$，$A,B^{T} \in \R^{m \times n}$</li><li>$tr(X \pm Y)=tr(X) \pm tr(Y)$</li><li>$tr((A \odot B)^{T} C)=tr(A^{T} (B \odot C))$，$A,B,C \in \R^{m \times n}$</li></ul><h3 id="微分法求导"><a href="#微分法求导" class="headerlink" title="微分法求导"></a>微分法求导</h3><p>例1：$y=a^{T} X b$，求 $\cfrac{\partial{y}}{\partial{X}}$？</p><p>对于上述例子，按照之前提到的微分法求导思路，先求出其微分：</p><p>$$ dy=d(a^{T} X b)=da^{T} X b+a^{T} dX b+a^{T} X db=a^{T} dX b $$</p><p>再利用迹技巧构造出 $dy=tr(M^{T} dX)$ 的形式：</p><p>$$ dy=tr(dy)=tr(a^{T} dX b)=tr(b a^{T} dX)=tr((a b^{T})^{T} dX) $$</p><p>最终得到求导结果：</p><p>$$ \cfrac{\partial{y}}{\partial{X}}=a b^{T} $$</p><p>例2：$y=a^{T} exp(X b)$，求 $\cfrac{\partial{y}}{\partial{X}}$？</p><p>这个例子相对比较复杂，但是思路不变，先计算微分：</p><p>$$ dy=d(a^{T} exp(X b))=a^{T} dexp(X b)=a^{T} (exp(X b) \odot d(X b)) $$</p><p>再利用迹技巧构造出 $dy=tr(M^{T} dX)$ 的形式：</p><p>$$ \begin{aligned} dy&amp;=tr(dy) \\ &amp;=tr(a^{T} (exp(X b) \odot d(X b))) \\ &amp;=tr((a \odot exp(X b))^{T} d(X b)) \\ &amp;=tr(((a \odot exp(X b)) b^{T})^{T} dX) \end{aligned} $$</p><p>最终得到求导结果：</p><p>$$ \cfrac{\partial{y}}{\partial{X}}=((a \odot exp(X b)) b^{T} $$</p><h3 id="迹函数求导"><a href="#迹函数求导" class="headerlink" title="迹函数求导"></a>迹函数求导</h3><p>注意到，微分法中有一个步骤是利用迹技巧给微分套上$tr(\cdot)$，那么迹函数本身进行矩阵求导的情况显然会更方便些。接下来给出几个常见迹函数的求导：</p><p>例3：$\cfrac{\partial{tr(AB)}}{\partial{A}}=B^{T}$</p><p>$$ d(tr(AB))=tr(d(BA))=tr((B^{T})^{T} dA) $$</p><p>例4：$\cfrac{\partial{tr(AB)}}{\partial{B}}=A^{T}$</p><p>$$ d(tr(AB))=tr(d(AB))=tr((A^{T})^{T} dB) $$</p><p>例5：$ \cfrac{\partial{tr(W^{T} A W)}}{\partial{W}}=(A+A^{T}) W $</p><p>$$ \begin{aligned} d(tr(W^{T} A W))&amp;=tr(d(W^{T} A W)) \\ &amp;=tr(d(W^{T}) A W+W^{T} d(A W)) \\ &amp;=tr((dW)^{T} A W)+tr(W^{T} A dW) \\ &amp;=tr(W^{T} (A+A^{T}) dW) \\ &amp;=tr(((A+A^{T})^{T} W)^{T} dW) \end{aligned} $$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上一篇博客中已经说明了定义法进行矩阵求导的局限性：复杂麻烦且破坏整体性。微分法很大程度上避免了这些问题。但在一些复杂的多层链式求导中微分法使用起来仍然有些麻烦，这就需要我们能够记忆一些常用的求导结果，并利用链式求导法则来进行计算。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/pinard/p/10791506.html" target="_blank" rel="noopener">机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法</a></p><p><a href="https://zhuanlan.zhihu.com/p/46908990" target="_blank" rel="noopener">机器学习中的矩阵求导技术</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数学天坑】之矩阵/向量求导（二）</title>
      <link href="mathematics/matrix-vector-derivation-2/"/>
      <url>mathematics/matrix-vector-derivation-2/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>这是【数学天坑】系列博客的第二篇，或者说是开篇的第二部分。</p><p>对于矩阵/向量求导，哪怕只是机器学习中会涉及到的部分，显然也不是一篇博客能搞定的，再加上我对自己博客的期望是<strong>“短小精悍”</strong>，即在文章比较简短的前提下确保每次都完整地记录下自己想要说明的数个知识点（这样做最大的好处就是每篇博客都能用碎片时间看完，而碎片化阅读几乎是现在绝大多数人的常态），所以矩阵/向量求导部分我会分成几篇博客来完成。</p><p>本篇将搞定<strong>定义法</strong>求导的部分。</p><a id="more"></a><h2 id="定义法"><a href="#定义法" class="headerlink" title="定义法"></a>定义法</h2><p>在上一篇博客已经说过，<strong>矩阵求导本质上不过是多元函数求导</strong>，只是将自变量/因变量/求导结果排列成矩阵形式，方便表达、计算及推导。因此，我们完全可以对矩阵的分量分别进行求导，然后再按照布局规范重新排列成矩阵，这就是所谓的<strong>定义法</strong>。</p><p>在实际计算时显然不会使用这种方法，原因也很简单，先来几个例子感受一下就明白了。</p><h3 id="标量对向量"><a href="#标量对向量" class="headerlink" title="标量对向量"></a>标量对向量</h3><p>例1：$y=\bold{a}^{T} \bold{x}$，$\bold{a}, \bold{x} \in \R^{n}$，求 $\cfrac{\partial{\bold{a}^{T} \bold{x}}}{\partial{\bold{x}}}$？</p><p>根据定义，对 $\bold{x}$ 的第 $i$ 个分量求导：</p><p>$$ \cfrac{\partial{\bold{a}^{T} \bold{x}}}{\partial{x_{i}}}=\cfrac{\partial{\displaystyle\sum_{j=1}^{n}{a_{j}x_{j}}}}{\partial{x_{i}}}=\cfrac{\partial{a_{i}x_{i}}}{\partial{x_{i}}}=a_{i} $$</p><p>由此易得：</p><p>$$ \cfrac{\partial{\bold{a}^{T} \bold{x}}}{\partial{\bold{x}}}=\bold{a} $$</p><p>同理可得：</p><p>$$ \cfrac{\partial{\bold{x}^{T} \bold{a}}}{\partial{\bold{x}}}=\bold{a} \qquad \bold{a}, \bold{x} \in \R^{n} $$</p><p>$$ \cfrac{\partial{\bold{x}^{T} \bold{x}}}{\partial{\bold{x}}}=2 \bold{x} \qquad \bold{x} \in \R^{n} $$</p><p>例2：$y=\bold{x}^{T} A \bold{x}$，$\bold{x} \in \R^{n}, A \in \R^{n \times n}$，求 $\cfrac{\partial{\bold{x}^{T} A \bold{x}}}{\partial{\bold{x}}}$？</p><p>同样，对 $\bold{x}$ 的第 $i$ 个分量求导：</p><p>$$ \cfrac{\partial{\bold{x}^{T} A \bold{x}}}{\partial{x_{i}}}=\cfrac{\partial{\displaystyle\sum_{j=1}^{n}\displaystyle\sum_{k=1}^{n}{x_{j} a_{jk} x_{k}}}}{\partial{x_{i}}}=\displaystyle\sum_{j=1}^{n}{a_{ji} x_{j}}+\displaystyle\sum_{k=1}^{n}{a_{ik} x_{k}} $$</p><p>可得：</p><p>$$ \cfrac{\partial{\bold{x}^{T} A \bold{x}}}{\partial{\bold{x}}}=A^{T} x+A x $$</p><p>标量对向量求导的几条<strong>基本法则</strong>：</p><ul><li>常量对向量求导结果为 0</li><li>线性法则：若 $f, g$ 为标量函数，$c_{1}, c_{2}$ 为常数，则：</li></ul><p>$$ \cfrac{\partial{(c_{1} f+c_{2} g)}}{\partial{\bold{x}}}=c_{1} \cfrac{\partial{f}}{\partial{\bold{x}}}+c_{2} \cfrac{\partial{g}}{\partial{\bold{x}}} $$</p><ul><li>乘法法则：若 $f, g$ 为标量函数，则：</li></ul><p>$$ \cfrac{\partial{f g}}{\partial{\bold{x}}}=f \cfrac{\partial{g}}{\partial{\bold{x}}}+g \cfrac{\partial{f}}{\partial{\bold{x}}} $$</p><ul><li>除法法则：若 $f, g$ 为标量函数，且 $g(\bold{x}) \neq 0$，则：</li></ul><p>$$ \cfrac{\partial{f/g}}{\partial{\bold{x}}}=\cfrac{1}{g^{2}}(g \cfrac{\partial{f}}{\partial{\bold{x}}}-f \cfrac{\partial{g}}{\partial{\bold{x}}}) $$</p><p>这些法则在形式上其实与我们非常熟悉的标量对标量求导是统一的。</p><h3 id="标量对矩阵"><a href="#标量对矩阵" class="headerlink" title="标量对矩阵"></a>标量对矩阵</h3><p>例3：$y=\bold{a}^{T} X \bold{b}$，$\bold{a} \in \R^{m}, \bold{b} \in \R^{n}, X \in \R^{m \times n}$，求 $\cfrac{\partial{\bold{a}^{T} X \bold{b}}}{\partial{X}}$？</p><p>对矩阵 $X$ 的任一分量 $x_{ij}$ 求导：</p><p>$$ \cfrac{\partial{\bold{a}^{T} X \bold{b}}}{x_{ij}}=\cfrac{\partial{\displaystyle\sum_{p=1}^{m}\displaystyle\sum_{q=1}^{n}{a_{p} x_{pq} b_{q}}}}{\partial{x_{ij}}}=a_{i} b_{j} $$</p><p>可得：</p><p>$$ \cfrac{\partial{\bold{a}^{T} X \bold{b}}}{\partial{X}}=a b^{T} $$</p><h3 id="向量对向量"><a href="#向量对向量" class="headerlink" title="向量对向量"></a>向量对向量</h3><p>例4：$\bold{y}=A \bold{x}$，$x \in \R^{n}, A \in \R^{m \times n}$，求 $\cfrac{\partial{A \bold{x}}}{\partial{\bold{x}}}$？</p><p>$\bold{x} \in \R^{n}, \bold{y} \in \R^{m}$，根据上一篇博客，其求导结果（分子布局）应为 $\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}} \in R^{m \times n}$。</p><p>现计算  $\bold{y}$ 任一分量 $y_{i}$ 对 $\bold{x}$ 任一分量 $x_{j}$ 求导的结果：</p><p>$$ \cfrac{\partial{A_{i,} \bold{x}}}{\partial{x_{j}}}=\cfrac{\partial{\displaystyle\sum_{k=1}^{n}{a_{ik} x_{k}}}}{\partial{x_{j}}}=a_{ij} $$</p><p>因为（分子布局）$\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}} \in R^{m \times n}$，可得结果为 $A$ 而非 $A^{T}$：</p><p>$$ \cfrac{\partial{A \bold{x}}}{\partial{\bold{x}}}=A $$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可见，当函数比较简单时定义法还是能够胜任的，而一旦函数稍微复杂一些，定义法就显得太麻烦了。此外，矩阵求导之所以存在就是因为其整体性，方便表达、计算以及推导，定义法破坏了整体性，重新退化成多元函数求导，矩阵求导的意义不复存在。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/pinard/p/10773942.html" target="_blank" rel="noopener">机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法</a></p><p><a href="https://zhuanlan.zhihu.com/p/46908990" target="_blank" rel="noopener">机器学习中的矩阵求导技术</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【数学天坑】之矩阵/向量求导（一）</title>
      <link href="mathematics/matrix-vector-derivation-1/"/>
      <url>mathematics/matrix-vector-derivation-1/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>【数学天坑】系列博客的开篇。本系列主要作为接下来一段时间自己恶补荒废多年数学的记录与分享。这么多年下来挖出的“天坑”显然不是一朝一夕能够填上的，只能送给自己八个字——<strong>“戒骄戒躁、脚踏实地”</strong>。</p><p>矩阵/向量求导在包括机器学习在内的许多领域都有着广泛的使用，但对于这部分知识感jio自己一直都处于一种比较懵逼的状态，所以有了这篇博客。因为鄙人是菜鸡程序猿一枚，并不需要像数学大佬那样钻研得非常深入，所以本文只会涉及比较浅层的部分，主要为机器学习相关的矩阵/向量求导，旨在快速掌握矩阵/向量求导法则。</p><p>本篇主要介绍矩阵<strong>求导定义</strong>及<strong>求导布局</strong>。</p><a id="more"></a><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p>为了后续方便以及统一书写规范，在此先对符号表示进行说明：</p><ul><li>标量：普通小写字母或希腊字母，例如 $x$ 或 $\alpha$</li><li>向量：粗体小写字母，例如 $\bold{x}$，向量默认为<strong>列向量</strong>，且其每个分量记作 $x_{i}$</li><li>矩阵：普通大写字母，例如 $X$，$X_{i,}$ / $X_{,i}$ 分别表示第 $i$ 行/列，且其每个分量记作 $x_{ij}$</li><li>特殊说明的除外</li></ul><h2 id="导数定义"><a href="#导数定义" class="headerlink" title="导数定义"></a>导数定义</h2><p>在高等数学中，通常都是标量对标量求导，例如标量 $y$ 对 标量 $x$ 求导记作 $\cfrac{\partial{y}}{\partial{x}}$，但有时会遇到一组标量 $y_{i}, i=1,2,\ldots,m$（记作向量 $\bold{y}$）对标量 $x$ 求导的情况，此时的求导结果 $\cfrac{\partial{y_{i}}}{\partial{x}},i=1,2,\ldots,m$ 可以排列成一个 $m$ 维向量：$\cfrac{\partial{\bold{y}}}{\partial{x}}$。</p><p>可见，向量对标量求导其实就是对向量的每个分量分别求导然后重新排列成一个向量。这一结论对后文提到的几种形式同样适用。由此，<strong>矩阵求导本质上不过是多元函数求导</strong>，只是将自变量/因变量/求导结果排列成矩阵形式，方便表达、计算及推导而已。</p><p>根据求导时因变量/自变量的形式不同，可以列出以下几种可能的求导定义：</p><table><thead><tr><th align="center">自变量\因变量</th><th align="center">标量 y​</th><th align="center">向量 y</th><th align="center">矩阵 Y​</th></tr></thead><tbody><tr><td align="center"><strong>标量 x​</strong></td><td align="center">$\cfrac{\partial{y}}{\partial{x}}$</td><td align="center">$\cfrac{\partial{\bold{y}}}{\partial{x}}$</td><td align="center">$\cfrac{\partial{Y}}{\partial{x}}$</td></tr><tr><td align="center"><strong>向量 x</strong></td><td align="center">$\cfrac{\partial{y}}{\partial{\bold{x}}}$</td><td align="center">$\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}}$</td><td align="center">$\cfrac{\partial{Y}}{\partial{\bold{x}}}$</td></tr><tr><td align="center"><strong>矩阵 X​</strong></td><td align="center">$\cfrac{\partial{y}}{\partial{X}}$</td><td align="center">$\cfrac{\partial{\bold{y}}}{\partial{X}}$</td><td align="center">$\cfrac{\partial{Y}}{\partial{X}}$</td></tr></tbody></table><p>需要注意的是，除了标量对标量求导以外，其他求导都会有一个排列方式的问题。譬如标量对列向量求导时，结果究竟是行向量还是列向量？</p><p>事实上对于单一求导过程来说，这两者都是可以的，只不过是定义的不同罢了，但是当你要使用向量/矩阵来进行一些推导时，必须要进行统一的规定（无论规定为哪种方式），否则会出现维度不匹配的问题。由此，引出了求导布局的概念。</p><h2 id="求导布局"><a href="#求导布局" class="headerlink" title="求导布局"></a>求导布局</h2><p>最基础的求导布局有两个：分子布局（numerator layout）和分母布局（denominator layout）。</p><p>举个栗子。</p><p>对于分子布局，求导结果的维度以分子为主。譬如：</p><ul><li>$m$ 维列向量 $\bold{y}$ 对标量 $x$ 求导，结果 $\cfrac{\partial{\bold{y}}}{\partial{x}}$ 为 $m$ 维列向量</li><li>标量 $y$ 对 $n$ 维列向量 $\bold{x}$ 求导，结果 $\cfrac{\partial{y}}{\partial{\bold{x}}}$ 为 $n$ 维行向量</li></ul><p>对于分母布局，求导结果的维度以分母为主。譬如：</p><ul><li>$m$ 维列向量 $\bold{y}$ 对标量 $x$ 求导，结果 $\cfrac{\partial{\bold{y}}}{\partial{x}}$ 为 $m$ 维行向量</li><li>标量 $y$ 对 $n$ 维列向量 $\bold{x}$ 求导，结果 $\cfrac{\partial{y}}{\partial{\bold{x}}}$ 为 $n$ 维列向量</li></ul><p>显然，两种布局的结果只相差一个转置。</p><blockquote><p>再次强调，本文默认向量为列向量，表示行向量时将使用 $\bold{x}^{T}$</p></blockquote><p>同理，很容易得出矩阵对标量或者标量对矩阵求导的结果。</p><p>由于在机器学习中几乎不太会遇到 <em>矩阵对向量/向量对矩阵/矩阵对矩阵</em> 这三种类型，所以略去不做讨论。</p><p>至此，就只剩下最后一种情况：向量对向量求导（只关注列向量对列向量求导）。</p><p>考虑到，向量 $\bold{y} \in \R^{m}$ 对向量 $\bold{x} \in \R^{n}$ 求导，实际上就是 $mn$ 个标量对标量求导，问题只在于求导结果应该如何排列。显然，排列成一个矩阵是合理的。那么在两种基础布局中，$\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}}$ 是如何排列的呢？</p><p>为方便书写，记 $D_{ij} \triangleq \cfrac{\partial{y_{i}}}{\partial{x_{j}}}$。</p><p>对于分子布局，想象 $\bold{y}$ 不变而 $\bold{x}$ 转置的情景，得到结果：</p><p>$$\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}}=\begin{pmatrix} D_{11} &amp; D_{12} &amp; \cdots &amp; D_{1n} \\ D_{21} &amp; D_{22} &amp; \cdots &amp; D_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ D_{m1} &amp; D_{m1} &amp; \cdots &amp; D_{mn} \end{pmatrix} $$</p><p>对于上述结果矩阵，通常称其为<strong>雅克比（Jacobian）矩阵</strong>，有些资料会使用 $\cfrac{\partial{\bold{y}}}{\partial{\bold{x^{T}}}}$ 来表示，记法不同但意义相同。</p><p>对于分母布局，想象 $\bold{y}$ 转置而 $\bold{x}$ 不变的情景，得到结果：</p><p>$$\cfrac{\partial{\bold{y}}}{\partial{\bold{x}}}=\begin{pmatrix} D_{11} &amp; D_{21} &amp; \cdots &amp; D_{m1} \\ D_{12} &amp; D_{22} &amp; \cdots &amp; D_{m2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ D_{1n} &amp; D_{2n} &amp; \cdots &amp; D_{mn} \end{pmatrix} $$</p><p>对于上述结果矩阵，通常称其为梯度矩阵，有些资料会使用 $\cfrac{\partial{\bold{y^{T}}}}{\partial{\bold{x}}}$ 来表示，记法不同但意义相同。</p><p>综上所述，如果 $\bold{x} \in \R^{n}$，$\bold{y} \in \R^{m}$，$X \in \R^{m \times n}$，$Y \in \R^{m \times n}$，则有：</p><table><thead><tr><th align="center">自变量\因变量</th><th align="center">标量 $y$</th><th align="center">向量 $\bold{y}$</th><th align="center">矩阵 $Y$</th></tr></thead><tbody><tr><td align="center"><strong>标量 $x$</strong></td><td align="center">$/$</td><td align="center">分子：$\R^{m \times 1}$（默认）<br>分母：$\R^{1 \times m}$</td><td align="center">分子：$\R^{m \times n}$（默认）<br>分母：$\R^{n \times m}$</td></tr><tr><td align="center"><strong>向量 $\bold{x}$</strong></td><td align="center">分子：$\R^{1 \times n}$<br>分母：$\R^{n \times 1}$（默认）</td><td align="center">分子：$\R^{m \times n}$（默认）<br>分母：$\R^{n \times m}$</td><td align="center">$/$</td></tr><tr><td align="center"><strong>矩阵 $X$</strong></td><td align="center">分子：$\R^{n \times m}$<br>分母：$\R^{m \times n}$（默认）</td><td align="center">$/$</td><td align="center">$/$</td></tr></tbody></table><p>注意到，以上表格展示的五种类型都有所谓的默认布局，其<strong>总体原则</strong>是：尽量保持与前面的向量/矩阵维度一致。向量对向量的求导布局比较有争议，个人习惯使用分子布局。</p><blockquote><p>备注：考虑到在机器学习中，几乎很少遇到 <em>向量对标量/矩阵对标量</em> 的情况，因此后续的博客将重点关注其他三种，即 <em>标量对向量/标量对矩阵/向量对向量</em>。</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/pinard/p/10750718.html" target="_blank" rel="noopener">机器学习中的矩阵向量求导(一) 求导定义与求导布局</a></p><p><a href="https://www.zhihu.com/question/352174717" target="_blank" rel="noopener">矩阵求导中布局约定及其意义？- 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/46908990" target="_blank" rel="noopener">机器学习中的矩阵求导技术</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解语言模型 Language Model</title>
      <link href="mathematics/understanding-language-model/"/>
      <url>mathematics/understanding-language-model/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>语言模型，在百度百科中的描述是：<em>根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。语言模型与语言客观事实之间的关系，如同数学上的抽象直线与具体直线之间的关系</em>。在我看来，语言模型本质上其实是在解决这样一个问题：<strong>语句是否合理</strong>（更直白的说法就是，说的是不是人话 <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8">😏</span>）。本文会介绍语言模型在计算机领域的几个转变的重要节点以及个人的一点小小的理解。</p><a id="more"></a><h2 id="语言模型定义"><a href="#语言模型定义" class="headerlink" title="语言模型定义"></a>语言模型定义</h2><p>标准定义：对于给定语言序列 $w_{1}, w_{2}, \ldots, w_{n}$，计算其概率大小，即 $P(w_{1}, w_{2}, \ldots, w_{n})$。</p><p>白话解释：给定一句话，判断其是不是正常的语句，或者说其作为正常语句出现的概率有多大。</p><h2 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h2><p>说到这里，就不得不谈谈 N 元文法模型（N-Gram Model）了。</p><h3 id="N-Gram-模型基础"><a href="#N-Gram-模型基础" class="headerlink" title="N-Gram 模型基础"></a>N-Gram 模型基础</h3><p>N-Gram 模型将语句（词序列）看作一个随机事件，并赋予其相应的概率来描述某语句出现的可能性。即，给定一个词汇集合 $V$，对于由 $V$ 中词汇组成的序列 $S=&lt;w_{1}, w_{2}, \ldots, w_{n}&gt;, w_{i} \in V$，N-Gram 模型将计算其出现的概率 $P(w_{1}^{n})$。</p><p>提前声明，为简化书写，我们使用符号 $w_{i}^{j}$ 来表示 $w_{i}, w_{i+1}, \ldots, w_{j}$。</p><p>首先，由链式法则（chain rule）得：<br>$$ P(w_{1}^{n})=P(w_{1})P(w_{2}|w_{1}) \cdots P(w_{n}|w_{1}^{n-1}) $$<br>然后，在统计语言模型中，我们会采用<a href="#">极大似然估计</a>来计算每个词出现的条件概率（“统计语言模型”中的“统计”一词就体现在这儿）：<br>$$ \begin{aligned} P(w_{i}|w_{1}^{i-1})&amp;=\frac{C(w_{1}^{i-1}, w_{i})}{\sum_w C(w_{1}^{i-1}, w)} \\ &amp;=\frac{C(w_{1}^{i-1}, w_{i})}{C(w_{1}^{i-1})} \end{aligned} $$</p><p>其中，$C(\cdot)$ 表示子序列在训练集中出现的次数，即用频率近似概率。</p><p>显然，当序列较长时，这样直接进行计算是不现实的，原因有两点：</p><ul><li>参数空间过大：当序列过长时，$P(w_{n}|w_{1}^{n-1})$ 的可能性过多，难以估算</li><li>数据过于稀疏：当序列过长时，很容易出现 $w_{1}^{n}$ 根本没在训练集中出现过的情况</li></ul><p>因此，我们引入<a href="#">马尔可夫假设</a>，即假设当前词的概率仅与前 $n-1$ 个词相关，得：</p><p>$$ P(w_{i}|w_{1}^{i-1}) \approx P(w_{i}|w_{i-n+1}^{i-1}) $$</p><p>基于上式，可以得到 N-Gram 模型的定义：</p><ul><li>unigram：$n=1$，$P(w_{1}^{n}) \approx \prod_{i=1}^{n} P(w_{i})$</li><li>bigram：$n=2$，$P(w_{1}^{n}) \approx P(w_{1}) \prod_{i=2}^{n}P(w_{i}|w_{i-1})$</li><li>trigram：$n=3$，$P(w_{1}^{n}) \approx P(w_{1}) P(w_{2}|w_{1}) \prod_{i=1}^{n}P(w_{i}|w_{i-2}, w_{i-1})$</li><li>……</li></ul><p>需要注意的是，在进行实际操作时，有两个小 tricks。</p><p>其一，我们注意到在上述定义中，当 $n&gt;1$ 时，会出现 $w_{i} (i \le 0)$ 的情况，此时可以通过在序列首添加一个或多个伪词（起始符，$\langle s \rangle$）来解决。譬如 $n=2$ 时，$P(w_{1}|\langle s \rangle)$。</p><p>其二，我们往往不会直接计算上述概率，而是采用<strong>对数概率</strong>，即 $\log (\prod_{i} p_{i}) = \sum_{i} \log p_{i}$</p><p>这样计算会有两大优势：</p><ul><li>将连乘转化为累加，加速计算</li><li>防止数值溢出（概率本就是一些较小的数，连乘容易造成数值下溢）</li></ul><h3 id="起始标签-结束标签"><a href="#起始标签-结束标签" class="headerlink" title="起始标签/结束标签"></a>起始标签/结束标签</h3><p><strong>为什么 N-Gram 需要开始标签/结束标签？</strong></p><p>开门见山，先放结论：</p><p>$P(w_{1}^{n})$ 建模的 <strong>是</strong> 在无限长序列中出现子序列 $w_{1}^{n}$ 的概率，而 <strong>不是</strong> 序列 $w_{1}^{n}$ 出现的概率！</p><p>由于没有准确理解到这一点，导致一开始看 N-Gram 模型时对开始/结束标签的存在十分的困惑。</p><blockquote><p>额外多说一句，上述理解仅代表个人的看法，我会在下面给出自己的解释，不一定正确，但以我目前的能力，只有这个解释能够说服我自己。（数学渣滓的悲哀）</p></blockquote><p>这是很容易被初学者误解的一点，但只要理解了，会有豁然开朗的感觉。理由的话其实也很简单，我们回过头再仔细看看公式：$P(w_{1}^{n})=P(w_{1})P(w_{2}|w_{1}) \cdots P(w_{n}|w_{1}^{n-1})$，很容易发现，公式的第一项 $P(w_{1})$ 表示的是 $w_{1}$ 出现的概率，即 $w_{1}$ 在任何位置出现都被包含在内。这样就很清晰了，这里并没有限定 $w_{1}$ 前面还有多少词汇，当然了，也没有限定 $w_{n}$ 后面还有多少词汇，所以才说 $P(w_{1}^{n})$ 建模的是在无限长序列中所有出现子序列 $w_{1}^{n}$ 的总概率，如果用正则来表示的话大概就是 $(.\star?)w_{1}w_{2} \cdots w_{n}(.\star?)$ 吧。</p><p>这时候就体现出开始/结束标签的重要性了。在实践中，语句肯定只会是有限长的，像这种对无限长序列的建模其实毫无实际意义，但加上开始/结束标签就不一样了，界定了语句的开始与结束之后我们的模型就有能力建模任意长序列了（当然也包括无限长序列，只是由于概率的累乘，过长序列的出现概率几乎可以忽略不计，这也符合我们的直觉）。</p><p>下面举个栗子，为了简化说明过程，我们先考虑有开始标签而没有结束标签的情况。</p><p>假设我们有以下语料（是的，你没看错，就三句，词汇表 $V = (a, b, c)$）：</p><pre class=" language-sh"><code class="language-sh">⟨s⟩ a b⟨s⟩ a c⟨s⟩ b a</code></pre><p>取 $n=2$，即 bigram 模型，可得：</p><pre class=" language-sh"><code class="language-sh">P(a|⟨s⟩) = 2/3P(b|⟨s⟩) = 1/3P(b|a) = 1/2P(c|a) = 1/2P(a|b) = 1</code></pre><p>那么可以计算出结果如下：</p><pre class=" language-sh"><code class="language-sh">P(ab) = 2/3 * 1/2 = 1/3P(ac) = 2/3 * 1/2 = 1/3P(ba) = 1/3 * 1 = 1/3P(aa) = P(bb) = P(bc) = P(ca) = P(cb) = P(cc) = 0</code></pre><p>桥豆麻袋！是不是有哪里不对？长度为 2 的序列概率和就等于 1 了，那其他长度的序列可咋办？但是考虑到我们前面说的就很容易理解了，这里其实应该是：</p><pre class=" language-sh"><code class="language-sh">P(aa...) + P(ab...) + ... + P(cb...) + P(cc...) = 1</code></pre><p>Bingo！那么现在加上结束标签再算一次：</p><pre class=" language-sh"><code class="language-sh">⟨s⟩ a b ⟨/s⟩⟨s⟩ a c ⟨/s⟩⟨s⟩ b a ⟨/s⟩</code></pre><p>各概率如下：</p><pre class=" language-sh"><code class="language-sh">P(a|⟨s⟩) = 2/3P(b|⟨s⟩) = 1/3P(b|a) = 1/3P(c|a) = 1/3P(a|b) = 1/2P(⟨/s⟩|a) = 1/3P(⟨/s⟩|b) = 1/2P(⟨/s⟩|c) = 1</code></pre><p>计算结果如下：</p><pre class=" language-sh"><code class="language-sh">P(ab) = 2/3 * 1/3 * 1/2 = 1/9P(ac) = 2/3 * 1/3 * 1 = 2/9P(ba) = 1/3 * 1/2 * 1/3 = 1/18P(aa) = P(bb) = P(bc) = P(ca) = P(cb) = P(cc) = 0P(aa) + P(ab) + ... + P(cb) + P(cc) = 7/18 < 1</code></pre><p>可以再继续算算序列长度为 1 和 3 的概率。我们的语料分布情况使然，序列长度为 1~3 的概率和应该已经接近 1 了，这是合理的。</p><p>以上例子已经说明了为什么需要结束标签，反过来也是同样成立的。综上所述，要建模有限长序列，必须要有开始/结束标签的存在。</p><h2 id="神经语言模型"><a href="#神经语言模型" class="headerlink" title="神经语言模型"></a>神经语言模型</h2><p>有了 N-Gram 模型的基础，我们应该已经能够大致理解语言模型在做什么了。其实就是在给定一个词序列的前提下，预测该词序列的下一个词的概率情况，即 $P(w_{i}|w_{1}^{i-1})$，然后根据链式法则就可以计算出所有词序列出现的概率，即 $P(w_{1}^{n})$。N-Gram 模型中的 $n$ 的取值不同归根结底只是对 $P(w_{i}|w_{1}^{i-1})$ 的近似程度不同罢了。</p><h3 id="基于前馈神经网络"><a href="#基于前馈神经网络" class="headerlink" title="基于前馈神经网络"></a>基于前馈神经网络</h3><p>既然我们已经理解了语言模型做的是在给定一个词序列的前提下预测该词序列的下一个词的概率情况，那么神经网络就想说话了，这事儿我熟啊。既然能用统计模型做，那么肯定也能用前馈神经网络来做。说到这里就不得不提到 Bengio 等人在 2001 年发表在 NIPS 上的论文 <a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>。</p><p><img src="https://i.loli.net/2020/07/17/74V2kGAzhCEadx5.png" alt="FFNN-LM.png" loading="lazy"></p><p>事实上这个神经网络模型也是一个 N-Gram 模型，即只考虑前 $n-1$ 个词的依赖关系，只不过是先将每个词都映射为连续空间中的一个词向量，再通过一个三层前馈神经网络去建模这种依赖关系（约束关系），相较于统计模型而言， 极大地增强了模型的泛化能力。模型公式如下（结合模型结构图很容易看懂了，不多 BB）：</p><p>$$ \begin{aligned} x &amp;= concat(C(w)_{t-n+1}^{t-1}) \\ h &amp;= tanh(Hx + d)  \\ y &amp;= Wx + Uh + b \end{aligned} $$</p><p>其中，$concat(\cdot)$ 为拼接函数，$C(w)<em>{i}^{j}$ 表示 $C(w</em>{i}), \ldots, C(w_{j})$，$C(w_{i})$ 为 $w_{i}$ 的词向量。</p><h3 id="基于递归神经网络"><a href="#基于递归神经网络" class="headerlink" title="基于递归神经网络"></a>基于递归神经网络</h3><p>上面提到的前馈神经网络模型虽然很好地改善了统计模型泛化能力较差的问题，但是实质上还是基于 N-Gram 模型的思想，只考虑了有限的前文信息，那么在遇到长序列时候有没有什么办法能够考虑到足够远的前文信息呢？当然有！那就是 Mikolov 于 2010 年发表的论文 <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf" target="_blank" rel="noopener">Recurrent neural network based language model</a> 中提出的方法，该论文将 RNN 用在了 LM 训练任务上。公式如下（注意，这里的 RNN 指的是狭义上最基础的 RNN 网络，即只在 RNNCell 内部建立了权连接）：</p><p>$$ \begin{aligned} x_{t}^{i} &amp;= concat(w_{t}^{i}, h_{t-1}^{i}) \\ h_{t}^{i} &amp;= f(W^{i} x_{t}^{i} + b^{i}) \\ y_{t} &amp;= g(U h_{t}^{n} + b) \end{aligned} $$</p><p>其中，输入的词嵌入 $w_{t}^{1}$ 使用最简单的 one-hot 编码，$n$ 为 RNN 网络层数，$i \in [1, n]$，$f(\cdot)$ 为 $sigmoid(\cdot)$，$g(\cdot)$ 为 $softmax(\cdot)$。</p><p>额外多说几句。单从理论上来说，RNN 应该是能够捕获足够远的前文信息的，但在实践中并非如此，一个合理的直觉是：因为 RNN 使用的这种最简单的 Cell 结构导致前文信息很容易随着时间步而逐渐被稀释，当遇到长序列时，较远的前文信息已经被稀释到几乎可以忽略不计了。也正因为如此，后来又进一步发展出了 <a href="#">LSTM</a> 和 <a href="#">GRU</a> 等各种衍生的递归神经网络（关于这部分，以后我会专门开一篇博客进行介绍）。</p><p>事实上，从统计语言模型开始一直到后面的各种递归神经网络，只要能够理解语言模型（Language Model）到底在做些什么，那么其他所有的东西也只是实现方法的不断改进优化罢了。</p><h2 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h2><p>（空）</p><hr><p>To Be Continued.</p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 语言模型 </tag>
            
            <tag> N-Grams </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【搞定神经网络】循环神经网络篇</title>
      <link href="mathematics/understanding-rnn-networks/"/>
      <url>mathematics/understanding-rnn-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>【搞定神经网络系列】博客开坑第一篇，循环神经网络篇。</p><p>循环神经网络（Recurrent Neural Network, RNN）是一类人工神经网络，通常以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接。需要注意的是循环神经网络具有记忆性且参数共享。</p><a id="more"></a><h2 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h2><p>要了解递归神经网络，首先就得了解神经网络。</p><p>那么，什么是神经网络呢？你可以将神经网络简单地看成是一个<strong>黑盒子</strong>，里面装了一大堆的可训练参数，参数与参数的组织形式共同构成了所谓的“<strong>模型</strong>”，这个模型建模了具有实际意义的一项或几项任务，当你手头拥有与任务相对应的数据时，你就可以使用你的数据训练这些参数来拟合真实的模型（上帝的模型？除了上帝，没人知道真实的模型是啥样的，只能通过已有的数据进行拟合）。</p><blockquote><p>当然，要训练一个神经网络，除了模型本身之外，还需要有<strong>优化器</strong>和<strong>损失函数</strong>（这就是另外的话题了）。</p></blockquote><p>既然说参数与参数的组织形式共同构成了模型，那么我们自然可以通过改变参数及其组织形式来构造出各种不同的（神经网络）模型，本博客介绍的就是其中的循环神经网络。</p><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>考虑平时阅读文章时的情景，显然你并不是将文章的每个字/词当成一个个完全独立的单元来进行阅读的，你在阅读每个字/词时都会考虑前文内容（甚至预想后文内容），也就是说，在处理类似于文本/语音这类序列数据时，我们的模型需要具备获取上下文信息的能力，传统的神经网络结构显然对此无能为力，然鹅，循环神经网络解决了这一问题。</p><p>循环神经网络，顾名思义，具有循环结构的神经网络，它（理论上）可以持续保存上下文信息，其结构如下图：</p><p><img src="https://i.loli.net/2020/07/23/GZcjyeQiAWLsNaX.png" alt="RNN.png" loading="lazy"></p><p>看起来貌似有那么一丢丢的复杂，那么我们将循环展开来看看：</p><p><img src="https://i.loli.net/2020/07/23/3AdiHYOaXCqTzw8.png" alt="RNN-.png" loading="lazy"></p><p>Emmm，结构清晰多了，可以看到前文信息确实是可以随着循环一层一层地往后传递的。</p><blockquote><p>需要注意的是，在单向 RNN 中，模型只能向后传递前文信息，若要同时考虑上下文的信息则应使用双向 RNN，关于双向 RNN 的内容后面也会提到。</p></blockquote><p>事实上这个结构应该叫做简单循环网络（simple recurrent network，SRN），又称为 Elman Network，是由 Jeff Elman 在 1990 年提出的，在 Jordan Network（1986）的基础上进行了改进并简化了结构。关于两者的区别这里不做赘述。</p><p>Notes：前文提到，循环神经网络通常用于处理序列数据，因此，在进一步介绍循环神经网络之前，我想在这儿丢张图，列举出序列数据处理任务可能的几种形式，具体就不多 BB 了，很容易看懂。</p><p><img src="https://i.loli.net/2020/08/07/FPe4EL9rbmwizVG.png" alt="seq2seq.png" loading="lazy"></p><h2 id="长短时记忆网络"><a href="#长短时记忆网络" class="headerlink" title="长短时记忆网络"></a>长短时记忆网络</h2><p>上一节已经介绍了循环神经网络的基本结构，但是循环结构中的重复单元（可以类比于编程中的循环体）是啥呢？</p><p>在具体说明之前，我们需要提前定义几个操作的表示方式：</p><p><img src="https://i.loli.net/2020/08/07/GIm9ilRwsjEaOgS.png" alt="operation.png" loading="lazy"></p><p>定义操作完毕，我们圆规正转（<del>谐音梗扣钱</del>），继续来讨论重复单元的问题。显而易见，根据重复单元的不同，可以定义出完全不同类型的循环神经网络。</p><p>事实上，标准 RNN 中的重复单元具有非常简单的结构，就是一个单层的神经网络。</p><p><img src="https://i.loli.net/2020/08/07/8YSrXD74nuL2iVT.png" alt="rnn.png" loading="lazy"></p><p>具体公式如下：</p><p>$$ h_{t}=g(W \cdot [x_{t}, h_{t-1}]+b) $$</p><p>其中，$g(\cdot)$ 为激活函数，通常选择 $sigmoid(\cdot)$ 或 $tanh(\cdot)$。</p><p>很遗憾的一点是，尽管 RNN 的结构在理论上具备获取足够的上下文信息的能力，但在实践中似乎不是这样的，标准 RNN 在实际表现中并不尽如人意，尤其是随着序列的上下文信息的跨度变大，标准 RNN  开始无法学习相应的信息。</p><p>幸运的是，后续衍生出的 LSTM 较好地解决了这个问题。</p><p><img src="https://i.loli.net/2020/08/07/ndczrI1UkKu9BoA.png" alt="lstm.png" loading="lazy"></p><p>LSTM 的关键点在于 Cell State，也就是如下图所示，贯穿了整个“循环”过程的 $C$。</p><p><img src="https://i.loli.net/2020/08/07/CtQHRNah2bYGIVk.png" alt="cell.png" loading="lazy"></p><p>这其实有点像是传送带，cell state 会沿着这条传送带逐层往后传送，在传送过程中不断有旧的状态被删除、新的状态被添加，而对状态信息的增删进行控制的部分就是其中门的机制。具体的（控制）操作方式就是逐元素乘法（pointwise multiplication operation）。</p><p><img src="https://i.loli.net/2020/08/07/9cHTfi1PgjZAsJG.png" alt="gated.png" loading="lazy"></p><h2 id="LSTM-逐步分解"><a href="#LSTM-逐步分解" class="headerlink" title="LSTM 逐步分解"></a>LSTM 逐步分解</h2><p>遗忘门（forget gate）控制有哪些旧的信息会被删除。</p><p><img src="https://i.loli.net/2020/08/07/Ofw8mvycq9pFSH7.png" alt="lstm-forget.png" loading="lazy"></p><p>输入门（input gate）控制有哪些新的信息会被添加。</p><p><img src="https://i.loli.net/2020/08/07/wvOMDegrj1idyNG.png" alt="lstm-input.png" loading="lazy"></p><p>旧的状态信息经过遗忘门和输入门对信息的增删之后，得到新的状态信息（cell state）。</p><p><img src="https://i.loli.net/2020/08/07/TvKPbklNqOdjw9L.png" alt="lstm-cell.png" loading="lazy"></p><p>输出门（output gate）控制最终有哪些信息会被输出。</p><p><img src="https://i.loli.net/2020/08/07/ecyrjV6iN2OvpgH.png" alt="lstm-output.png" loading="lazy"></p><h2 id="LSTM-的变体"><a href="#LSTM-的变体" class="headerlink" title="LSTM 的变体"></a>LSTM 的变体</h2><p>窥孔连接：</p><p><img src="https://i.loli.net/2020/08/07/Y8yXdcvHnqGmbgN.png" alt="lstm-var1.png" loading="lazy"></p><p>耦合遗忘/输入门：</p><p><img src="https://i.loli.net/2020/08/07/28VcwqbzKu5lE9L.png" alt="lstm-var2.png" loading="lazy"></p><p>GRU（Gated Recurrent Unit）将遗忘/输入门合并为更新门（update gate），并合并了 cell state 与 hidden state。</p><p><img src="https://i.loli.net/2020/08/07/35ldwRJIc6sEoQm.png" alt="gru.png" loading="lazy"></p><p>需要注意的是，以上这几种类型（包括标准的 LSTM）各擅胜场，并没有明确的优劣势，在具体的任务中选择哪种类型通常需要通过实验来确定。</p><h2 id="多层-RNN-结构"><a href="#多层-RNN-结构" class="headerlink" title="多层 RNN 结构"></a>多层 RNN 结构</h2><p>（空）</p><h2 id="双向-RNN-结构"><a href="#双向-RNN-结构" class="headerlink" title="双向 RNN 结构"></a>双向 RNN 结构</h2><p>（空）</p><hr><p>To Be Continued.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></p><p><a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" target="_blank" rel="noopener">long short-term memory (LSTM)</a></p><p><a href="https://arxiv.org/abs/1409.1259" target="_blank" rel="noopener">gated recurrent units (GRU)</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数学研究 </category>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> LSTM </tag>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【PyTorch 源码阅读】 torch.nn.Module 篇</title>
      <link href="info-science/torch-nn-Module-source-code/"/>
      <url>info-science/torch-nn-Module-source-code/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>【PyTorch 源码阅读系列】主要是记录一些阅读 PyTorch 源码时的笔记（<strong>好记性不如烂笔头</strong>）。事实上 PyTorch 的文档齐全，哪怕你不阅读源码也能够很好地使用它来搭建并训练自己的模型，我之所以选择阅读源码，一方面是为了对 PyTorch 有更深入的理解，另一方面是学习这种优秀的源码也能够帮助自己写出更优雅规范的代码。本文为 <code>torch.nn.Module</code> 篇，本系列的第一篇。</p><a id="more"></a><h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><blockquote><p>PyTorch 版本：1.5.1 - py3.7_cuda102_cudnn7_0</p></blockquote><p><code>torch.nn.Module</code> 是 PyTorch 所有神经网络模块的基类（官方文档：<em>Base class for all neural network modules</em>），无论是官方实现还是自己创建的的网络模块都应该是它的子类。</p><p>一个简单的使用示例：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SimpleNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>       <span class="token comment" spellcheck="true"># 定义神经网络</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SimpleNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Convolution kennel</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Max pooling over (2, 2) window</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token operator">*</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x</code></pre><hr><p><code>torch.nn.Module</code> 类中共包含 49 个函数（指定版本 1.5.1），下面一一进行分析。</p><hr><p><code>__init__</code> 和 <code>forward</code>。</p><p><code>__init__</code> 函数主要是初始化模块内部状态（<em>internal Module state</em>）。<code>forward</code> 函数需要在子类中实现，如果子类中没有实现会引发 <code>NotImplementedError</code>。</p><hr><p><code>register_buffer</code>、<code>register_parameter</code> 和 <code>add_module</code>。</p><p><code>register_buffer</code> 添加 <code>name: buffer</code> 到模块的 <code>self._buffers</code> 字典中，这里 <code>buffer</code> 指的是一些非模型参数的持久状态（<em>the persistent state</em>），譬如 <code>BatchNorm</code> 的 <code>running_mean</code> 等。</p><p><code>register_parameter</code> 添加 <code>name: parameter</code> 到模块的 <code>self._parameters</code> 字典中。</p><p><code>add_module</code> 根据 <code>name: module</code> 添加子模块到模块的 <code>self._modules</code> 字典中。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># register_buffer</span>self<span class="token punctuation">.</span>_buffers<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> tensor<span class="token comment" spellcheck="true"># register_parameter</span>self<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param<span class="token comment" spellcheck="true"># add_module</span>self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> module</code></pre><hr><p><code>_apply</code> 和 <code>apply</code>。</p><p><code>_apply</code> 的作用是对模块中的所有 <code>tensor</code>（包括 <code>parameters</code> 和 <code>buffers</code> ）进行一遍传入的 <code>fn</code> 操作。通过 <code>_apply</code> 我们可以方便地对模块中的 <code>tensor</code> 做很多操作，譬如下面会讲到的 <code>cuda</code> 和 <code>cpu</code>。</p><p>以下是简化源码：第一个循环就是递归地对所有的子模块进行一遍操作；<code>compute_should_use_set_data</code> 决定是否采用 <code>in-place</code> 的方式（<em>change the tensor in-place</em>），即就地修改 <code>tensor</code>；第二个循环对所有的 <code>parameters</code> 进行 <code>fn</code> 操作，如果 <code>param</code> 有 <code>.grad</code> 的话需要对其也进行 <code>fn</code> 操作；第三个循环对所有的 <code>buffers</code> 进行 <code>fn</code> 操作。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_apply</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fn<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        module<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span>fn<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">compute_should_use_set_data</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor_applied<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">for</span> key<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> param <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># ...</span>            compute_should_use_set_data<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># ...</span>            <span class="token keyword">if</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">for</span> key<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">return</span> self</code></pre><p><code>apply</code> 与 <code>_apply</code> 有所不同，它的作用是递归地对所有子模块用传入的 <code>fn</code> 操作一遍（<em>Applies <code>fn</code> recursively to every submodule</em>）。这里的子模块指的是 <code>.children()</code> 列出的内容。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">apply</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fn<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        module<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>fn<span class="token punctuation">)</span>    fn<span class="token punctuation">(</span>self<span class="token punctuation">)</span>    <span class="token keyword">return</span> self</code></pre><p>譬如可以利用 <code>apply</code> 对模型权重（<em>weight</em>）进行初始化。源码中给出的示例：</p><pre class=" language-sh"><code class="language-sh">>>> @torch.no_grad()>>> def init_weights(m):>>>     print(m)>>>     if type(m) == nn.Linear:>>>         m.weight.fill_(1.0)>>>         print(m.weight)>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))>>> net.apply(init_weights)</code></pre><hr><p><code>cuda</code>、<code>cpu</code> 和 <code>share_memory</code>。</p><p>将模块所有的 <code>tensor</code> 移入指定位置（GPU/CPU/共享内存）中。通过源码可以看到，这三者都调用了前面讲到的 <code>_apply</code> 函数。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># cuda</span><span class="token keyword">def</span> <span class="token function">cuda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># cpu</span><span class="token keyword">def</span> <span class="token function">cpu</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># shared memory</span><span class="token keyword">def</span> <span class="token function">share_memory</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>share_memory_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><hr><p><code>type</code>、<code>float</code>、<code>double</code>、<code>half</code> 和 <code>bfloat16</code>。</p><p>这几个函数都是将模块中所有的 <code>tensor</code> 转成指定的类型。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">type</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dst_type<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>type<span class="token punctuation">(</span>dst_type<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">float</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">double</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>double<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">half</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">bfloat16</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>bfloat16<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span></code></pre><hr><p><code>to</code>。</p><p><code>to</code> 可以用来对模块的所有 <code>tensor</code> 进行设备和/或类型的 <code>in-place</code> 方式的操作（<em>Moves and/or casts the parameters and buffers</em>）。源码中给出的示例：</p><pre class=" language-sh"><code class="language-sh">>>> linear = nn.Linear(2, 2)>>> linear.weight>>> linear.to(torch.double)>>> linear.weight>>> gpu1 = torch.device("cuda:1")>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)>>> linear.weight>>> cpu = torch.device("cpu")>>> linear.to(cpu)>>> linear.weight</code></pre><p>大家可以实际运行下看看结果。</p><hr><p><code>register_backward_hook</code>、<code>register_forward_pre_hook</code> 和 <code>register_forward_hook</code>。</p><p>这三个函数分别在模块中注册 <code>forward_pre_hook</code>、<code>forward_hook</code> 和 <code>backward_hook</code>。关于 <code>Hooks</code> （钩子？挂钩？咋翻好听？）的作用我会在后面专门开一篇文章说明。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># backward_hook</span>hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> grad_input<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tensor <span class="token operator">or</span> None<span class="token comment" spellcheck="true"># forward_pre_hook</span>hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> input<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None <span class="token operator">or</span> modified input<span class="token comment" spellcheck="true"># forward_hook</span>hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> output<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None <span class="token operator">or</span> modified output</code></pre><hr><p><code>_slow_forward</code> 和 <code>__call__</code>。</p><p><code>__call__</code> 是模块计算的真正入口，内部会调用 <code>_slow_forward</code> 函数或者 <code>forward</code> 函数进行计算，事实上 <code>_slow_forward</code> 内部最终也是调用 <code>forward</code> 函数进行计算，两者的差别在于有些自定义操作是没有 C 代码的，这种情况就会直接调用 Python 版本，反之调用 C 版本（效率高）。简化源码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_forward_pre_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_get_tracing_state<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        result <span class="token operator">=</span> self<span class="token punctuation">.</span>_slow_forward<span class="token punctuation">(</span><span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        result <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_forward_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_backward_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># ...</span>    <span class="token keyword">return</span> result</code></pre><hr><p><code>__setstate__</code>。</p><p><code>__setstate__</code> 设置 <code>state</code>，这个比较简单，直接看源码。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__setstate__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>update<span class="token punctuation">(</span>state<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Support loading old checkpoints that don't have the following attrs:</span>    <span class="token keyword">if</span> <span class="token string">'_forward_pre_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_forward_pre_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token string">'_state_dict_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_state_dict_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token string">'_load_state_dict_pre_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_load_state_dict_pre_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><hr><p><code>__getattr__</code>、<code>__setattr__</code> 和 <code>__delattr__</code>。</p><p><code>__getattr__</code> 用于获取指定 <code>name</code> 的模块成员（包括 <code>parameters</code>、<code>buffers</code> 和 <code>modules</code>，查找顺序从前到后）。</p><p><code>__getattr__</code> 用于查找指定 <code>name</code> 的模块成员后对其进行设置（同上）。</p><p><code>__delattr__</code> 用于删除指定 <code>name</code> 的模块成员（同上）。</p><p>这三个函数都比较简单，不多做说明。</p><hr><p><code>_register_state_dict_hook</code> 和 <code>_register_load_state_dict_pre_hook</code>。</p><p>这三个函数与前面提到的 <code>register_*_hook</code> 类似，用于注册 <code>Hooks</code>。</p><p><code>_register_state_dict_hook</code> ：</p><p><em>These hooks will be called with arguments: <code>self</code>, <code>state_dict</code>, <code>prefix</code>, <code>local_metadata</code>, after the <code>state_dict</code> of <code>self</code> is set.</em></p><p><code>_register_load_state_dict_pre_hook</code>：</p><p><em>These hooks will be called with arguments: <code>state_dict</code>, <code>prefix</code>, <code>local_metadata</code>, <code>strict</code>, <code>missing_keys</code>, <code>unexpected_keys</code>, <code>error_msgs</code>, before loading <code>state_dict</code> into <code>self</code>.</em></p><hr><p><code>_save_to_state_dict</code> 和 <code>_load_from_state_dict</code>。</p><p><code>_save_to_state_dict</code> 作用是保存 <code>state</code> 到 <code>destination</code> 指定的字典中，此函数会被当前模块的所有子模块调用（<em>This is called on every submodule in :meth:<code>~torch.nn.Module.state_dict</code></em>）。</p><p><code>_load_from_state_dict</code> 作用与 <code>_save_to_state_dict</code> 相反，用来加载模块。</p><hr><p><code>state_dict</code> 和 <code>load_state_dict</code>。</p><p><code>state_dict</code> 的作用是返回一个包含模块完整 <code>state</code> 的字典（<em>Returns a dictionary containing a whole state of the module</em>），字典中的键值对对应着 <code>name: parameters</code> 或 <code>name: buffers</code>。此函数会调用上面的 <code>_save_to_state_dict</code> 函数。</p><p><code>load_state_dict</code> 的作用与 <code>state_dict</code> 相反，是将  <code>name: parameters</code> 或 <code>name: buffers</code> 加载到模块及其子模块中去。此函数会调用上面的 <code>_load_from_state_dict</code> 函数。</p><hr><p><code>named_parameters</code>、<code>named_buffers</code>、<code>named_children</code> 和 <code>named_modules</code>。</p><p>这四个函数的作用是返回 <code>name: menbers</code> 的生成器（<em>generator</em>），其中 <code>members</code> 可以是 <code>parameters</code>、<code>buffers</code>、<code>children</code> 或 <code>modules</code>。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_named_members</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> get_members_fn<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    r<span class="token triple-quoted-string string">"""Helper method for yielding various names + members of modules."""</span>    memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>    modules <span class="token operator">=</span> self<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span>prefix<span class="token operator">=</span>prefix<span class="token punctuation">)</span> <span class="token keyword">if</span> recurse <span class="token keyword">else</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> module_prefix<span class="token punctuation">,</span> module <span class="token keyword">in</span> modules<span class="token punctuation">:</span>        members <span class="token operator">=</span> get_members_fn<span class="token punctuation">(</span>module<span class="token punctuation">)</span>        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> members<span class="token punctuation">:</span>            <span class="token keyword">if</span> v <span class="token keyword">is</span> None <span class="token operator">or</span> v <span class="token keyword">in</span> memo<span class="token punctuation">:</span>                <span class="token keyword">continue</span>                memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>v<span class="token punctuation">)</span>                name <span class="token operator">=</span> module_prefix <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token string">'.'</span> <span class="token keyword">if</span> module_prefix <span class="token keyword">else</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token operator">+</span> k                <span class="token keyword">yield</span> name<span class="token punctuation">,</span> v<span class="token keyword">def</span> <span class="token function">named_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    gen <span class="token operator">=</span> self<span class="token punctuation">.</span>_named_members<span class="token punctuation">(</span>        <span class="token keyword">lambda</span> module<span class="token punctuation">:</span> module<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span> recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span>    <span class="token keyword">for</span> elem <span class="token keyword">in</span> gen<span class="token punctuation">:</span>        <span class="token keyword">yield</span> elem<span class="token keyword">def</span> <span class="token function">named_buffers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    gen <span class="token operator">=</span> self<span class="token punctuation">.</span>_named_members<span class="token punctuation">(</span>        <span class="token keyword">lambda</span> module<span class="token punctuation">:</span> module<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span> recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span>    <span class="token keyword">for</span> elem <span class="token keyword">in</span> gen<span class="token punctuation">:</span>        <span class="token keyword">yield</span> elem<span class="token keyword">def</span> <span class="token function">named_children</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> module <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> module <span class="token operator">not</span> <span class="token keyword">in</span> memo<span class="token punctuation">:</span>            memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>module<span class="token punctuation">)</span>            <span class="token keyword">yield</span> name<span class="token punctuation">,</span> module<span class="token keyword">def</span> <span class="token function">named_modules</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> memo <span class="token keyword">is</span> None<span class="token punctuation">:</span>        memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> self <span class="token operator">not</span> <span class="token keyword">in</span> memo<span class="token punctuation">:</span>        memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>self<span class="token punctuation">)</span>        <span class="token keyword">yield</span> prefix<span class="token punctuation">,</span> self        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> module <span class="token keyword">is</span> None<span class="token punctuation">:</span>                <span class="token keyword">continue</span>            submodule_prefix <span class="token operator">=</span> prefix <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token string">'.'</span> <span class="token keyword">if</span> prefix <span class="token keyword">else</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token operator">+</span> name            <span class="token keyword">for</span> m <span class="token keyword">in</span> module<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span>memo<span class="token punctuation">,</span> submodule_prefix<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">yield</span> m</code></pre><p>注意：<code>named_children</code> 非递归，只返回下一级的子模块（<em>immediate children modules</em>）；<code>named_modules</code> 不返回重复的模块（<em>Duplicate modules are returned only once</em>）。</p><p>此外还有相应的 <code>parameters</code>、<code>buffers</code>、<code>children</code> 和 <code>modules</code>，只会返回 <code>members</code> 的生成器。通过源码可以看到，这四个函数调用了上面的四个函数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span>recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> param<span class="token keyword">def</span> <span class="token function">buffers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> name<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_buffers<span class="token punctuation">(</span>recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> buf<span class="token keyword">def</span> <span class="token function">children</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> module<span class="token keyword">def</span> <span class="token function">modules</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">yield</span> module</code></pre><hr><p><code>train</code> 和 <code>eval</code>。</p><p>这两个函数的作用是将模块及其子模块设置成训练/评估模式（<em>training/evaluation mode</em>）。这只对特定的模块起作用，譬如 <code>Dropout</code> 和 <code>BatchNorm</code>。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     self<span class="token punctuation">.</span>training <span class="token operator">=</span> mode    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        module<span class="token punctuation">.</span>train<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>    <span class="token keyword">return</span> self<span class="token keyword">def</span> <span class="token function">eval</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><hr><p><code>requires_grad_</code> 和 <code>zero_grad</code>。</p><p><code>requires_grad_</code> 用于设置模块中的 <code>parameters</code> 是否需要追踪梯度，操作层面来说即 <code>param</code> 的 <code>.requires_grad</code> 是否为 <code>True</code>。</p><p><code>zero_grad</code> 用于将模块中 <code>parameters</code> 的梯度清零。</p><hr><p><code>_get_name</code>、<code>extra_repr</code>、<code>__repr__</code> 和 <code>__dir__</code>。</p><p>这四个函数都是用于输出模块相关信息的，并不复杂，直接看源码即可。</p><hr><p>想要进一步了解的点：</p><ul><li>buffers</li><li>in-place</li><li>hooks</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://pytorch.org/docs/stable/nn.html#module" target="_blank" rel="noopener">PyTorch 官方文档</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> 源码阅读 </tag>
            
            <tag> Module </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【转载】Markdown &amp; HTML 特殊字符转义表</title>
      <link href="info-science/markdown-html-special-characters/"/>
      <url>info-science/markdown-html-special-characters/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Markdown &amp; HTML 特殊字符转义表，方便自己后续查询。</p><a id="more"></a><h2 id="特殊字符转义表"><a href="#特殊字符转义表" class="headerlink" title="特殊字符转义表"></a>特殊字符转义表</h2><p><strong>注意</strong>：</p><ol><li>使用数字代码或英文代码需要在代码后面加一个英文的分号「;」</li><li>第二列转义符号为使用数字代码或英文代码后得的的符号</li></ol><table><thead><tr><th>原符号</th><th>转义符号</th><th>数字代码</th><th>英文代码</th><th>解释</th></tr></thead><tbody><tr><td>制表符</td><td></td><td>&amp;#09</td><td></td><td>/t Horizontal tab</td></tr><tr><td>换行</td><td></td><td>&amp;#10</td><td></td><td>/n Line feed</td></tr><tr><td>回车</td><td></td><td>&amp;#13</td><td></td><td>/r Carriage Return</td></tr><tr><td>空格</td><td></td><td>&amp;#32</td><td>&amp;nbsp</td><td>Space</td></tr><tr><td>!</td><td>!</td><td>&amp;#33</td><td></td><td>惊叹号Exclamation mark</td></tr><tr><td>”</td><td>“</td><td>&amp;#34</td><td>&amp;quot</td><td>双引号Quotation mark</td></tr><tr><td>#</td><td>#</td><td>&amp;#35</td><td></td><td>数字标志Number sign</td></tr><tr><td>$</td><td>$</td><td>&amp;#36</td><td></td><td>美元标志Dollar sign</td></tr><tr><td>%</td><td>%</td><td>&amp;#37</td><td></td><td>百分号Percent sign</td></tr><tr><td>&amp;</td><td>&amp;</td><td>&amp;#38</td><td>&amp;amp</td><td>Ampersand</td></tr><tr><td>‘</td><td>‘</td><td>&amp;#39</td><td></td><td>单引号Apostrophe</td></tr><tr><td>(</td><td>(</td><td>&amp;#40</td><td></td><td>小括号左边部分Left parenthesis</td></tr><tr><td>)</td><td>)</td><td>&amp;#41</td><td></td><td>小括号右边部分Right parenthesis</td></tr><tr><td>*</td><td>*</td><td>&amp;#42</td><td></td><td>星号Asterisk</td></tr><tr><td>+</td><td>+</td><td>&amp;#43</td><td></td><td>加号Plus sign</td></tr><tr><td>,</td><td>,</td><td>&amp;#44</td><td></td><td>逗号Comma</td></tr><tr><td>-</td><td>-</td><td>&amp;#45</td><td></td><td>连字号Hyphen</td></tr><tr><td>.</td><td>.</td><td>&amp;#46</td><td></td><td>句号Period (fullstop)</td></tr><tr><td>/</td><td>/</td><td>&amp;#47</td><td></td><td>斜杠Solidus (slash)</td></tr><tr><td>0</td><td>0</td><td>&amp;#48</td><td></td><td>数字0 Digit 0</td></tr><tr><td>1</td><td>1</td><td>&amp;#49</td><td></td><td>数字1 Digit 1</td></tr><tr><td>2</td><td>2</td><td>&amp;#50</td><td></td><td>数字2 Digit 2</td></tr><tr><td>3</td><td>3</td><td>&amp;#51</td><td></td><td>数字3 Digit 3</td></tr><tr><td>4</td><td>4</td><td>&amp;#52</td><td></td><td>数字4 Digit 4</td></tr><tr><td>5</td><td>5</td><td>&amp;#53</td><td></td><td>数字5 Digit 5</td></tr><tr><td>6</td><td>6</td><td>&amp;#54</td><td></td><td>数字6 Digit 6</td></tr><tr><td>7</td><td>7</td><td>&amp;#55</td><td></td><td>数字7 Digit 7</td></tr><tr><td>8</td><td>8</td><td>&amp;#56</td><td></td><td>数字8 Digit 8</td></tr><tr><td>9</td><td>9</td><td>&amp;#57</td><td></td><td>数字9 Digit 9</td></tr><tr><td>:</td><td>:</td><td>&amp;#58</td><td></td><td>冒号Colon</td></tr><tr><td>;</td><td>;</td><td>&amp;#59</td><td></td><td>分号Semicolon</td></tr><tr><td>&lt;</td><td>&lt;</td><td>&amp;#60</td><td>&amp;lt</td><td>小于号Less than</td></tr><tr><td>=</td><td>=</td><td>&amp;#61</td><td></td><td>等于符号Equals sign</td></tr><tr><td>&gt;</td><td>&gt;</td><td>&amp;#62</td><td>&amp;gt</td><td>大于号Greater than</td></tr><tr><td>?</td><td>?</td><td>&amp;#63</td><td></td><td>问号Question mark</td></tr><tr><td>@</td><td>@</td><td>&amp;#64</td><td></td><td>Commercial at</td></tr><tr><td>A</td><td>A</td><td>&amp;#65</td><td></td><td>大写A Capital A</td></tr><tr><td>B</td><td>B</td><td>&amp;#66</td><td></td><td>大写B Capital B</td></tr><tr><td>C</td><td>C</td><td>&amp;#67</td><td></td><td>大写C Capital C</td></tr><tr><td>D</td><td>D</td><td>&amp;#68</td><td></td><td>大写D Capital D</td></tr><tr><td>E</td><td>E</td><td>&amp;#69</td><td></td><td>大写E Capital E</td></tr><tr><td>F</td><td>F</td><td>&amp;#70</td><td></td><td>大写F Capital F</td></tr><tr><td>G</td><td>G</td><td>&amp;#71</td><td></td><td>大写G Capital G</td></tr><tr><td>H</td><td>H</td><td>&amp;#72</td><td></td><td>大写H Capital H</td></tr><tr><td>I</td><td>I</td><td>&amp;#73</td><td></td><td>大写J Capital I</td></tr><tr><td>J</td><td>J</td><td>&amp;#74</td><td></td><td>大写K Capital J</td></tr><tr><td>K</td><td>K</td><td>&amp;#75</td><td></td><td>大写L Capital K</td></tr><tr><td>L</td><td>L</td><td>&amp;#76</td><td></td><td>大写K Capital L</td></tr><tr><td>M</td><td>M</td><td>&amp;#77</td><td></td><td>大写M Capital M</td></tr><tr><td>N</td><td>N</td><td>&amp;#78</td><td></td><td>大写N Capital N</td></tr><tr><td>O</td><td>O</td><td>&amp;#79</td><td></td><td>大写O Capital O</td></tr><tr><td>P</td><td>P</td><td>&amp;#80</td><td></td><td>大写P Capital P</td></tr><tr><td>Q</td><td>Q</td><td>&amp;#81</td><td></td><td>大写Q Capital Q</td></tr><tr><td>R</td><td>R</td><td>&amp;#82</td><td></td><td>大写R Capital R</td></tr><tr><td>S</td><td>S</td><td>&amp;#83</td><td></td><td>大写S Capital S</td></tr><tr><td>T</td><td>T</td><td>&amp;#84</td><td></td><td>大写T Capital T</td></tr><tr><td>U</td><td>U</td><td>&amp;#85</td><td></td><td>大写U Capital U</td></tr><tr><td>V</td><td>V</td><td>&amp;#86</td><td></td><td>大写V Capital V</td></tr><tr><td>W</td><td>W</td><td>&amp;#87</td><td></td><td>大写W Capital W</td></tr><tr><td>X</td><td>X</td><td>&amp;#88</td><td></td><td>大写X Capital X</td></tr><tr><td>Y</td><td>Y</td><td>&amp;#89</td><td></td><td>大写Y Capital Y</td></tr><tr><td>Z</td><td>Z</td><td>&amp;#90</td><td></td><td>大写Z Capital Z</td></tr><tr><td>[</td><td>[</td><td>&amp;#91</td><td></td><td>中括号左边部分Left square bracket</td></tr><tr><td>\</td><td>\</td><td>&amp;#92</td><td></td><td>反斜杠Reverse solidus (backslash)</td></tr><tr><td>]</td><td>]</td><td>&amp;#93</td><td></td><td>中括号右边部分Right square bracket</td></tr><tr><td>^</td><td>^</td><td>&amp;#94</td><td></td><td>插入符号Caret</td></tr><tr><td>_</td><td>_</td><td>&amp;#95</td><td></td><td>下划线Horizontal bar (underscore)</td></tr><tr><td>`</td><td>`</td><td>&amp;#96</td><td></td><td>尖重音符Acute accent</td></tr><tr><td>a</td><td>a</td><td>&amp;#97</td><td></td><td>小写a Small a</td></tr><tr><td>b</td><td>b</td><td>&amp;#98</td><td></td><td>小写b Small b</td></tr><tr><td>c</td><td>c</td><td>&amp;#99</td><td></td><td>小写c Small c</td></tr><tr><td>d</td><td>d</td><td>&amp;#100</td><td></td><td>小写d Small d</td></tr><tr><td>e</td><td>e</td><td>&amp;#101</td><td></td><td>小写e Small e</td></tr><tr><td>f</td><td>f</td><td>&amp;#102</td><td></td><td>小写f Small f</td></tr><tr><td>g</td><td>g</td><td>&amp;#103</td><td></td><td>小写g Small g</td></tr><tr><td>h</td><td>h</td><td>&amp;#104</td><td></td><td>小写h Small h</td></tr><tr><td>i</td><td>i</td><td>&amp;#105</td><td></td><td>小写i Small i</td></tr><tr><td>j</td><td>j</td><td>&amp;#106</td><td></td><td>小写j Small j</td></tr><tr><td>k</td><td>k</td><td>&amp;#107</td><td></td><td>小写k Small k</td></tr><tr><td>l</td><td>l</td><td>&amp;#108</td><td></td><td>小写l Small l</td></tr><tr><td>m</td><td>m</td><td>&amp;#109</td><td></td><td>小写m Small m</td></tr><tr><td>n</td><td>n</td><td>&amp;#110</td><td></td><td>小写n Small n</td></tr><tr><td>o</td><td>o</td><td>&amp;#111</td><td></td><td>小写o Small o</td></tr><tr><td>p</td><td>p</td><td>&amp;#112</td><td></td><td>小写p Small p</td></tr><tr><td>q</td><td>q</td><td>&amp;#113</td><td></td><td>小写q Small q</td></tr><tr><td>r</td><td>r</td><td>&amp;#114</td><td></td><td>小写r Small r</td></tr><tr><td>s</td><td>s</td><td>&amp;#115</td><td></td><td>小写s Small s</td></tr><tr><td>t</td><td>t</td><td>&amp;#116</td><td></td><td>小写t Small t</td></tr><tr><td>u</td><td>u</td><td>&amp;#117</td><td></td><td>小写u Small u</td></tr><tr><td>v</td><td>v</td><td>&amp;#118</td><td></td><td>小写v Small v</td></tr><tr><td>w</td><td>w</td><td>&amp;#119</td><td></td><td>小写w Small w</td></tr><tr><td>x</td><td>x</td><td>&amp;#120</td><td></td><td>小写x Small x</td></tr><tr><td>y</td><td>y</td><td>&amp;#121</td><td></td><td>小写y Small y</td></tr><tr><td>z</td><td>z</td><td>&amp;#122</td><td></td><td>小写z Small z</td></tr><tr><td>{</td><td>{</td><td>&amp;#123</td><td></td><td>大括号左边部分Left curly brace</td></tr><tr><td>|</td><td>|</td><td>&amp;#124</td><td></td><td>竖线Vertical bar</td></tr><tr><td>}</td><td>}</td><td>&amp;#125</td><td></td><td>大括号右边部分Right curly brace</td></tr><tr><td>~</td><td>~</td><td>&amp;#126</td><td></td><td>Tilde</td></tr><tr><td>�</td><td>�</td><td>&amp;#127</td><td></td><td>未使用Unused</td></tr><tr><td>空格</td><td></td><td>&amp;#160</td><td>&amp;nbsp</td><td>Nonbreaking space</td></tr><tr><td>¡</td><td>¡</td><td>&amp;#161</td><td>&amp;iexcl</td><td>Inverted exclamation</td></tr><tr><td>¢</td><td>¢</td><td>&amp;#162</td><td>&amp;cent</td><td>货币分标志Cent sign</td></tr><tr><td>￡</td><td>£</td><td>&amp;#163</td><td>&amp;pound</td><td>英镑标志Pound sterling</td></tr><tr><td>¤</td><td>¤</td><td>&amp;#164</td><td>&amp;curren</td><td>通用货币标志General currency sign</td></tr><tr><td>￥</td><td>¥</td><td>&amp;#165</td><td>&amp;yen</td><td>日元标志Yen sign</td></tr><tr><td>¦</td><td>¦</td><td>&amp;#166</td><td>&amp;brvbar or &amp;brkbar</td><td>断竖线Broken vertical bar</td></tr><tr><td>§</td><td>§</td><td>&amp;#167</td><td>&amp;sect</td><td>分节号Section sign</td></tr><tr><td>¨</td><td>¨</td><td>&amp;#168</td><td>&amp;uml or &amp;die</td><td>变音符号Umlaut</td></tr><tr><td>©</td><td>©</td><td>&amp;#169</td><td>&amp;copy</td><td>版权标志Copyright</td></tr><tr><td>ª</td><td>ª</td><td>&amp;#170</td><td>&amp;ordf</td><td>Feminine ordinal</td></tr><tr><td>«</td><td>«</td><td>&amp;#171</td><td>&amp;laquo</td><td>Left angle quote, guillemet left</td></tr><tr><td>¬</td><td>¬</td><td>&amp;#172</td><td>&amp;not</td><td>Not sign 否号</td></tr><tr><td></td><td>­</td><td>&amp;#173</td><td>&amp;shy</td><td>Soft hyphen 软连字符</td></tr><tr><td>®</td><td>®</td><td>&amp;#174</td><td>&amp;reg</td><td>注册商标标志Registered trademark</td></tr><tr><td>ˉ</td><td>¯</td><td>&amp;#175</td><td>&amp;macr or &amp;hibar</td><td>长音符号Macron accent</td></tr><tr><td>°</td><td>°</td><td>&amp;#176</td><td>&amp;deg</td><td>度数标志Degree sign</td></tr><tr><td>±</td><td>±</td><td>&amp;#177</td><td>&amp;plusmn</td><td>加或减Plus or minus</td></tr><tr><td>²</td><td>²</td><td>&amp;#178</td><td>&amp;sup2</td><td>上标2 Superscrīpt two</td></tr><tr><td>³</td><td>³</td><td>&amp;#179</td><td>&amp;sup3</td><td>上标3 Superscrīpt three</td></tr><tr><td>′</td><td>´</td><td>&amp;#180</td><td>&amp;acute</td><td>尖重音符Acute accent</td></tr><tr><td>μ</td><td>µ</td><td>&amp;#181</td><td>&amp;micro</td><td>Micro sign</td></tr><tr><td>¶</td><td>¶</td><td>&amp;#182</td><td>&amp;para</td><td>Paragraph sign</td></tr><tr><td>·</td><td>·</td><td>&amp;#183</td><td>&amp;middot</td><td>Middle dot</td></tr><tr><td>¸</td><td>¸</td><td>&amp;#184</td><td>&amp;cedil</td><td>Cedilla</td></tr><tr><td>¹</td><td>¹</td><td>&amp;#185</td><td>&amp;sup1</td><td>上标1 Superscrīpt one</td></tr><tr><td>º</td><td>º</td><td>&amp;#186</td><td>&amp;ordm</td><td>Masculine ordinal</td></tr><tr><td>»</td><td>»</td><td>&amp;#187</td><td>&amp;raquo</td><td>Right angle quote, guillemet right</td></tr><tr><td>¼</td><td>¼</td><td>&amp;#188</td><td>&amp;frac14</td><td>四分之一Fraction one-fourth</td></tr><tr><td>½</td><td>½</td><td>&amp;#189</td><td>&amp;frac12</td><td>二分之一Fraction one-half</td></tr><tr><td>¾</td><td>¾</td><td>&amp;#190</td><td>&amp;frac34</td><td>四分之三Fraction three-fourths</td></tr><tr><td>¿</td><td>¿</td><td>&amp;#191</td><td>&amp;iquest</td><td>倒问号Inverted question mark</td></tr><tr><td>À</td><td>À</td><td>&amp;#192</td><td>&amp;Agrave</td><td>Capital A, grave accent</td></tr><tr><td>Á</td><td>Á</td><td>&amp;#193</td><td>&amp;Aacute</td><td>Capital A, acute accent</td></tr><tr><td>Â</td><td>Â</td><td>&amp;#194</td><td>&amp;Acirc</td><td>Capital A, circumflex</td></tr><tr><td>Ã</td><td>Ã</td><td>&amp;#195</td><td>&amp;Atilde</td><td>Capital A, tilde</td></tr><tr><td>Ä</td><td>Ä</td><td>&amp;#196</td><td>&amp;Auml</td><td>Capital A, di?esis / umlaut</td></tr><tr><td>Å</td><td>Å</td><td>&amp;#197</td><td>&amp;Aring</td><td>Capital A, ring</td></tr><tr><td>Æ</td><td>Æ</td><td>&amp;#198</td><td>&amp;AElig</td><td>Capital AE ligature</td></tr><tr><td>Ç</td><td>Ç</td><td>&amp;#199</td><td>&amp;Ccedil</td><td>Capital C, cedilla</td></tr><tr><td>È</td><td>È</td><td>&amp;#200</td><td>&amp;Egrave</td><td>Capital E, grave accent</td></tr><tr><td>É</td><td>É</td><td>&amp;#201</td><td>&amp;Eacute</td><td>Capital E, acute accent</td></tr><tr><td>Ê</td><td>Ê</td><td>&amp;#202</td><td>&amp;Ecirc</td><td>Capital E, circumflex</td></tr><tr><td>Ë</td><td>Ë</td><td>&amp;#203</td><td>&amp;Euml</td><td>Capital E, di?esis / umlaut</td></tr><tr><td>ì</td><td>Ì</td><td>&amp;#204</td><td>&amp;Igrave</td><td>Capital I, grave accent</td></tr><tr><td>í</td><td>Í</td><td>&amp;#205</td><td>&amp;Iacute</td><td>Capital I, acute accent</td></tr><tr><td>Î</td><td>Î</td><td>&amp;#206</td><td>&amp;Icirc</td><td>Capital I, circumflex</td></tr><tr><td>Ï</td><td>Ï</td><td>&amp;#207</td><td>&amp;Iuml</td><td>Capital I, di?esis / umlaut</td></tr><tr><td>Ð</td><td>Ð</td><td>&amp;#208</td><td>&amp;ETH</td><td>Capital Eth, Icelandic</td></tr><tr><td>Ñ</td><td>Ñ</td><td>&amp;#209</td><td>&amp;Ntilde</td><td>Capital N, tilde</td></tr><tr><td>Ò</td><td>Ò</td><td>&amp;#210</td><td>&amp;Ograve</td><td>Capital O, grave accent</td></tr><tr><td>Ó</td><td>Ó</td><td>&amp;#211</td><td>&amp;Oacute</td><td>Capital O, acute accent</td></tr><tr><td>Ô</td><td>Ô</td><td>&amp;#212</td><td>&amp;Ocirc</td><td>Capital O, circumflex</td></tr><tr><td>Õ</td><td>Õ</td><td>&amp;#213</td><td>&amp;Otilde</td><td>Capital O, tilde</td></tr><tr><td>Ö</td><td>Ö</td><td>&amp;#214</td><td>&amp;Ouml</td><td>Capital O, di?esis / umlaut</td></tr><tr><td>×</td><td>×</td><td>&amp;#215</td><td>&amp;times</td><td>乘号Multiply sign</td></tr><tr><td>Ø</td><td>Ø</td><td>&amp;#216</td><td>&amp;Oslash</td><td>Capital O, slash</td></tr><tr><td>ù</td><td>Ù</td><td>&amp;#217</td><td>&amp;Ugrave</td><td>Capital U, grave accent</td></tr><tr><td>ú</td><td>Ú</td><td>&amp;#218</td><td>&amp;Uacute</td><td>Capital U, acute accent</td></tr><tr><td>Û</td><td>Û</td><td>&amp;#219</td><td>&amp;Ucirc</td><td>Capital U, circumflex</td></tr><tr><td>ü</td><td>Ü</td><td>&amp;#220</td><td>&amp;Uuml</td><td>Capital U, di?esis / umlaut</td></tr><tr><td>Ý</td><td>Ý</td><td>&amp;#221</td><td>&amp;Yacute</td><td>Capital Y, acute accent</td></tr><tr><td>Þ</td><td>Þ</td><td>&amp;#222</td><td>&amp;THORN</td><td>Capital Thorn, Icelandic</td></tr><tr><td>ß</td><td>ß</td><td>&amp;#223</td><td>&amp;szlig</td><td>Small sharp s, German sz</td></tr><tr><td>à</td><td>à</td><td>&amp;#224</td><td>&amp;agrave</td><td>Small a, grave accent</td></tr><tr><td>á</td><td>á</td><td>&amp;#225</td><td>&amp;aacute</td><td>Small a, acute accent</td></tr><tr><td>â</td><td>â</td><td>&amp;#226</td><td>&amp;acirc</td><td>Small a, circumflex</td></tr><tr><td>ã</td><td>ã</td><td>&amp;#227</td><td>&amp;atilde</td><td>Small a, tilde</td></tr><tr><td>ä</td><td>ä</td><td>&amp;#228</td><td>&amp;auml</td><td>Small a, di?esis / umlaut</td></tr><tr><td>å</td><td>å</td><td>&amp;#229</td><td>&amp;aring</td><td>Small a, ring</td></tr><tr><td>æ</td><td>æ</td><td>&amp;#230</td><td>&amp;aelig</td><td>Small ae ligature</td></tr><tr><td>ç</td><td>ç</td><td>&amp;#231</td><td>&amp;ccedil</td><td>Small c, cedilla</td></tr><tr><td>è</td><td>è</td><td>&amp;#232</td><td>&amp;egrave</td><td>Small e, grave accent</td></tr><tr><td>é</td><td>é</td><td>&amp;#233</td><td>&amp;eacute</td><td>Small e, acute accent</td></tr><tr><td>ê</td><td>ê</td><td>&amp;#234</td><td>&amp;ecirc</td><td>Small e, circumflex</td></tr><tr><td>ë</td><td>ë</td><td>&amp;#235</td><td>&amp;euml</td><td>Small e, di?esis / umlaut</td></tr><tr><td>ì</td><td>ì</td><td>&amp;#236</td><td>&amp;igrave</td><td>Small i, grave accent</td></tr><tr><td>í</td><td>í</td><td>&amp;#237</td><td>&amp;iacute</td><td>Small i, acute accent</td></tr><tr><td>î</td><td>î</td><td>&amp;#238</td><td>&amp;icirc</td><td>Small i, circumflex</td></tr><tr><td>ï</td><td>ï</td><td>&amp;#239</td><td>&amp;iuml</td><td>Small i, di?esis / umlaut</td></tr><tr><td>ð</td><td>ð</td><td>&amp;#240</td><td>&amp;eth</td><td>Small eth, Icelandic</td></tr><tr><td>ñ</td><td>ñ</td><td>&amp;#241</td><td>&amp;ntilde</td><td>Small n, tilde</td></tr><tr><td>ò</td><td>ò</td><td>&amp;#242</td><td>&amp;ograve</td><td>Small o, grave accent</td></tr><tr><td>ó</td><td>ó</td><td>&amp;#243</td><td>&amp;oacute</td><td>Small o, acute accent</td></tr><tr><td>ô</td><td>ô</td><td>&amp;#244</td><td>&amp;ocirc</td><td>Small o, circumflex</td></tr><tr><td>õ</td><td>õ</td><td>&amp;#245</td><td>&amp;otilde</td><td>Small o, tilde</td></tr><tr><td>ö</td><td>ö</td><td>&amp;#246</td><td>&amp;ouml</td><td>Small o, di?esis / umlaut</td></tr><tr><td>÷</td><td>÷</td><td>&amp;#247</td><td>&amp;divide</td><td>除号Division sign</td></tr><tr><td>ø</td><td>ø</td><td>&amp;#248</td><td>&amp;oslash</td><td>Small o, slash</td></tr><tr><td>ù</td><td>ù</td><td>&amp;#249</td><td>&amp;ugrave</td><td>Small u, grave accent</td></tr><tr><td>ú</td><td>ú</td><td>&amp;#250</td><td>&amp;uacute</td><td>Small u, acute accent</td></tr><tr><td>û</td><td>û</td><td>&amp;#251</td><td>&amp;ucirc</td><td>Small u, circumflex</td></tr><tr><td>ü</td><td>ü</td><td>&amp;#252</td><td>&amp;uuml</td><td>Small u, di?esis / umlaut</td></tr><tr><td>ý</td><td>ý</td><td>&amp;#253</td><td>&amp;yacute</td><td>Small y, acute accent</td></tr><tr><td>þ</td><td>þ</td><td>&amp;#254</td><td>&amp;thorn</td><td>Small thorn, Icelandic</td></tr><tr><td>ÿ</td><td>ÿ</td><td>&amp;#255</td><td>&amp;yuml</td><td>Small y, umlaut</td></tr><tr><td>ƒ</td><td>ƒ</td><td></td><td>&amp;fnof</td><td></td></tr><tr><td>Α</td><td>Α</td><td></td><td>&amp;Alpha</td><td></td></tr><tr><td>Β</td><td>Β</td><td></td><td>&amp;Beta</td><td></td></tr><tr><td>Γ</td><td>Γ</td><td></td><td>&amp;Gamma</td><td></td></tr><tr><td>Δ</td><td>Δ</td><td></td><td>&amp;Delta</td><td></td></tr><tr><td>Ε</td><td>Ε</td><td></td><td>&amp;Epsilon</td><td></td></tr><tr><td>Ζ</td><td>Ζ</td><td></td><td>&amp;Zeta</td><td></td></tr><tr><td>Η</td><td>Η</td><td></td><td>&amp;Eta</td><td></td></tr><tr><td>Θ</td><td>Θ</td><td></td><td>&amp;Theta</td><td></td></tr><tr><td>Ι</td><td>Ι</td><td></td><td>&amp;Iota</td><td></td></tr><tr><td>Κ</td><td>Κ</td><td></td><td>&amp;Kappa</td><td></td></tr><tr><td>Λ</td><td>Λ</td><td></td><td>&amp;Lambda</td><td></td></tr><tr><td>Μ</td><td>Μ</td><td></td><td>&amp;Mu</td><td></td></tr><tr><td>Ν</td><td>Ν</td><td></td><td>&amp;Nu</td><td></td></tr><tr><td>Ξ</td><td>Ξ</td><td></td><td>&amp;Xi</td><td></td></tr><tr><td>Ο</td><td>Ο</td><td></td><td>&amp;Omicron</td><td></td></tr><tr><td>Π</td><td>Π</td><td></td><td>&amp;Pi</td><td></td></tr><tr><td>Ρ</td><td>Ρ</td><td></td><td>&amp;Rho</td><td></td></tr><tr><td>Σ</td><td>Σ</td><td></td><td>&amp;Sigma</td><td></td></tr><tr><td>Τ</td><td>Τ</td><td></td><td>&amp;Tau</td><td></td></tr><tr><td>Υ</td><td>Υ</td><td></td><td>&amp;Upsilon</td><td></td></tr><tr><td>Φ</td><td>Φ</td><td></td><td>&amp;Phi</td><td></td></tr><tr><td>Χ</td><td>Χ</td><td></td><td>&amp;Chi</td><td></td></tr><tr><td>Ψ</td><td>Ψ</td><td></td><td>&amp;Psi</td><td></td></tr><tr><td>Ω</td><td>Ω</td><td></td><td>&amp;Omega</td><td></td></tr><tr><td>α</td><td>α</td><td></td><td>&amp;alpha</td><td></td></tr><tr><td>β</td><td>β</td><td></td><td>&amp;beta</td><td></td></tr><tr><td>γ</td><td>γ</td><td></td><td>&amp;gamma</td><td></td></tr><tr><td>δ</td><td>δ</td><td></td><td>&amp;delta</td><td></td></tr><tr><td>ε</td><td>ε</td><td></td><td>&amp;epsilon</td><td></td></tr><tr><td>ζ</td><td>ζ</td><td></td><td>&amp;zeta</td><td></td></tr><tr><td>η</td><td>η</td><td></td><td>&amp;eta</td><td></td></tr><tr><td>θ</td><td>θ</td><td></td><td>&amp;theta</td><td></td></tr><tr><td>ι</td><td>ι</td><td></td><td>&amp;iota</td><td></td></tr><tr><td>κ</td><td>κ</td><td></td><td>&amp;kappa</td><td></td></tr><tr><td>λ</td><td>λ</td><td></td><td>&amp;lambda</td><td></td></tr><tr><td>μ</td><td>μ</td><td></td><td>&amp;mu</td><td></td></tr><tr><td>ν</td><td>ν</td><td></td><td>&amp;nu</td><td></td></tr><tr><td>ξ</td><td>ξ</td><td></td><td>&amp;xi</td><td></td></tr><tr><td>ο</td><td>ο</td><td></td><td>&amp;omicron</td><td></td></tr><tr><td>π</td><td>π</td><td></td><td>&amp;pi</td><td></td></tr><tr><td>ρ</td><td>ρ</td><td></td><td>&amp;rho</td><td></td></tr><tr><td>ς</td><td>ς</td><td></td><td>&amp;sigmaf</td><td></td></tr><tr><td>σ</td><td>σ</td><td></td><td>&amp;sigma</td><td></td></tr><tr><td>τ</td><td>τ</td><td></td><td>&amp;tau</td><td></td></tr><tr><td>υ</td><td>υ</td><td></td><td>&amp;upsilon</td><td></td></tr><tr><td>φ</td><td>φ</td><td></td><td>&amp;phi</td><td></td></tr><tr><td>χ</td><td>χ</td><td></td><td>&amp;chi</td><td></td></tr><tr><td>ψ</td><td>ψ</td><td></td><td>&amp;psi</td><td></td></tr><tr><td>ω</td><td>ω</td><td></td><td>&amp;omega</td><td></td></tr><tr><td>ϑ</td><td>ϑ</td><td></td><td>&amp;thetasym</td><td></td></tr><tr><td>ϒ</td><td>ϒ</td><td></td><td>&amp;upsih</td><td></td></tr><tr><td>ϖ</td><td>ϖ</td><td></td><td>&amp;piv</td><td></td></tr><tr><td>•</td><td>•</td><td></td><td>&amp;bull</td><td></td></tr><tr><td>…</td><td>…</td><td></td><td>&amp;hellip</td><td></td></tr><tr><td>′</td><td>′</td><td></td><td>&amp;prime</td><td></td></tr><tr><td>″</td><td>″</td><td></td><td>&amp;Prime</td><td></td></tr><tr><td>￣</td><td>‾</td><td></td><td>&amp;oline</td><td></td></tr><tr><td>⁄</td><td>⁄</td><td></td><td>&amp;frasl</td><td></td></tr><tr><td>℘</td><td>℘</td><td></td><td>&amp;weierp</td><td></td></tr><tr><td>ℑ</td><td>ℑ</td><td></td><td>&amp;image</td><td></td></tr><tr><td>ℜ</td><td>ℜ</td><td></td><td>&amp;real</td><td></td></tr><tr><td>™</td><td>™</td><td></td><td>&amp;trade</td><td></td></tr><tr><td>ℵ</td><td>ℵ</td><td></td><td>&amp;alefsym</td><td></td></tr><tr><td>←</td><td>←</td><td></td><td>&amp;larr</td><td></td></tr><tr><td>↑</td><td>↑</td><td></td><td>&amp;uarr</td><td></td></tr><tr><td>→</td><td>→</td><td></td><td>&amp;rarr</td><td></td></tr><tr><td>↓</td><td>↓</td><td></td><td>&amp;darr</td><td></td></tr><tr><td>↔</td><td>↔</td><td></td><td>&amp;harr</td><td></td></tr><tr><td>↵</td><td>↵</td><td></td><td>&amp;crarr</td><td></td></tr><tr><td>⇐</td><td>⇐</td><td></td><td>&amp;lArr</td><td></td></tr><tr><td>⇑</td><td>⇑</td><td></td><td>&amp;uArr</td><td></td></tr><tr><td>⇒</td><td>⇒</td><td></td><td>&amp;rArr</td><td></td></tr><tr><td>⇓</td><td>⇓</td><td></td><td>&amp;dArr</td><td></td></tr><tr><td>⇔</td><td>⇔</td><td></td><td>&amp;hArr</td><td></td></tr><tr><td>∀</td><td>∀</td><td></td><td>&amp;forall</td><td></td></tr><tr><td>∂</td><td>∂</td><td></td><td>&amp;part</td><td></td></tr><tr><td>∃</td><td>∃</td><td></td><td>&amp;exist</td><td></td></tr><tr><td>∅</td><td>∅</td><td></td><td>&amp;empty</td><td></td></tr><tr><td>∇</td><td>∇</td><td></td><td>&amp;nabla</td><td></td></tr><tr><td>∈</td><td>∈</td><td></td><td>&amp;isin</td><td></td></tr><tr><td>∉</td><td>∉</td><td></td><td>&amp;notin</td><td></td></tr><tr><td>∋</td><td>∋</td><td></td><td>&amp;ni</td><td></td></tr><tr><td>∏</td><td>∏</td><td></td><td>&amp;prod</td><td></td></tr><tr><td>∑</td><td>∑</td><td></td><td>&amp;sum</td><td></td></tr><tr><td>−</td><td>−</td><td></td><td>&amp;minus</td><td></td></tr><tr><td>∗</td><td>∗</td><td></td><td>&amp;lowast</td><td></td></tr><tr><td>√</td><td>√</td><td></td><td>&amp;radic</td><td></td></tr><tr><td>∝</td><td>∝</td><td></td><td>&amp;prop</td><td></td></tr><tr><td>∞</td><td>∞</td><td></td><td>&amp;infin</td><td></td></tr><tr><td>∠</td><td>∠</td><td></td><td>&amp;ang</td><td></td></tr><tr><td>∧</td><td>∧</td><td></td><td>&amp;and</td><td></td></tr><tr><td>∨</td><td>∨</td><td></td><td>&amp;or</td><td></td></tr><tr><td>∩</td><td>∩</td><td></td><td>&amp;cap</td><td></td></tr><tr><td>∪</td><td>∪</td><td></td><td>&amp;cup</td><td></td></tr><tr><td>∫</td><td>∫</td><td></td><td>&amp;int</td><td></td></tr><tr><td>∴</td><td>∴</td><td></td><td>&amp;there4</td><td></td></tr><tr><td>～</td><td>∼</td><td></td><td>&amp;sim</td><td></td></tr><tr><td>≅</td><td>≅</td><td></td><td>&amp;cong</td><td></td></tr><tr><td>≈</td><td>≈</td><td></td><td>&amp;asymp</td><td></td></tr><tr><td>≠</td><td>≠</td><td></td><td>&amp;ne</td><td></td></tr><tr><td>≡</td><td>≡</td><td></td><td>&amp;equiv</td><td></td></tr><tr><td>≤</td><td>≤</td><td></td><td>&amp;le</td><td></td></tr><tr><td>≥</td><td>≥</td><td></td><td>&amp;ge</td><td></td></tr><tr><td>⊂</td><td>⊂</td><td></td><td>&amp;sub</td><td></td></tr><tr><td>⊃</td><td>⊃</td><td></td><td>&amp;sup</td><td></td></tr><tr><td>⊄</td><td>⊄</td><td></td><td>&amp;nsub</td><td></td></tr><tr><td>⊆</td><td>⊆</td><td></td><td>&amp;sube</td><td></td></tr><tr><td>⊇</td><td>⊇</td><td></td><td>&amp;supe</td><td></td></tr><tr><td>⊕</td><td>⊕</td><td></td><td>&amp;oplus</td><td></td></tr><tr><td>⊗</td><td>⊗</td><td></td><td>&amp;otimes</td><td></td></tr><tr><td>⊥</td><td>⊥</td><td></td><td>&amp;perp</td><td></td></tr><tr><td>⋅</td><td>⋅</td><td></td><td>&amp;sdot</td><td></td></tr><tr><td>⌈</td><td>⌈</td><td></td><td>&amp;lceil</td><td></td></tr><tr><td>⌉</td><td>⌉</td><td></td><td>&amp;rceil</td><td></td></tr><tr><td>⌊</td><td>⌊</td><td></td><td>&amp;lfloor</td><td></td></tr><tr><td>⌋</td><td>⌋</td><td></td><td>&amp;rfloor</td><td></td></tr><tr><td>⟨</td><td>⟨</td><td></td><td>&amp;lang</td><td></td></tr><tr><td>⟩</td><td>⟩</td><td></td><td>&amp;rang</td><td></td></tr><tr><td>◊</td><td>◊</td><td></td><td>&amp;loz</td><td></td></tr><tr><td>♠</td><td>♠</td><td></td><td>&amp;spades</td><td></td></tr><tr><td>♣</td><td>♣</td><td></td><td>&amp;clubs</td><td></td></tr><tr><td>♥</td><td>♥</td><td></td><td>&amp;hearts</td><td></td></tr><tr><td>♦</td><td>♦</td><td></td><td>&amp;diams</td><td></td></tr><tr><td>“</td><td>“</td><td></td><td>&amp;quot</td><td></td></tr><tr><td>&amp;</td><td>&amp;</td><td></td><td>&amp;amp</td><td></td></tr><tr><td>&lt;</td><td>&lt;</td><td></td><td>&amp;lt</td><td></td></tr><tr><td>&gt;</td><td>&gt;</td><td></td><td>&amp;gt</td><td></td></tr><tr><td>Œ</td><td>Œ</td><td></td><td>&amp;OElig</td><td></td></tr><tr><td>œ</td><td>œ</td><td></td><td>&amp;oelig</td><td></td></tr><tr><td>Š</td><td>Š</td><td></td><td>&amp;Scaron</td><td></td></tr><tr><td>š</td><td>š</td><td></td><td>&amp;scaron</td><td></td></tr><tr><td>Ÿ</td><td>Ÿ</td><td></td><td>&amp;Yuml</td><td></td></tr><tr><td>ˆ</td><td>ˆ</td><td></td><td>&amp;circ</td><td></td></tr><tr><td>˜</td><td>˜</td><td></td><td>&amp;tilde</td><td></td></tr><tr><td></td><td></td><td></td><td>&amp;ensp</td><td>半个空白位</td></tr><tr><td></td><td></td><td></td><td>&amp;emsp</td><td>一个空白位</td></tr><tr><td></td><td></td><td></td><td>&amp;thinsp</td><td>窄空格</td></tr><tr><td></td><td>‌</td><td>&amp;#8204</td><td>&amp;zwnj</td><td>零宽不连字</td></tr><tr><td></td><td>‍</td><td>&amp;#8205</td><td>&amp;zwj</td><td>零宽连字</td></tr><tr><td></td><td>‎</td><td></td><td>&amp;lrm</td><td>隐式定向格式化字符Left-to-Right</td></tr><tr><td></td><td>‏</td><td></td><td>&amp;rlm</td><td>隐式定向格式化字符Right-to-Left</td></tr><tr><td>–</td><td>–</td><td></td><td>&amp;ndash</td><td></td></tr><tr><td>—</td><td>—</td><td></td><td>&amp;mdash</td><td></td></tr><tr><td>‘</td><td>‘</td><td></td><td>&amp;lsquo</td><td></td></tr><tr><td>’</td><td>’</td><td></td><td>&amp;rsquo</td><td></td></tr><tr><td>‚</td><td>‚</td><td></td><td>&amp;sbquo</td><td></td></tr><tr><td>“</td><td>“</td><td></td><td>&amp;ldquo</td><td></td></tr><tr><td>”</td><td>”</td><td></td><td>&amp;rdquo</td><td></td></tr><tr><td>„</td><td>„</td><td></td><td>&amp;bdquo</td><td></td></tr><tr><td>†</td><td>†</td><td></td><td>&amp;dagger</td><td>剑号，用来标志一个死者的姓名</td></tr><tr><td>‡</td><td>‡</td><td></td><td>&amp;Dagger</td><td>双剑号</td></tr><tr><td>‰</td><td>‰</td><td></td><td>&amp;permil</td><td>千分之</td></tr><tr><td>‹</td><td>‹</td><td></td><td>&amp;lsaquo</td><td></td></tr><tr><td>›</td><td>›</td><td></td><td>&amp;rsaquo</td><td></td></tr><tr><td>€</td><td>€</td><td></td><td>&amp;euro</td><td>欧元</td></tr></tbody></table><h2 id="转载声明"><a href="#转载声明" class="headerlink" title="转载声明"></a>转载声明</h2><p>本文转自网络文章 <a href="https://www.cnblogs.com/yifeiyu/p/11402743.html" target="_blank" rel="noopener">markdown、html转义特殊字符代码大全</a>，转载仅为个人收藏、知识分享，版权归原作者所有。如有侵权，请联系<a href="mailto:atomicoo95@gmail.com">博主</a>进行删除。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 转载 </tag>
            
            <tag> Markdown </tag>
            
            <tag> HTML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch 自动微分：Autograd</title>
      <link href="info-science/pytorch-auto-diff-autograd/"/>
      <url>info-science/pytorch-auto-diff-autograd/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>PyTorch 是一个开源的 Python 机器学习库，提供了两个高级功能：</p><ul><li>具有强大的 GPU 加速的张量计算</li><li>包含自动求导系统的的深度神经网络（Autograd）</li></ul><p><strong>Autograd</strong> 包是 PyTorch 所有神经网络的核心，为张量上的所有操作提供自动求导机制。它是一个运行时定义的框架，即反向传播是随着对张量的操作来逐步决定的，这也意味着在每个迭代中都是可以不同的。</p><p>现在由于很多封装好的 API 的存在，导致我们在搭建自己的网络的时候并不需要过多地去关注求导这个问题，但如果能够对这个自动求导机制有所了解的话，对于我们写出更优雅更高效的代码无疑是帮助极大的。本文会简单介绍 PyTorch 的 Autograd 机制。</p><a id="more"></a><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>首先我们用一个简单的类比来了解一下什么是<strong>计算图</strong>：可以将计算图想象成一个复杂的管道结构，张量（数据）在其中通过特定的路径从入口向出口缓缓流动（这也就是 TensorFlow 中 <strong>Flow</strong> 的由来），每次张量从入口“流”到出口就表示完成了一次正向传播。在计算图中有两个最重要的元素：<strong>Tensor</strong> 和 <strong>Function</strong>，两者互相连接生成一个有向无环图（计算图），编码了完整的计算历史。</p><h3 id="Tensor：n-维向量"><a href="#Tensor：n-维向量" class="headerlink" title="Tensor：n 维向量"></a>Tensor：n 维向量</h3><p>张量 Tensor 可以理解为 n 维的向量，<code>Tensor</code> 是包中的核心类，通过将某个张量的属性 <code>.requires_grad</code> （默认为 <code>False</code>）设置为 <code>True</code> 可以追踪该张量上发生的所有操作，在完成计算之后可以通过 <code>.backward()</code> 反向传播来自动计算所有<strong>梯度</strong>（Gradient），并累加到 <code>.grad</code> 属性中。这就是 PyTorch 的自动求导。</p><pre class=" language-sh"><code class="language-sh">$ x = torch.ones(2, 2, requires_grad=True); print(x)tensor([[1., 1.],        [1., 1.]], requires_grad=True)</code></pre><blockquote><p>Note：可以通过调用 <code>.detach()</code> 方法阻止一个张量被追踪，或者通过将代码块包裹在 <code>with torch.no_grad():</code> 中，这点在评估模型时非常实用（因为在模型评估阶段并不需要进行梯度计算）。</p></blockquote><h3 id="Function：张量运算"><a href="#Function：张量运算" class="headerlink" title="Function：张量运算"></a>Function：张量运算</h3><p>在 PyTorch 的自动求导机制中，除了 <code>Tensor</code> 之外，还有另一个类 <code>Function</code> 也是非常重要的。</p><p>Function 指的是在计算图中对张量 Tensor 所进行的运算，譬如加减乘除卷积等。<code>Function</code> 内部有 <code>.forward()</code> 和 <code>.backward()</code> 两个方法，分别用于正向传播与反向传播。</p><p>我们的计算图在做正向传播的时候，除了 <code>.forward()</code> 本身的操作之外，还会自动为反向传播做准备，添加相应的 Function 节点，保存在张量的 <code>.grad_fn</code> 属性中。</p><pre class=" language-sh"><code class="language-sh">$ y = x + 2; print(y)tensor([[3., 3.],        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p>在计算图需要反向传播求梯度时只需调用 <code>.backward()</code> 即可。上例中，反向传播时需要做的操作就是 <code>&lt;AddBackward0&gt;</code>。</p><h3 id="Gradient：方向导数"><a href="#Gradient：方向导数" class="headerlink" title="Gradient：方向导数"></a>Gradient：方向导数</h3><p>梯度：方向导数，即函数在该点处沿着该方向变化最快，变化率最大（梯度的模）。</p><h2 id="一个实例"><a href="#一个实例" class="headerlink" title="一个实例"></a>一个实例</h2><p>对自动求导机制有基本的了解之后可以来看一个简单的实例：</p><pre class=" language-sh"><code class="language-sh">l1 = input x w1l2 = l1 + w2l3 = l1 x w3l4 = l2 x l3loss = mean(l4)</code></pre><p>可以画出对应的计算图如下：</p><p><img src="https://i.loli.net/2020/07/04/9PUHp5aoE2JOvLR.jpg" alt="1781041624f4c9fb31df04d11dd6a84a.jpg" loading="lazy"></p><blockquote><p>Note：其中 <code>input</code> 张量在实操时 <code>.requires_grad</code> 应设置为 <code>False</code>，并不需要进行追踪（因为在实操中 <code>input</code> 对应的是数据输入，并不是可训练参数，自然无需求导）。</p></blockquote><p>手推结果：</p><pre class=" language-sh"><code class="language-sh">in = [1., 1., 1., 1.]w1 = [2., 2., 2., 2.]w2 = [3., 3., 3., 3.]w3 = [4., 4., 4., 4.]l1 = in x w1 = [2., 2., 2., 2.]l2 = l1 + w2 = [5., 5., 5., 5.]l3 = l1 x w3 = [8., 8., 8., 8.]l4 = l2 x l3 = [40., 40., 40., 40.]ls = 40.dls/dl4 = [0.25, 0.25, 0.25, 0.25]dl4/dl3 = [5., 5., 5., 5.]dl4/dl2 = [8., 8., 8., 8.]dl3/dl1 = [4., 4., 4., 4.]dl3/dw3 = [2., 2., 2., 2.]dl2/dl1 = [1., 1., 1., 1.]dl2/dw2 = [1., 1., 1., 1.]dl1/dw1 = [1., 1., 1., 1.]dls/dw3 = dls/dl4 * dl4/dl3 * dl3/dw3 = [2.5, 2.5, 2.5, 2.5]dls/dw2 = dls/dl4 * dl4/dl2 * dl2/dw2 = [2.0, 2.0, 2.0, 2.0]dls/dw1 = dls/dl4 * dl4/dl3 * dl3/dl1 * dl1/dw1        + dls/dl4 * dl4/dl2 * dl2/dl1 * dl1/dw1        = [7., 7., 7., 7.]</code></pre><p>验证结果：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torchinput <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>w3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>l1 <span class="token operator">=</span> input <span class="token operator">*</span> w1l2 <span class="token operator">=</span> l1 <span class="token operator">+</span> w2l3 <span class="token operator">=</span> l1 <span class="token operator">*</span> w3l4 <span class="token operator">=</span> l2 <span class="token operator">*</span> l3loss <span class="token operator">=</span> l4<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w1<span class="token punctuation">.</span>data<span class="token punctuation">,</span> w1<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> w1<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>l1<span class="token punctuation">.</span>data<span class="token punctuation">,</span> l1<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> l1<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Out:</span><span class="token comment" spellcheck="true"># tensor(2.)  None  None</span><span class="token comment" spellcheck="true"># tensor([[2., 2.],</span><span class="token comment" spellcheck="true">#         [2., 2.]])  None  &lt;MulBackward0 object at 0x000001A9FC571E48></span><span class="token comment" spellcheck="true"># tensor(40.)  None  &lt;MeanBackward0 object at 0x000001A9FC571E10></span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w1<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> w2<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> w3<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>l1<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> l2<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> l3<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> l4<span class="token punctuation">.</span>grad<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Out:</span><span class="token comment" spellcheck="true"># tensor(28.)  tensor(8.)  tensor(10.)</span><span class="token comment" spellcheck="true"># None  None  None  None  None</span></code></pre><p>这个例子中给定的参数 <code>w</code> 都只是常数，且涉及的运算都是加减乘除这类简单运算，但换成卷积之类的复杂运算之后原理仍然是一样的，只不过计算过程变得复杂了而已。</p><blockquote><p>Note：可以看到 l1 ~ l4 的 <code>.grad</code> 都为 None，这其实是为了节省内存，所以默认并不会保存中间结果。可以通过调用 <code>tensor.retain_grad()</code> 来进行保存，或者通过调用 <code>tensor.register_hook</code> 来进行输出（并不保存）</p></blockquote><pre class=" language-python"><code class="language-python">l1<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>l1<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'l1 grad: '</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/67184419" target="_blank" rel="noopener">浅谈 PyTorch 中的 tensor 及使用</a></p><p><a href="https://zhuanlan.zhihu.com/p/69294347" target="_blank" rel="noopener">PyTorch 的 Autograd</a></p><p><a href="https://baijiahao.baidu.com/s?id=1659101311176614112&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">PyTorch自动求导：Autograd</a></p><p><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener">PyTorch 官方英文教程</a></p><p><a href="https://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">PyTorch 官方英文文档</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 信息时代 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> 自动微分 </tag>
            
            <tag> Autograd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建立你自己的个人静态网站</title>
      <link href="everything/how-to-build-your-site/"/>
      <url>everything/how-to-build-your-site/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文介绍如何利用 Github Pages + Hexo 搭建自己的个人博客静态网站</p><ul><li><p><strong>Github Pages</strong> 可直接从 Github 仓库创建网站</p></li><li><p><strong>Hexo</strong> 是一个快速、简洁且高效的博客框架</p></li></ul><p>我会尽量将建立过程讲述清楚，但清楚并不等于事无巨细，一些基础操作（例如注册账号等）会用简单的一句话略过</p><a id="more"></a><h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><h3 id="Node-js-和-Git"><a href="#Node-js-和-Git" class="headerlink" title="Node.js 和 Git"></a>Node.js 和 Git</h3><h4 id="Node-JS"><a href="#Node-JS" class="headerlink" title="Node.JS"></a>Node.JS</h4><p>下载 <a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.JS</a> 并安装</p><p>验证安装</p><pre class=" language-sh"><code class="language-sh">node -vnpm -v</code></pre><p>国内可使用 <code>cnpm</code> 加速</p><pre class=" language-sh"><code class="language-sh">npm install -g cnpm --registry=https://registry.npm.taobao.org</code></pre><p>验证安装</p><pre class=" language-sh"><code class="language-sh">cnpm -v</code></pre><h4 id="Git-Github"><a href="#Git-Github" class="headerlink" title="Git/Github"></a>Git/Github</h4><p>下载 <a href="https://git-scm.com/" target="_blank" rel="noopener">Git</a> 并安装</p><p>验证安装</p><pre class=" language-sh"><code class="language-sh">git --version</code></pre><h3 id="Github-仓库"><a href="#Github-仓库" class="headerlink" title="Github 仓库"></a>Github 仓库</h3><p>创建 Github 仓库，仓库名称格式为 <code>&lt;username&gt;.github.io</code></p><p><img src="https://i.loli.net/2020/06/26/n6LOXk3E2V5crIM.png" alt="1592987275265" loading="lazy"></p><p>并在 setting 中勾选 githubpage</p><p><img src="https://i.loli.net/2020/06/26/f5aTZ3YiIeWhF2b.png" alt="1592987502534.png" loading="lazy"></p><p>完成后即可通过 <a href="https://atomicoo.github.io/" target="_blank" rel="noopener"><code>https://&lt;username&gt;.github.io/</code></a> 访问</p><h3 id="Hexo-配置博客"><a href="#Hexo-配置博客" class="headerlink" title="Hexo 配置博客"></a>Hexo 配置博客</h3><p>Github Pages 支持 Jelly 和 Hexo 来搭建静态界面，此处选择使用 Hexo 搭建</p><h4 id="初始化-Hexo"><a href="#初始化-Hexo" class="headerlink" title="初始化 Hexo"></a>初始化 Hexo</h4><p>安装 Hexo 并初始化项目，参考 <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo 官方文档</a></p><pre class=" language-sh"><code class="language-sh">cnpm install hexo-cli -ghexo init [target-dir]cd [target-dir]cnpm install</code></pre><p>完成后指定文件夹目录结构</p><pre class=" language-sh"><code class="language-sh">.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes</code></pre><h4 id="静态网页"><a href="#静态网页" class="headerlink" title="静态网页"></a>静态网页</h4><p>生成静态网页，此命令会在项目根目录下生成 <code>public</code> 文件夹</p><pre class=" language-sh"><code class="language-sh">hexo generate</code></pre><p>本地运行静态网页</p><pre class=" language-sh"><code class="language-sh">hexo server</code></pre><p>首次运行时会先自动安装 <code>hexo-server</code>，也可手动安装</p><pre class=" language-sh"><code class="language-sh">cnpm install hexo-server --save</code></pre><p>启动 Hexo 项目后即可通过 <code>http://localhost:4000</code> 进行本地访问</p><p><img src="https://i.loli.net/2020/06/26/xGcBhFer9gVDXMK.png" alt="1592989535582.png" loading="lazy"></p><h4 id="Hexo-主题（可选）"><a href="#Hexo-主题（可选）" class="headerlink" title="Hexo 主题（可选）"></a>Hexo 主题（可选）</h4><p>Hexo 默认提供的 <a href="https://github.com/hexojs/hexo-theme-landscape" target="_blank" rel="noopener">hexo-theme-landscape</a> 主题样式简单、功能较少，大多数人都会根据自己的喜好使用其他主题</p><p>这里列几个我认为比较有意思的主题：<a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">hexo-theme-ocean</a>、<a href="https://github.com/shixiaohu2206/hexo-theme-huhu" target="_blank" rel="noopener">hexo-theme-huhu</a>、<a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="noopener">hexo-theme-fluid</a>、<a href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank" rel="noopener">hexo-theme-yun</a>。此外还可以在 <a href="https://hexo.io/themes/" target="_blank" rel="noopener">Themes | Hexo</a> 发现更多主题或者根据自己的审美偏好开发属于自己的主题（可以参考：<a href="https://www.yunyoujun.cn/note/make-hexo-theme-yun/" target="_blank" rel="noopener">hexo-theme-yun 制作笔记</a>）</p><p>这里选用最后一个 <a href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank" rel="noopener">hexo-theme-yun</a> 来示范如何使用，<a href="#">展示效果</a></p><p>下载主题，在根目录下运行命令</p><pre class=" language-sh"><code class="language-sh">git clone -b master https://github.com/YunYouJun/hexo-theme-yun themes/yun</code></pre><p>编辑 Hexo 配置文件 <code>_config.yml</code></p><pre class=" language-sh"><code class="language-sh">- theme: landscape+ theme: yun</code></pre><p>需要注意的是，有的主题是有额外的依赖要求的，比如此次示范的主题由于使用了 <code>hexo-render-pug</code> 和 <code>hexo-renderer-stylus</code>，需要运行命令安装</p><pre class=" language-sh"><code class="language-sh">cnpm install hexo-render-pug hexo-renderer-stylus --save</code></pre><p>该主题的详细使用文档 <a href="https://yun.yunyoujun.cn/" target="_blank" rel="noopener">Docs of Hexo-Theme-Yun</a>，可根据文档进行额外的配置，定制出符合自己喜好的主题</p><p>完成后运行 <code>hexo server</code> 重新启动即可看到一个不同主题风格的页面</p><p><img src="https://i.loli.net/2020/06/26/mHgpUCWE6PaAnqS.png" alt="1593085668274.png" loading="lazy"></p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="部署到-Github"><a href="#部署到-Github" class="headerlink" title="部署到 Github"></a>部署到 Github</h3><p>需要先安装部署插件 <code>hexo-deploy-git</code></p><pre class=" language-sh"><code class="language-sh">cnpm install hexo-deployer-git --save</code></pre><p>然后编辑配置文件 <code>_config.yml</code> 添加内容</p><pre class=" language-sh"><code class="language-sh">deploy:  type: git  repo: git@github.com:zhanghanlun/zhanghanlun.github.io.git  branch: master</code></pre><p>最后通过命令部署</p><pre class=" language-sh"><code class="language-sh">hexo deployorhexo clean && hexo deploy</code></pre><p>保存，部署，完成后即可通过 <a href="https://atomicoo.github.io/" target="_blank" rel="noopener"><code>https://atomicoo.github.io/</code></a> 访问</p><h3 id="备份与自动部署"><a href="#备份与自动部署" class="headerlink" title="备份与自动部署"></a>备份与自动部署</h3><h4 id="源码备份"><a href="#源码备份" class="headerlink" title="源码备份"></a>源码备份</h4><p>以上步骤已将静态文件部署到 Github 上了，但最好将源码也备份一份以防万一（比如误删源码的情况等）</p><p>创建新分支 <code>source</code> 用于备份源码，仅首次</p><pre class=" language-sh"><code class="language-sh">git initgit branch sourcegit branch -v</code></pre><blockquote><p>注意：Git 的初始化仓库必须至少 <code>commot</code> 过一次才会真正创建 <code>master</code>分支，所以以上 <code>git init</code> 之后需要先随便提交一次，比如， <code>git add README.md &amp;&amp; git commit -m "first commit"</code>，否则会报错</p></blockquote><p>与 Github 仓库建立连接，仅首次</p><pre class=" language-sh"><code class="language-sh">git remote add origin https://github.com/atomicoo/atomicoo.github.io</code></pre><p>提交源码至本地 Git 仓库的 <code>source</code> 分支</p><pre class=" language-sh"><code class="language-sh">git checkout sourcegit add -Agit commit -m "commit message"</code></pre><p>将源码推送至远方 Github 仓库的  <code>source</code> 分支</p><pre class=" language-sh"><code class="language-sh">git push origin source</code></pre><p>此处需要输入 Github 用户名和密码</p><blockquote><p>注意：为了避免我们所使用的 Hexo 主题相关仓库的变动导致我们的站点出问题，我们可以自行 Fork 相关仓库然后作为子库调用。这里使用了 <code>git-submodule</code>，参考 <a href="https://git-scm.com/docs/git-submodule" target="_blank" rel="noopener">Docs of Git Submodule</a></p></blockquote><pre class=" language-sh"><code class="language-sh"># git submodule add <远程仓库> <本地路径>git submodule add -b master https://github.com/atomicoo/hexo-theme-yun.git themes/yun</code></pre><h4 id="自动部署"><a href="#自动部署" class="headerlink" title="自动部署"></a>自动部署</h4><p>如果每次修改都需要人工进行部署的话其实是非常麻烦的，此时我们就可以考虑使用持续集成进行自动部署，比较常用的是 <a href="https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions" target="_blank" rel="noopener">Github Actions</a>，<a href="https://www.netlify.com/" target="_blank" rel="noopener">Netlify</a>，<a href="https://travis-ci.org/" target="_blank" rel="noopener">Travis CI</a></p><p>具体方案及部署步骤请参考 <a href="https://blog.ichr.me/post/automated-deployment-of-serverless-static-blog/" target="_blank" rel="noopener">初探无后端静态博客自动化部署方案</a> 或者 Hexo 官方文档 <a href="https://hexo.io/zh-cn/docs/github-pages" target="_blank" rel="noopener">将 Hexo 部署到 GitHub Pages</a></p><p>若选择使用 Github Actions，可参考我的自动部署脚本 <a href="https://github.com/atomicoo/atomicoo.github.io/blob/source/.github/workflows/gh-pages.yml" target="_blank" rel="noopener">gh-pages.yml</a></p><p>对应的 Github 项目 <code>&lt;username&gt;.github.io</code> 的 <code>Actions</code> 中可查看自动部署的历史情况</p><p><img src="https://i.loli.net/2020/06/26/wuy1atskNe4HbZx.png" alt="1593085858921.png" loading="lazy"></p><p>至此，个人博客网站基本搭建完成，接下来根据自己的需求与喜好持续完善即可。</p><h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><h3 id="文章"><a href="#文章" class="headerlink" title="文章"></a>文章</h3><p>新建博客 <code>&lt;title&gt;.md</code></p><pre class=" language-sh"><code class="language-sh">hexo new [layout] [title]</code></pre><blockquote><p>.md 是 Markdown 文档的后缀名，对 Markdown 的简单的了解见 <a href="#">Markdown 基本语法</a> 和 <a href="#">Markdown 样式示例</a></p></blockquote><h4 id="布局（Layout）"><a href="#布局（Layout）" class="headerlink" title="布局（Layout）"></a>布局（Layout）</h4><table><thead><tr><th>布局</th><th>路径</th></tr></thead><tbody><tr><td>post</td><td>source/_posts</td></tr><tr><td>page</td><td>source</td></tr><tr><td>draft</td><td>source/_drafts</td></tr></tbody></table><blockquote><p>如果你不想你的文章被处理，你可以将 Front-Matter 中的<code>layout:</code> 设为 <code>false</code> </p></blockquote><h4 id="文件名称"><a href="#文件名称" class="headerlink" title="文件名称"></a>文件名称</h4><table><thead><tr><th align="left">变量</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>:title</code></td><td align="left">标题（小写，空格会替换为短杠）</td></tr><tr><td align="left"><code>:year</code></td><td align="left">建立的年份</td></tr><tr><td align="left"><code>:month</code></td><td align="left">建立的月份（有前导零）</td></tr><tr><td align="left"><code>:i_month</code></td><td align="left">建立的月份（无前导零）</td></tr><tr><td align="left"><code>:day</code></td><td align="left">建立的日期（有前导零）</td></tr><tr><td align="left"><code>:i_day</code></td><td align="left">建立的日期（无前导零）</td></tr></tbody></table><h4 id="草稿（Draft）"><a href="#草稿（Draft）" class="headerlink" title="草稿（Draft）"></a>草稿（Draft）</h4><p><code>Draft</code> 是 Hexo 的一种特殊布局，会被保存到 <code>source/_drafts</code> 中，通过 <code>pubilsh</code> 可以将其移动到 <code>source/_posts</code></p><pre class=" language-sh"><code class="language-sh">hexo publish [layout] [title]</code></pre><h4 id="模板（Scaffold）"><a href="#模板（Scaffold）" class="headerlink" title="模板（Scaffold）"></a><span id="scaffold">模板（Scaffold）</span></h4><p>新建文章时，Hexo 会根据 <code>scaffolds</code> 文件夹中对应的文件来建立文件。模板中可使用的变量如下</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th><th align="left">默认值</th></tr></thead><tbody><tr><td align="left"><code>layout</code></td><td align="left">布局</td><td align="left"></td></tr><tr><td align="left"><code>title</code></td><td align="left">标题</td><td align="left">文章的文件名</td></tr><tr><td align="left"><code>date</code></td><td align="left">建立日期</td><td align="left">文件建立日期</td></tr><tr><td align="left"><code>updated</code></td><td align="left">更新日期</td><td align="left">文件更新日期</td></tr><tr><td align="left"><code>comments</code></td><td align="left">开启文章的评论功能</td><td align="left">true</td></tr><tr><td align="left"><code>tags</code></td><td align="left">标签（不适用于分页）</td><td align="left"></td></tr><tr><td align="left"><code>categories</code></td><td align="left">分类（不适用于分页）</td><td align="left"></td></tr><tr><td align="left"><code>permalink</code></td><td align="left">覆盖文章网址</td><td align="left"></td></tr><tr><td align="left"><code>keywords</code></td><td align="left">仅用于 meta 标签和 Open Graph 的关键词（不推荐使用）</td><td align="left"></td></tr></tbody></table><h3 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h3><p>Front-matter 是文件最上方以 <code>---</code> 分隔的区域，用于指定个别文件的变量，例如</p><pre class=" language-sh"><code class="language-sh">---title: Hello Worlddate: 2013/7/13 20:46:25---</code></pre><p>预定义变量详见 <a href="#scaffold">模板（Scaffold）</a></p><h4 id="分类和标签"><a href="#分类和标签" class="headerlink" title="分类和标签"></a>分类和标签</h4><pre class=" language-sh"><code class="language-sh">categories:- Diarytags:- PlayStation- Games</code></pre><blockquote><p>注意：同级多分类</p></blockquote><pre class=" language-sh"><code class="language-sh">categories:- [Diary, PlayStation]- [Diary, Games]</code></pre><h3 id="标签插件"><a href="#标签插件" class="headerlink" title="标签插件"></a>标签插件</h3><p>标签插件用于在文章中快速插入特定内容的插件</p><h4 id="引用块"><a href="#引用块" class="headerlink" title="引用块"></a>引用块</h4><p>别名：quote</p><pre class=" language-sh"><code class="language-sh">{% blockquote [author[, source]] [link] [source_link_title] %}content{% endblockquote %}</code></pre><h4 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h4><p>别名：code</p><pre class=" language-sh"><code class="language-sh">{% codeblock [title] [lang:language] [url] [link text] [additional options] %}code snippet{% endcodeblock %}</code></pre><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a><span id="others-tag">其他</span></h4><pre class=" language-sh"><code class="language-sh"># iframe{% iframe url [width] [height] %}# image{% img [class names] /path/to/image [width] [height] '"title text" "alt text"' %}# link{% link text url [external] [title] %}# include code{% include_code [title] [lang:language] [from:line] [to:line] path/to/file %}# youtube{% youtube video_id %}# 引用文章链接{% post_link filename [title] %}# 引用文章资源{% asset_link filename [title] %}</code></pre><h4 id="文章摘要和截断"><a href="#文章摘要和截断" class="headerlink" title="文章摘要和截断"></a>文章摘要和截断</h4><p>在文章中使用 <code>&lt;!-- more --&gt;</code>，那么 <code>&lt;!-- more --&gt;</code> 之前的文字将会被视为摘要。首页中将只出现这部分文字，同时这部分文字也会出现在正文之中</p><h3 id="资源文件夹"><a href="#资源文件夹" class="headerlink" title="资源文件夹"></a>资源文件夹</h3><p>资源（Asset）表示除了文章外的其他文件，例如图片、JS、CSS 等。当资源文件较少时，可以直接放在同一个文件夹下，例如图片文件统一放在 <code>source/images</code> 中，但当资源文件较多时这显然不是一个合适的解决方案。Hexo 提供了更组织化的方式来管理资源文件，通过 设置 <code>_config.yml</code> 中的 <code>post_asset_folder</code> 为 <code>true</code> 即可开启。开启该功能后，每次新建文章时 Hexo 都会自动创建一个相应的同名文件夹用以存放文章的资源文件，引用时按照 <a href="#others-tag">其他</a> 中的 <code>引用文章资源</code> 方式即可</p><h3 id="数据文件夹"><a href="#数据文件夹" class="headerlink" title="数据文件夹"></a>数据文件夹</h3><p>这部分我用的比较少，暂略。此处插眼 <a href="https://hexo.io/zh-cn/docs/data-files" target="_blank" rel="noopener">Hexo 文档 | 数据文件夹</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo 官方文档</a></p><p><a href="https://hexo.io/zh-cn/api/" target="_blank" rel="noopener">Hexo 官方 API 信息</a></p><p><a href="https://www.yunyoujun.cn/share/how-to-build-your-site/" target="_blank" rel="noopener">教你如何从零开始搭建一个属于自己的网站</a></p><p><a href="https://yun.yunyoujun.cn/guide/" target="_blank" rel="noopener">Hexo-Theme-Yun 使用指南</a></p><p><a href="https://segmentfault.com/a/1190000018803949" target="_blank" rel="noopener">Hexo+Next 集成 Algolia 搜索</a></p><!-- Q.E.D. --><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 杂项 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Github Pages </tag>
            
            <tag> Hexo </tag>
            
            <tag> 静态网站 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
