---
title: 理解语言模型 Language Model（LM）
date: 2020-07-16 15:57:53
updated: 2020-07-16 15:57:53
tags:
  - 语言模型
  - N-Grams
  - NN
  - RNN
categories:
  - 一点理论
katex: true
---

## 引言

语言模型，在百度百科中的描述是：*根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。语言模型与语言客观事实之间的关系，如同数学上的抽象直线与具体直线之间的关系*。在我看来，语言模型本质上其实是在解决这样一个问题：**语句是否合理**（更直白的说法就是，说的是不是人话 :smirk:）。本文会介绍语言模型在计算机领域的几个转变的重要节点以及个人的一点小小的理解。​

<!-- more -->

## 定义

标准定义：对于给定语言序列 $w_{1}, w_{2}, \ldots, w_{n}$，计算其概率大小，即 $P(w_{1}, w_{2}, \ldots, w_{n})$。

白话解释：给定一句话，判断其是不是正常的语句，即，说的是不是人话。

## 统计语言模型

提到统计语言模型就不得不谈谈 N 元文法模型（N-Gram Model）了。

N-Gram 模型将语句（词序列）看作一个随机事件，并赋予其相应的概率来描述某语句出现的可能性。即，给定一个词汇集合 $V$，对于由 $V$ 中词汇组成的序列 $S=<w_{1}, w_{2}, \ldots, w_{n}>, w_{i} \in V$，N-Gram 模型将计算其出现的概率 $P(w_{1}, w_{2}, \ldots, w_{n})$。

首先，由链式法则（chain rule）得：
$$P(w_{1}, w_{2}, \ldots, w_{n})=P(w_{1})P(w_{2}|w_{1}) \cdots P(w_{n}|w_{1}, \ldots, w_{n-1})$$
然后，在统计语言模型中，我们会采用极大似然估计来计算每个词出现的条件概率（“统计语言模型”中的“统计”一词就体现在这儿）：
$$\begin{aligned} P(w_{i}|w_{1}, \ldots, w_{i-1})&=\frac{C(w_{1}, \ldots, w_{i-1}, w_{i})}{\sum_w C(w_{1}, \ldots, w_{i-1}, w)} \\\\ &=\frac{C(w_{1}, \ldots, w_{i-1}, w_{i})}{C(w_{1}, \ldots, w_{i-1})} \end{aligned}$$



## 神经网络语言模型



---

To Be Continued.

<!-- Q.E.D. -->