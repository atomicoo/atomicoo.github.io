<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="原子态"><meta name="copyright" content="原子态"><meta name="generator" content="Hexo 4.2.1"><meta name="theme" content="hexo-theme-yun"><title>理解语言模型 Language Model | 原子态</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;family=Source+Code+Pro&amp;display=swap" media="none" onload="this.media='all'"><script src="//at.alicdn.com/t/font_1140697_stqaphw3j4.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/gh/atomicoo/hexo-theme-yun@latest/source/js/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="原子态"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"root":"/","title":"一行代码调一天","version":"0.9.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"algolia":{"appID":"KP7IPEQ96T","apiKey":"4d1c9fc1470aaacb209d4eecaf3b4879","indexName":"my-hexo-blog","hits":{"per_page":8},"labels":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容: ${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><link rel="preconnect" href="https://stats.g.doubleclick.net" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=UA-170889853-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-170889853-1');</script><meta name="description" content="引言语言模型，在百度百科中的描述是：根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。语言模型与语言客观事实之间的关系，如同数学上的抽象直线与具体直线之间的关系。在我看来，语言模型本质上其实是在解决这样一个问题：语句是否合理（更直白的说法就是，说的是不是人话 😏）。本文会介绍语言模型在计算机领域的几个转变的重要节点以及个人的一点小小的理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="理解语言模型 Language Model">
<meta property="og:url" content="https://atomicoo.com/mathematics/understanding-language-model/index.html">
<meta property="og:site_name" content="原子态">
<meta property="og:description" content="引言语言模型，在百度百科中的描述是：根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。语言模型与语言客观事实之间的关系，如同数学上的抽象直线与具体直线之间的关系。在我看来，语言模型本质上其实是在解决这样一个问题：语句是否合理（更直白的说法就是，说的是不是人话 😏）。本文会介绍语言模型在计算机领域的几个转变的重要节点以及个人的一点小小的理解。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/07/17/74V2kGAzhCEadx5.png">
<meta property="article:published_time" content="2020-07-16T07:57:53.000Z">
<meta property="article:modified_time" content="2020-07-16T07:57:53.000Z">
<meta property="article:author" content="原子态">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="语言模型">
<meta property="article:tag" content="N-Grams">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/07/17/74V2kGAzhCEadx5.png"><script src="/js/ui/mode.js"></script><link rel="alternate" href="/atom.xml" title="原子态" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script defer src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="原子态"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="原子态"></a><div class="site-author-name"><a href="/about/">原子态</a></div><a class="site-name" href="/about/site.html">原子态</a><sub class="site-subtitle">Atomicoo's</sub><div class="site-desciption">“咚！咚！咚！”</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">24</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="留言板"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/atomicoo" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/409646386" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:atomicoo95@gmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="Links" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="Girls" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#语言模型定义"><span class="toc-number">2.</span> <span class="toc-text">语言模型定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#统计语言模型"><span class="toc-number">3.</span> <span class="toc-text">统计语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#N-Gram-模型基础"><span class="toc-number">3.1.</span> <span class="toc-text">N-Gram 模型基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#起始标签-结束标签"><span class="toc-number">3.2.</span> <span class="toc-text">起始标签&#x2F;结束标签</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经语言模型"><span class="toc-number">4.</span> <span class="toc-text">神经语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于前馈神经网络"><span class="toc-number">4.1.</span> <span class="toc-text">基于前馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于递归神经网络"><span class="toc-number">4.2.</span> <span class="toc-text">基于递归神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型评价指标"><span class="toc-number">5.</span> <span class="toc-text">模型评价指标</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://atomicoo.com/mathematics/understanding-language-model/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="原子态"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="原子态"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">理解语言模型 Language Model<a class="post-edit-link" href="https://github.com/atomicoo/atomicoo.github.io/tree/source/source/_posts/mathematics/understanding-language-model.md" target="_blank" title="编辑" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-07-16 15:57:53" itemprop="dateCreated datePublished" datetime="2020-07-16T15:57:53+08:00">2020-07-16</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">8.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">15 分钟</span></span></span><span class="leancloud_visitors" id="/mathematics/understanding-language-model/" data-flag-title="理解语言模型 Language Model"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E6%95%B0%E5%AD%A6%E7%A0%94%E7%A9%B6/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">数学研究</span></a></span> > <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E6%95%B0%E5%AD%A6%E7%A0%94%E7%A9%B6/%E4%BF%A1%E6%81%AF%E6%97%B6%E4%BB%A3/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">信息时代</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">神经网络</span></a><a class="tag" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">语言模型</span></a><a class="tag" href="/tags/N-Grams/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">N-Grams</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>语言模型，在百度百科中的描述是：<em>根据语言客观事实而进行的语言抽象数学建模，是一种对应关系。语言模型与语言客观事实之间的关系，如同数学上的抽象直线与具体直线之间的关系</em>。在我看来，语言模型本质上其实是在解决这样一个问题：<strong>语句是否合理</strong>（更直白的说法就是，说的是不是人话 <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png?v8">😏</span>）。本文会介绍语言模型在计算机领域的几个转变的重要节点以及个人的一点小小的理解。</p>
<a id="more"></a>

<h2 id="语言模型定义"><a href="#语言模型定义" class="headerlink" title="语言模型定义"></a>语言模型定义</h2><p>标准定义：对于给定语言序列 $w_{1}, w_{2}, \ldots, w_{n}$，计算其概率大小，即 $P(w_{1}, w_{2}, \ldots, w_{n})$。</p>
<p>白话解释：给定一句话，判断其是不是正常的语句，或者说其作为正常语句出现的概率有多大。</p>
<h2 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h2><p>说到这里，就不得不谈谈 N 元文法模型（N-Gram Model）了。</p>
<h3 id="N-Gram-模型基础"><a href="#N-Gram-模型基础" class="headerlink" title="N-Gram 模型基础"></a>N-Gram 模型基础</h3><p>N-Gram 模型将语句（词序列）看作一个随机事件，并赋予其相应的概率来描述某语句出现的可能性。即，给定一个词汇集合 $V$，对于由 $V$ 中词汇组成的序列 $S=&lt;w_{1}, w_{2}, \ldots, w_{n}&gt;, w_{i} \in V$，N-Gram 模型将计算其出现的概率 $P(w_{1}^{n})$。</p>
<p>提前声明，为简化书写，我们使用符号 $w_{i}^{j}$ 来表示 $w_{i}, w_{i+1}, \ldots, w_{j}$。</p>
<p>首先，由链式法则（chain rule）得：<br>$$ P(w_{1}^{n})=P(w_{1})P(w_{2}|w_{1}) \cdots P(w_{n}|w_{1}^{n-1}) $$<br>然后，在统计语言模型中，我们会采用<a href="#">极大似然估计</a>来计算每个词出现的条件概率（“统计语言模型”中的“统计”一词就体现在这儿）：<br>$$ \begin{aligned} P(w_{i}|w_{1}^{i-1})&amp;=\frac{C(w_{1}^{i-1}, w_{i})}{\sum_w C(w_{1}^{i-1}, w)} \\ &amp;=\frac{C(w_{1}^{i-1}, w_{i})}{C(w_{1}^{i-1})} \end{aligned} $$</p>
<p>其中，$C(\cdot)$ 表示子序列在训练集中出现的次数，即用频率近似概率。</p>
<p>显然，当序列较长时，这样直接进行计算是不现实的，原因有两点：</p>
<ul>
<li>参数空间过大：当序列过长时，$P(w_{n}|w_{1}^{n-1})$ 的可能性过多，难以估算</li>
<li>数据过于稀疏：当序列过长时，很容易出现 $w_{1}^{n}$ 根本没在训练集中出现过的情况</li>
</ul>
<p>因此，我们引入<a href="#">马尔可夫假设</a>，即假设当前词的概率仅与前 $n-1$ 个词相关，得：</p>
<p>$$ P(w_{i}|w_{1}^{i-1}) \approx P(w_{i}|w_{i-n+1}^{i-1}) $$</p>
<p>基于上式，可以得到 N-Gram 模型的定义：</p>
<ul>
<li>unigram：$n=1$，$P(w_{1}^{n}) \approx \prod_{i=1}^{n} P(w_{i})$</li>
<li>bigram：$n=2$，$P(w_{1}^{n}) \approx P(w_{1}) \prod_{i=2}^{n}P(w_{i}|w_{i-1})$</li>
<li>trigram：$n=3$，$P(w_{1}^{n}) \approx P(w_{1}) P(w_{2}|w_{1}) \prod_{i=1}^{n}P(w_{i}|w_{i-2}, w_{i-1})$</li>
<li>……</li>
</ul>
<p>需要注意的是，在进行实际操作时，有两个小 tricks。</p>
<p>其一，我们注意到在上述定义中，当 $n&gt;1$ 时，会出现 $w_{i} (i \le 0)$ 的情况，此时可以通过在序列首添加一个或多个伪词（起始符，$\langle s \rangle$）来解决。譬如 $n=2$ 时，$P(w_{1}|\langle s \rangle)$。</p>
<p>其二，我们往往不会直接计算上述概率，而是采用<strong>对数概率</strong>，即 $\log (\prod_{i} p_{i}) = \sum_{i} \log p_{i}$</p>
<p>这样计算会有两大优势：</p>
<ul>
<li>将连乘转化为累加，加速计算</li>
<li>防止数值溢出（概率本就是一些较小的数，连乘容易造成数值下溢）</li>
</ul>
<h3 id="起始标签-结束标签"><a href="#起始标签-结束标签" class="headerlink" title="起始标签/结束标签"></a>起始标签/结束标签</h3><p><strong>为什么 N-Gram 需要开始标签/结束标签？</strong></p>
<p>开门见山，先放结论：</p>
<p>$P(w_{1}^{n})$ 建模的 <strong>是</strong> 在无限长序列中出现子序列 $w_{1}^{n}$ 的概率，而 <strong>不是</strong> 序列 $w_{1}^{n}$ 出现的概率！</p>
<p>由于没有准确理解到这一点，导致一开始看 N-Gram 模型时对开始/结束标签的存在十分的困惑。</p>
<blockquote>
<p>额外多说一句，上述理解仅代表个人的看法，我会在下面给出自己的解释，不一定正确，但以我目前的能力，只有这个解释能够说服我自己。（数学渣滓的悲哀）</p>
</blockquote>
<p>这是很容易被初学者误解的一点，但只要理解了，会有豁然开朗的感觉。理由的话其实也很简单，我们回过头再仔细看看公式：$P(w_{1}^{n})=P(w_{1})P(w_{2}|w_{1}) \cdots P(w_{n}|w_{1}^{n-1})$，很容易发现，公式的第一项 $P(w_{1})$ 表示的是 $w_{1}$ 出现的概率，即 $w_{1}$ 在任何位置出现都被包含在内。这样就很清晰了，这里并没有限定 $w_{1}$ 前面还有多少词汇，当然了，也没有限定 $w_{n}$ 后面还有多少词汇，所以才说 $P(w_{1}^{n})$ 建模的是在无限长序列中所有出现子序列 $w_{1}^{n}$ 的总概率，如果用正则来表示的话大概就是 $(.\star?)w_{1}w_{2} \cdots w_{n}(.\star?)$ 吧。</p>
<p>这时候就体现出开始/结束标签的重要性了。在实践中，语句肯定只会是有限长的，像这种对无限长序列的建模其实毫无实际意义，但加上开始/结束标签就不一样了，界定了语句的开始与结束之后我们的模型就有能力建模任意长序列了（当然也包括无限长序列，只是由于概率的累乘，过长序列的出现概率几乎可以忽略不计，这也符合我们的直觉）。</p>
<p>下面举个栗子，为了简化说明过程，我们先考虑有开始标签而没有结束标签的情况。</p>
<p>假设我们有以下语料（是的，你没看错，就三句，词汇表 $V = (a, b, c)$）：</p>
<pre class=" language-sh"><code class="language-sh">⟨s⟩ a b
⟨s⟩ a c
⟨s⟩ b a</code></pre>
<p>取 $n=2$，即 bigram 模型，可得：</p>
<pre class=" language-sh"><code class="language-sh">P(a|⟨s⟩) = 2/3
P(b|⟨s⟩) = 1/3
P(b|a) = 1/2
P(c|a) = 1/2
P(a|b) = 1</code></pre>
<p>那么可以计算出结果如下：</p>
<pre class=" language-sh"><code class="language-sh">P(ab) = 2/3 * 1/2 = 1/3
P(ac) = 2/3 * 1/2 = 1/3
P(ba) = 1/3 * 1 = 1/3
P(aa) = P(bb) = P(bc) = P(ca) = P(cb) = P(cc) = 0</code></pre>
<p>桥豆麻袋！是不是有哪里不对？长度为 2 的序列概率和就等于 1 了，那其他长度的序列可咋办？但是考虑到我们前面说的就很容易理解了，这里其实应该是：</p>
<pre class=" language-sh"><code class="language-sh">P(aa...) + P(ab...) + ... + P(cb...) + P(cc...) = 1</code></pre>
<p>Bingo！那么现在加上结束标签再算一次：</p>
<pre class=" language-sh"><code class="language-sh">⟨s⟩ a b ⟨/s⟩
⟨s⟩ a c ⟨/s⟩
⟨s⟩ b a ⟨/s⟩</code></pre>
<p>各概率如下：</p>
<pre class=" language-sh"><code class="language-sh">P(a|⟨s⟩) = 2/3
P(b|⟨s⟩) = 1/3
P(b|a) = 1/3
P(c|a) = 1/3
P(a|b) = 1/2
P(⟨/s⟩|a) = 1/3
P(⟨/s⟩|b) = 1/2
P(⟨/s⟩|c) = 1</code></pre>
<p>计算结果如下：</p>
<pre class=" language-sh"><code class="language-sh">P(ab) = 2/3 * 1/3 * 1/2 = 1/9
P(ac) = 2/3 * 1/3 * 1 = 2/9
P(ba) = 1/3 * 1/2 * 1/3 = 1/18
P(aa) = P(bb) = P(bc) = P(ca) = P(cb) = P(cc) = 0

P(aa) + P(ab) + ... + P(cb) + P(cc) = 7/18 < 1</code></pre>
<p>可以再继续算算序列长度为 1 和 3 的概率。我们的语料分布情况使然，序列长度为 1~3 的概率和应该已经接近 1 了，这是合理的。</p>
<p>以上例子已经说明了为什么需要结束标签，反过来也是同样成立的。综上所述，要建模有限长序列，必须要有开始/结束标签的存在。</p>
<h2 id="神经语言模型"><a href="#神经语言模型" class="headerlink" title="神经语言模型"></a>神经语言模型</h2><p>有了 N-Gram 模型的基础，我们应该已经能够大致理解语言模型在做什么了。其实就是在给定一个词序列的前提下，预测该词序列的下一个词的概率情况，即 $P(w_{i}|w_{1}^{i-1})$，然后根据链式法则就可以计算出所有词序列出现的概率，即 $P(w_{1}^{n})$。N-Gram 模型中的 $n$ 的取值不同归根结底只是对 $P(w_{i}|w_{1}^{i-1})$ 的近似程度不同罢了。</p>
<h3 id="基于前馈神经网络"><a href="#基于前馈神经网络" class="headerlink" title="基于前馈神经网络"></a>基于前馈神经网络</h3><p>既然我们已经理解了语言模型做的是在给定一个词序列的前提下预测该词序列的下一个词的概率情况，那么神经网络就想说话了，这事儿我熟啊。既然能用统计模型做，那么肯定也能用前馈神经网络来做。说到这里就不得不提到 Bengio 等人在 2001 年发表在 NIPS 上的论文 <a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>。</p>
<p><img src="https://i.loli.net/2020/07/17/74V2kGAzhCEadx5.png" alt="FFNN-LM.png" loading="lazy"></p>
<p>事实上这个神经网络模型也是一个 N-Gram 模型，即只考虑前 $n-1$ 个词的依赖关系，只不过是先将每个词都映射为连续空间中的一个词向量，再通过一个三层前馈神经网络去建模这种依赖关系（约束关系），相较于统计模型而言， 极大地增强了模型的泛化能力。模型公式如下（结合模型结构图很容易看懂了，不多 BB）：</p>
<p>$$ \begin{aligned} x &amp;= concat(C(w)_{t-n+1}^{t-1}) \\ h &amp;= tanh(Hx + d)  \\ y &amp;= Wx + Uh + b \end{aligned} $$</p>
<p>其中，$concat(\cdot)$ 为拼接函数，$C(w)<em>{i}^{j}$ 表示 $C(w</em>{i}), \ldots, C(w_{j})$，$C(w_{i})$ 为 $w_{i}$ 的词向量。</p>
<h3 id="基于递归神经网络"><a href="#基于递归神经网络" class="headerlink" title="基于递归神经网络"></a>基于递归神经网络</h3><p>上面提到的前馈神经网络模型虽然很好地改善了统计模型泛化能力较差的问题，但是实质上还是基于 N-Gram 模型的思想，只考虑了有限的前文信息，那么在遇到长序列时候有没有什么办法能够考虑到足够远的前文信息呢？当然有！那就是 Mikolov 于 2010 年发表的论文 <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf" target="_blank" rel="noopener">Recurrent neural network based language model</a> 中提出的方法，该论文将 RNN 用在了 LM 训练任务上。公式如下（注意，这里的 RNN 指的是狭义上最基础的 RNN 网络，即只在 RNNCell 内部建立了权连接）：</p>
<p>$$ \begin{aligned} x_{t}^{i} &amp;= concat(w_{t}^{i}, h_{t-1}^{i}) \\ h_{t}^{i} &amp;= f(W^{i} x_{t}^{i} + b^{i}) \\ y_{t} &amp;= g(U h_{t}^{n} + b) \end{aligned} $$</p>
<p>其中，输入的词嵌入 $w_{t}^{1}$ 使用最简单的 one-hot 编码，$n$ 为 RNN 网络层数，$i \in [1, n]$，$f(\cdot)$ 为 $sigmoid(\cdot)$，$g(\cdot)$ 为 $softmax(\cdot)$。</p>
<p>额外多说几句。单从理论上来说，RNN 应该是能够捕获足够远的前文信息的，但在实践中并非如此，一个合理的直觉是：因为 RNN 使用的这种最简单的 Cell 结构导致前文信息很容易随着时间步而逐渐被稀释，当遇到长序列时，较远的前文信息已经被稀释到几乎可以忽略不计了。也正因为如此，后来又进一步发展出了 <a href="#">LSTM</a> 和 <a href="#">GRU</a> 等各种衍生的递归神经网络（关于这部分，以后我会专门开一篇博客进行介绍）。</p>
<p>事实上，从统计语言模型开始一直到后面的各种递归神经网络，只要能够理解语言模型（Language Model）到底在做些什么，那么其他所有的东西也只是实现方法的不断改进优化罢了。</p>
<h2 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h2><p>（空）</p>
<hr>
<p>To Be Continued.</p>
<!-- Q.E.D. --><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">此路是我开，留下买路财 [ 狗头 ]</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="/images/alipay.jpg"><img loading="lazy" src="/images/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/images/qq-pay.jpg"><img loading="lazy" src="/images/qq-pay.jpg" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/images/wechat-pay.jpg"><img loading="lazy" src="/images/wechat-pay.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>原子态</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://atomicoo.com/mathematics/understanding-language-model/" title="理解语言模型 Language Model">https://atomicoo.com/mathematics/understanding-language-model/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/mathematics/matrix-vector-derivation-1/" rel="prev" title="【数学天坑】之矩阵/向量求导（一）"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">【数学天坑】之矩阵/向量求导（一）</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/mathematics/understanding-rnn-networks/" rel="next" title="【搞定神经网络】循环神经网络篇"><span class="post-nav-text">【搞定神经网络】循环神经网络篇</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>重要评论建议跳转 GitHub Issues 发布</span><br><span>每篇文章的首个评论需要先根据 Comment 模板创建相应的 Issue</span><br><span>请避免创建重复的 Issue，感谢配合</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/atomicoo/atomicoo.github.io/issues?q=is:issue+理解语言模型 Language Model" target="_blank" rel="noopener">GitHub Issues</a></div><div id="valine-container"></div><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  const valineConfig = {"enable":true,"appId":"frjzlYEWNDPLkxagtOz0FWFe-9Nh9j0Va","appKey":"gK2710zgTWTS3C6UchnWhQHd","placeholder":"大佬们缺捡肥皂的吗？_(:з」∠)_","visitor":true,"recordIP":true,"enableQQ":true,"requiredFields":["nick","mail"],"avatar":null,"pageSize":10,"highlight":true,"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = window.location.pathname
  new Valine(valineConfig)
}
setTimeout(initValine, 1000)</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="http://www.beian.miit.gov.cn" target="_blank">闽ICP备20015372号</a></div><div class="copyright"><span>&copy; 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 原子态</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.1</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.9.1</span></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script></body></html>