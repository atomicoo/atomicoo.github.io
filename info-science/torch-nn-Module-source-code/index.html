<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="原子态"><meta name="copyright" content="原子态"><meta name="generator" content="Hexo 4.2.1"><meta name="theme" content="hexo-theme-yun"><title>【PyTorch 源码阅读】 torch.nn.Module 篇 | 原子态</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;family=Source+Code+Pro&amp;display=swap" media="none" onload="this.media='all'"><script src="//at.alicdn.com/t/font_1140697_stqaphw3j4.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><link rel="alternate" href="/atom.xml" title="原子态"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"root":"/","title":"一行代码调一天","version":"0.9.1","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"algolia":{"appID":"KP7IPEQ96T","apiKey":"4d1c9fc1470aaacb209d4eecaf3b4879","indexName":"my-hexo-blog","hits":{"per_page":8},"labels":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容: ${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="preconnect" href="https://www.google-analytics.com" crossorigin><link rel="preconnect" href="https://stats.g.doubleclick.net" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=UA-170889853-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-170889853-1');</script><meta name="description" content="引言【PyTorch 源码阅读系列】主要是记录一些阅读 PyTorch 源码时的笔记（好记性不如烂笔头）。事实上 PyTorch 的文档齐全，哪怕你不阅读源码也能够很好地使用它来搭建并训练自己的模型，我之所以选择阅读源码，一方面是为了对 PyTorch 有更深入的理解，另一方面是学习这种优秀的源码也能够帮助自己写出更优雅规范的代码。本文为 torch.nn.Module 篇，本系列的第一篇。">
<meta property="og:type" content="article">
<meta property="og:title" content="【PyTorch 源码阅读】 torch.nn.Module 篇">
<meta property="og:url" content="https://atomicoo.com/info-science/torch-nn-Module-source-code/index.html">
<meta property="og:site_name" content="原子态">
<meta property="og:description" content="引言【PyTorch 源码阅读系列】主要是记录一些阅读 PyTorch 源码时的笔记（好记性不如烂笔头）。事实上 PyTorch 的文档齐全，哪怕你不阅读源码也能够很好地使用它来搭建并训练自己的模型，我之所以选择阅读源码，一方面是为了对 PyTorch 有更深入的理解，另一方面是学习这种优秀的源码也能够帮助自己写出更优雅规范的代码。本文为 torch.nn.Module 篇，本系列的第一篇。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-07-04T08:52:45.000Z">
<meta property="article:modified_time" content="2020-07-06T08:59:00.000Z">
<meta property="article:author" content="原子态">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="源码阅读">
<meta property="article:tag" content="Module">
<meta name="twitter:card" content="summary"><script src="/js/ui/mode.js"></script><link rel="alternate" href="/atom.xml" title="原子态" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script defer src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="原子态"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="原子态"></a><div class="site-author-name"><a href="/about/">原子态</a></div><a class="site-name" href="/about/site.html">原子态</a><sub class="site-subtitle">Atomicoo's</sub><div class="site-desciption">“咚！咚！咚！”</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">24</span></a></div><a class="site-state-item hty-icon-button" href="/about/#comment" title="留言板"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-clipboard-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/atomicoo" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/409646386" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:atomicoo95@gmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="Links" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a><a class="links-item hty-icon-button" href="/girls/" title="Girls" style="color:hotpink"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-women-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引言"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#源码阅读"><span class="toc-number">2.</span> <span class="toc-text">源码阅读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考资料"><span class="toc-number">3.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://atomicoo.com/info-science/torch-nn-Module-source-code/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="原子态"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="原子态"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">【PyTorch 源码阅读】 torch.nn.Module 篇<a class="post-edit-link" href="https://github.com/atomicoo/atomicoo.github.io/tree/source/source/_posts/info-science/torch-nn-Module-source-code.md" target="_blank" title="编辑" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="创建时间：2020-07-04 16:52:45" itemprop="dateCreated datePublished" datetime="2020-07-04T16:52:45+08:00">2020-07-04</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-2-line"></use></svg></span> <time title="修改时间：2020-07-06 16:59:00" itemprop="dateModified" datetime="2020-07-06T16:59:00+08:00">2020-07-06</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">43k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">1:18</span></span></span><span class="leancloud_visitors" id="/info-science/torch-nn-Module-source-code/" data-flag-title="【PyTorch 源码阅读】 torch.nn.Module 篇"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E4%BF%A1%E6%81%AF%E6%97%B6%E4%BB%A3/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">信息时代</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/PyTorch/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">PyTorch</span></a><a class="tag" href="/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">源码阅读</span></a><a class="tag" href="/tags/Module/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">Module</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content post-markdown"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>【PyTorch 源码阅读系列】主要是记录一些阅读 PyTorch 源码时的笔记（<strong>好记性不如烂笔头</strong>）。事实上 PyTorch 的文档齐全，哪怕你不阅读源码也能够很好地使用它来搭建并训练自己的模型，我之所以选择阅读源码，一方面是为了对 PyTorch 有更深入的理解，另一方面是学习这种优秀的源码也能够帮助自己写出更优雅规范的代码。本文为 <code>torch.nn.Module</code> 篇，本系列的第一篇。</p>
<a id="more"></a>

<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><blockquote>
<p>PyTorch 版本：1.5.1 - py3.7_cuda102_cudnn7_0</p>
</blockquote>
<p><code>torch.nn.Module</code> 是 PyTorch 所有神经网络模块的基类（官方文档：<em>Base class for all neural network modules</em>），无论是官方实现还是自己创建的的网络模块都应该是它的子类。</p>
<p>一个简单的使用示例：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SimpleNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>       <span class="token comment" spellcheck="true"># 定义神经网络</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>SimpleNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># Convolution kennel</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># an affine operation: y = Wx + b</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Max pooling over (2, 2) window</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token operator">*</span><span class="token number">6</span><span class="token operator">*</span><span class="token number">6</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x</code></pre>
<hr>
<p><code>torch.nn.Module</code> 类中共包含 49 个函数（指定版本 1.5.1），下面一一进行分析。</p>
<hr>
<p><code>__init__</code> 和 <code>forward</code>。</p>
<p><code>__init__</code> 函数主要是初始化模块内部状态（<em>internal Module state</em>）。<code>forward</code> 函数需要在子类中实现，如果子类中没有实现会引发 <code>NotImplementedError</code>。</p>
<hr>
<p><code>register_buffer</code>、<code>register_parameter</code> 和 <code>add_module</code>。</p>
<p><code>register_buffer</code> 添加 <code>name: buffer</code> 到模块的 <code>self._buffers</code> 字典中，这里 <code>buffer</code> 指的是一些非模型参数的持久状态（<em>the persistent state</em>），譬如 <code>BatchNorm</code> 的 <code>running_mean</code> 等。</p>
<p><code>register_parameter</code> 添加 <code>name: parameter</code> 到模块的 <code>self._parameters</code> 字典中。</p>
<p><code>add_module</code> 根据 <code>name: module</code> 添加子模块到模块的 <code>self._modules</code> 字典中。</p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># register_buffer</span>
self<span class="token punctuation">.</span>_buffers<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> tensor

<span class="token comment" spellcheck="true"># register_parameter</span>
self<span class="token punctuation">.</span>_parameters<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> param

<span class="token comment" spellcheck="true"># add_module</span>
self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> module</code></pre>
<hr>
<p><code>_apply</code> 和 <code>apply</code>。</p>
<p><code>_apply</code> 的作用是对模块中的所有 <code>tensor</code>（包括 <code>parameters</code> 和 <code>buffers</code> ）进行一遍传入的 <code>fn</code> 操作。通过 <code>_apply</code> 我们可以方便地对模块中的 <code>tensor</code> 做很多操作，譬如下面会讲到的 <code>cuda</code> 和 <code>cpu</code>。</p>
<p>以下是简化源码：第一个循环就是递归地对所有的子模块进行一遍操作；<code>compute_should_use_set_data</code> 决定是否采用 <code>in-place</code> 的方式（<em>change the tensor in-place</em>），即就地修改 <code>tensor</code>；第二个循环对所有的 <code>parameters</code> 进行 <code>fn</code> 操作，如果 <code>param</code> 有 <code>.grad</code> 的话需要对其也进行 <code>fn</code> 操作；第三个循环对所有的 <code>buffers</code> 进行 <code>fn</code> 操作。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_apply</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        module<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span>fn<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">compute_should_use_set_data</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor_applied<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># ...</span>

    <span class="token keyword">for</span> key<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> param <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># ...</span>
            compute_should_use_set_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># ...</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
                <span class="token comment" spellcheck="true"># ...</span>

    <span class="token keyword">for</span> key<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># ...</span>

    <span class="token keyword">return</span> self</code></pre>
<p><code>apply</code> 与 <code>_apply</code> 有所不同，它的作用是递归地对所有子模块用传入的 <code>fn</code> 操作一遍（<em>Applies <code>fn</code> recursively to every submodule</em>）。这里的子模块指的是 <code>.children()</code> 列出的内容。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">apply</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        module<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>fn<span class="token punctuation">)</span>
    fn<span class="token punctuation">(</span>self<span class="token punctuation">)</span>
    <span class="token keyword">return</span> self</code></pre>
<p>譬如可以利用 <code>apply</code> 对模型权重（<em>weight</em>）进行初始化。源码中给出的示例：</p>
<pre class=" language-sh"><code class="language-sh">>>> @torch.no_grad()
>>> def init_weights(m):
>>>     print(m)
>>>     if type(m) == nn.Linear:
>>>         m.weight.fill_(1.0)
>>>         print(m.weight)
>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
>>> net.apply(init_weights)</code></pre>
<hr>
<p><code>cuda</code>、<code>cpu</code> 和 <code>share_memory</code>。</p>
<p>将模块所有的 <code>tensor</code> 移入指定位置（GPU/CPU/共享内存）中。通过源码可以看到，这三者都调用了前面讲到的 <code>_apply</code> 函数。</p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># cuda</span>
<span class="token keyword">def</span> <span class="token function">cuda</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># cpu</span>
<span class="token keyword">def</span> <span class="token function">cpu</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># shared memory</span>
<span class="token keyword">def</span> <span class="token function">share_memory</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>share_memory_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<hr>
<p><code>type</code>、<code>float</code>、<code>double</code>、<code>half</code> 和 <code>bfloat16</code>。</p>
<p>这几个函数都是将模块中所有的 <code>tensor</code> 转成指定的类型。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">type</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dst_type<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>type<span class="token punctuation">(</span>dst_type<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">float</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">double</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>double<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">half</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>half<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">bfloat16</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>_apply<span class="token punctuation">(</span><span class="token keyword">lambda</span> t<span class="token punctuation">:</span> t<span class="token punctuation">.</span>bfloat16<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> t<span class="token punctuation">.</span>is_floating_point<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> t<span class="token punctuation">)</span></code></pre>
<hr>
<p><code>to</code>。</p>
<p><code>to</code> 可以用来对模块的所有 <code>tensor</code> 进行设备和/或类型的 <code>in-place</code> 方式的操作（<em>Moves and/or casts the parameters and buffers</em>）。源码中给出的示例：</p>
<pre class=" language-sh"><code class="language-sh">>>> linear = nn.Linear(2, 2)
>>> linear.weight
>>> linear.to(torch.double)
>>> linear.weight
>>> gpu1 = torch.device("cuda:1")
>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
>>> linear.weight
>>> cpu = torch.device("cpu")
>>> linear.to(cpu)
>>> linear.weight</code></pre>
<p>大家可以实际运行下看看结果。</p>
<hr>
<p><code>register_backward_hook</code>、<code>register_forward_pre_hook</code> 和 <code>register_forward_hook</code>。</p>
<p>这三个函数分别在模块中注册 <code>forward_pre_hook</code>、<code>forward_hook</code> 和 <code>backward_hook</code>。关于 <code>Hooks</code> （钩子？挂钩？咋翻好听？）的作用我会在后面专门开一篇文章说明。</p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># backward_hook</span>
hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> grad_input<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tensor <span class="token operator">or</span> None

<span class="token comment" spellcheck="true"># forward_pre_hook</span>
hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> input<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None <span class="token operator">or</span> modified input

<span class="token comment" spellcheck="true"># forward_hook</span>
hook<span class="token punctuation">(</span>module<span class="token punctuation">,</span> output<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None <span class="token operator">or</span> modified output</code></pre>
<hr>
<p><code>_slow_forward</code> 和 <code>__call__</code>。</p>
<p><code>__call__</code> 是模块计算的真正入口，内部会调用 <code>_slow_forward</code> 函数或者 <code>forward</code> 函数进行计算，事实上 <code>_slow_forward</code> 内部最终也是调用 <code>forward</code> 函数进行计算，两者的差别在于有些自定义操作是没有 C 代码的，这种情况就会直接调用 Python 版本，反之调用 C 版本（效率高）。简化源码如下：</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_forward_pre_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># ...</span>
    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>_C<span class="token punctuation">.</span>_get_tracing_state<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> self<span class="token punctuation">.</span>_slow_forward<span class="token punctuation">(</span><span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span><span class="token operator">*</span>input<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_forward_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># ...</span>
    <span class="token keyword">for</span> hook <span class="token keyword">in</span> self<span class="token punctuation">.</span>_backward_hooks<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># ...</span>
    <span class="token keyword">return</span> result</code></pre>
<hr>
<p><code>__setstate__</code>。</p>
<p><code>__setstate__</code> 设置 <code>state</code>，这个比较简单，直接看源码。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__setstate__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>__dict__<span class="token punctuation">.</span>update<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># Support loading old checkpoints that don't have the following attrs:</span>
    <span class="token keyword">if</span> <span class="token string">'_forward_pre_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_forward_pre_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token string">'_state_dict_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_state_dict_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token string">'_load_state_dict_pre_hooks'</span> <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>__dict__<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_load_state_dict_pre_hooks <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<hr>
<p><code>__getattr__</code>、<code>__setattr__</code> 和 <code>__delattr__</code>。</p>
<p><code>__getattr__</code> 用于获取指定 <code>name</code> 的模块成员（包括 <code>parameters</code>、<code>buffers</code> 和 <code>modules</code>，查找顺序从前到后）。</p>
<p><code>__getattr__</code> 用于查找指定 <code>name</code> 的模块成员后对其进行设置（同上）。</p>
<p><code>__delattr__</code> 用于删除指定 <code>name</code> 的模块成员（同上）。</p>
<p>这三个函数都比较简单，不多做说明。</p>
<hr>
<p><code>_register_state_dict_hook</code> 和 <code>_register_load_state_dict_pre_hook</code>。</p>
<p>这三个函数与前面提到的 <code>register_*_hook</code> 类似，用于注册 <code>Hooks</code>。</p>
<p><code>_register_state_dict_hook</code> ：</p>
<p><em>These hooks will be called with arguments: <code>self</code>, <code>state_dict</code>, <code>prefix</code>, <code>local_metadata</code>, after the <code>state_dict</code> of <code>self</code> is set.</em></p>
<p><code>_register_load_state_dict_pre_hook</code>：</p>
<p><em>These hooks will be called with arguments: <code>state_dict</code>, <code>prefix</code>, <code>local_metadata</code>, <code>strict</code>, <code>missing_keys</code>, <code>unexpected_keys</code>, <code>error_msgs</code>, before loading <code>state_dict</code> into <code>self</code>.</em></p>
<hr>
<p><code>_save_to_state_dict</code> 和 <code>_load_from_state_dict</code>。</p>
<p><code>_save_to_state_dict</code> 作用是保存 <code>state</code> 到 <code>destination</code> 指定的字典中，此函数会被当前模块的所有子模块调用（<em>This is called on every submodule in :meth:<code>~torch.nn.Module.state_dict</code></em>）。</p>
<p><code>_load_from_state_dict</code> 作用与 <code>_save_to_state_dict</code> 相反，用来加载模块。</p>
<hr>
<p><code>state_dict</code> 和 <code>load_state_dict</code>。</p>
<p><code>state_dict</code> 的作用是返回一个包含模块完整 <code>state</code> 的字典（<em>Returns a dictionary containing a whole state of the module</em>），字典中的键值对对应着 <code>name: parameters</code> 或 <code>name: buffers</code>。此函数会调用上面的 <code>_save_to_state_dict</code> 函数。</p>
<p><code>load_state_dict</code> 的作用与 <code>state_dict</code> 相反，是将  <code>name: parameters</code> 或 <code>name: buffers</code> 加载到模块及其子模块中去。此函数会调用上面的 <code>_load_from_state_dict</code> 函数。</p>
<hr>
<p><code>named_parameters</code>、<code>named_buffers</code>、<code>named_children</code> 和 <code>named_modules</code>。</p>
<p>这四个函数的作用是返回 <code>name: menbers</code> 的生成器（<em>generator</em>），其中 <code>members</code> 可以是 <code>parameters</code>、<code>buffers</code>、<code>children</code> 或 <code>modules</code>。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_named_members</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> get_members_fn<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    r<span class="token triple-quoted-string string">"""Helper method for yielding various names + members of modules."""</span>
    memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>
    modules <span class="token operator">=</span> self<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span>prefix<span class="token operator">=</span>prefix<span class="token punctuation">)</span> <span class="token keyword">if</span> recurse <span class="token keyword">else</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>prefix<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> module_prefix<span class="token punctuation">,</span> module <span class="token keyword">in</span> modules<span class="token punctuation">:</span>
        members <span class="token operator">=</span> get_members_fn<span class="token punctuation">(</span>module<span class="token punctuation">)</span>
        <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> members<span class="token punctuation">:</span>
            <span class="token keyword">if</span> v <span class="token keyword">is</span> None <span class="token operator">or</span> v <span class="token keyword">in</span> memo<span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
                memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
                name <span class="token operator">=</span> module_prefix <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token string">'.'</span> <span class="token keyword">if</span> module_prefix <span class="token keyword">else</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token operator">+</span> k
                <span class="token keyword">yield</span> name<span class="token punctuation">,</span> v

<span class="token keyword">def</span> <span class="token function">named_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    gen <span class="token operator">=</span> self<span class="token punctuation">.</span>_named_members<span class="token punctuation">(</span>
        <span class="token keyword">lambda</span> module<span class="token punctuation">:</span> module<span class="token punctuation">.</span>_parameters<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span> recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span>
    <span class="token keyword">for</span> elem <span class="token keyword">in</span> gen<span class="token punctuation">:</span>
        <span class="token keyword">yield</span> elem

<span class="token keyword">def</span> <span class="token function">named_buffers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    gen <span class="token operator">=</span> self<span class="token punctuation">.</span>_named_members<span class="token punctuation">(</span>
        <span class="token keyword">lambda</span> module<span class="token punctuation">:</span> module<span class="token punctuation">.</span>_buffers<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span> recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span>
    <span class="token keyword">for</span> elem <span class="token keyword">in</span> gen<span class="token punctuation">:</span>
        <span class="token keyword">yield</span> elem

<span class="token keyword">def</span> <span class="token function">named_children</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> module <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> module <span class="token operator">not</span> <span class="token keyword">in</span> memo<span class="token punctuation">:</span>
            memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>module<span class="token punctuation">)</span>
            <span class="token keyword">yield</span> name<span class="token punctuation">,</span> module

<span class="token keyword">def</span> <span class="token function">named_modules</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prefix<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> memo <span class="token keyword">is</span> None<span class="token punctuation">:</span>
        memo <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self <span class="token operator">not</span> <span class="token keyword">in</span> memo<span class="token punctuation">:</span>
        memo<span class="token punctuation">.</span>add<span class="token punctuation">(</span>self<span class="token punctuation">)</span>
        <span class="token keyword">yield</span> prefix<span class="token punctuation">,</span> self
        <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> module <span class="token keyword">is</span> None<span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            submodule_prefix <span class="token operator">=</span> prefix <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token string">'.'</span> <span class="token keyword">if</span> prefix <span class="token keyword">else</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token operator">+</span> name
            <span class="token keyword">for</span> m <span class="token keyword">in</span> module<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span>memo<span class="token punctuation">,</span> submodule_prefix<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">yield</span> m</code></pre>
<p>注意：<code>named_children</code> 非递归，只返回下一级的子模块（<em>immediate children modules</em>）；<code>named_modules</code> 不返回重复的模块（<em>Duplicate modules are returned only once</em>）。</p>
<p>此外还有相应的 <code>parameters</code>、<code>buffers</code>、<code>children</code> 和 <code>modules</code>，只会返回 <code>members</code> 的生成器。通过源码可以看到，这四个函数调用了上面的四个函数。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span>recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">yield</span> param

<span class="token keyword">def</span> <span class="token function">buffers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> buf <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_buffers<span class="token punctuation">(</span>recurse<span class="token operator">=</span>recurse<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">yield</span> buf

<span class="token keyword">def</span> <span class="token function">children</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">yield</span> module

<span class="token keyword">def</span> <span class="token function">modules</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">yield</span> module</code></pre>
<hr>
<p><code>train</code> 和 <code>eval</code>。</p>
<p>这两个函数的作用是将模块及其子模块设置成训练/评估模式（<em>training/evaluation mode</em>）。这只对特定的模块起作用，譬如 <code>Dropout</code> 和 <code>BatchNorm</code>。</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    self<span class="token punctuation">.</span>training <span class="token operator">=</span> mode
    <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        module<span class="token punctuation">.</span>train<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>
    <span class="token keyword">return</span> self

<span class="token keyword">def</span> <span class="token function">eval</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre>
<hr>
<p><code>requires_grad_</code> 和 <code>zero_grad</code>。</p>
<p><code>requires_grad_</code> 用于设置模块中的 <code>parameters</code> 是否需要追踪梯度，操作层面来说即 <code>param</code> 的 <code>.requires_grad</code> 是否为 <code>True</code>。</p>
<p><code>zero_grad</code> 用于将模块中 <code>parameters</code> 的梯度清零。</p>
<hr>
<p><code>_get_name</code>、<code>extra_repr</code>、<code>__repr__</code> 和 <code>__dir__</code>。</p>
<p>这四个函数都是用于输出模块相关信息的，并不复杂，直接看源码即可。</p>
<hr>
<p>想要进一步了解的点：</p>
<ul>
<li>buffers</li>
<li>in-place</li>
<li>hooks</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://pytorch.org/docs/stable/nn.html#module" target="_blank" rel="noopener">PyTorch 官方文档</a></p>
<!-- Q.E.D. --><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">此路是我开，留下买路财 [ 狗头 ]</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="/images/alipay.jpg"><img loading="lazy" src="/images/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/images/qq-pay.jpg"><img loading="lazy" src="/images/qq-pay.jpg" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/images/wechat-pay.jpg"><img loading="lazy" src="/images/wechat-pay.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>原子态</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://atomicoo.com/info-science/torch-nn-Module-source-code/" title="【PyTorch 源码阅读】 torch.nn.Module 篇">https://atomicoo.com/info-science/torch-nn-Module-source-code/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/mathematics/understanding-rnn-networks/" rel="prev" title="【搞定神经网络】循环神经网络篇"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">【搞定神经网络】循环神经网络篇</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/info-science/markdown-html-special-characters/" rel="next" title="【转载】Markdown &amp; HTML 特殊字符转义表"><span class="post-nav-text">【转载】Markdown &amp; HTML 特殊字符转义表</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>重要评论建议跳转 GitHub Issues 发布</span><br><span>每篇文章的首个评论需要先根据 Comment 模板创建相应的 Issue</span><br><span>请避免创建重复的 Issue，感谢配合</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/atomicoo/atomicoo.github.io/issues?q=is:issue+【PyTorch 源码阅读】 torch.nn.Module 篇" target="_blank" rel="noopener">GitHub Issues</a></div><div id="valine-container"></div><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  const valineConfig = {"enable":true,"appId":"frjzlYEWNDPLkxagtOz0FWFe-9Nh9j0Va","appKey":"gK2710zgTWTS3C6UchnWhQHd","placeholder":"大佬们缺捡肥皂的吗？_(:з」∠)_","visitor":true,"recordIP":true,"enableQQ":true,"requiredFields":["nick","mail"],"avatar":null,"pageSize":10,"highlight":true,"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = window.location.pathname
  new Valine(valineConfig)
}
setTimeout(initValine, 1000)</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="http://www.beian.miit.gov.cn" target="_blank">闽ICP备20015372号</a></div><div class="copyright"><span>&copy; 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 原子态</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.1</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.9.1</span></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script defer src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script defer src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script defer src="/js/search/algolia-search.js"></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div class="algolia-pagination" id="algolia-pagination"></div></div></div></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script></body></html>